{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My theoretic solution to the linear maze problem:\n\nNeural Network:\n\t- input neurons = 5 (since the maze has 5 states)\n\t- hidden layers = 0 (not needed in this problem)\n\t- output neurons = 3 (since we are predicting one action from a set of 3 possible actions)\n\nLoss Function:\nE(St, At) = (Rt + gamma * max(Q(St+1, a; w) - Q(St, At; w))^2\n\t- St = state at time t\n\t- At = action at time t\n\t- Rt = immediate reward received from the environment at time t\n\t- w = weights = network parameters\n\t- gamma = 0.5\n\t- (Rt + gamma * max(Q(St+1, a; w)) = actual value of the network\n\t- Q(St, At; w) = estimated / predicted value of the network\n\nLoss function explained:\nThe loss value given a state and an action = (actual value - estimated value)^2\n\nTraining the model:\n\t- I train the model by feeding it with a one-hot vector, e.g. [0,0,1,0,0] to represent a state.\n\nEvaluating the model:\n\t- I evaluate the model by feeding the network with each possible state\n\t- This gives us the optimal move (the highest value in the output) based on states and rewards\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Evaluate #############\nSTATE 0:\n\nInput:\n[[ 1.  0.  0.  0.  0.]]\n\nOutput:\nAction: 2\nQ values: [[  2.48779497e-28   3.18237197e-28   1.07692301e+00]]\n\n\nSTATE 1:\n\nInput:\n[[ 0.  1.  0.  0.  0.]]\n\nOutput:\nAction: 2\nQ values: [[  1.74145643e-28   2.22766033e-28   5.53849433e-15]]\n\n\nSTATE 2:\n\nInput:\n[[ 0.  0.  1.  0.  0.]]\n\nOutput:\nAction: 2\nQ values: [[  1.74145643e-28   2.22766033e-28   1.47509653e-24]]\n\n\nSTATE 3:\n\nInput:\n[[ 0.  0.  0.  1.  0.]]\n\nOutput:\nAction: 2\nQ values: [[  1.21901952e-28   1.55936216e-28   7.74748307e-27]]\n\n\nSTATE 4:\n\nInput:\n[[ 0.  0.  0.  0.  1.]]\n\nOutput:\nAction: 2\nQ values: [[  8.53313655e-29   1.09155354e-28   2.15384603e+00]]\n\n\n"
     ]
    }
   ],
   "source": [
    "# Reinforcement learning in 1d maze\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# explain the theory behind the solution\n",
    "print('My theoretic solution to the linear maze problem:')\n",
    "print('')\n",
    "print('Neural Network:')\n",
    "print('\\t- input neurons = 5 (since the maze has 5 states)')\n",
    "print('\\t- hidden layers = 0 (not needed in this problem)')\n",
    "print('\\t- output neurons = 3 (since we are predicting one action from a set of 3 possible actions)')\n",
    "print('')\n",
    "print('Loss Function:')\n",
    "print('E(St, At) = (Rt + gamma * max(Q(St+1, a; w) - Q(St, At; w))^2')\n",
    "print('\\t- St = state at time t')\n",
    "print('\\t- At = action at time t')\n",
    "print('\\t- Rt = immediate reward received from the environment at time t')\n",
    "print('\\t- w = weights = network parameters')\n",
    "print('\\t- gamma = 0.5')\n",
    "print('\\t- (Rt + gamma * max(Q(St+1, a; w)) = actual value of the network')\n",
    "print('\\t- Q(St, At; w) = estimated / predicted value of the network')\n",
    "print('')\n",
    "print('Loss function explained:')\n",
    "print('The loss value given a state and an action = (actual value - estimated value)^2')\n",
    "print('')\n",
    "print('Training the model:')\n",
    "print('\\t- I train the model by feeding it with a one-hot vector, e.g. [0,0,1,0,0] to represent a state.')\n",
    "print('')\n",
    "print('Evaluating the model:')\n",
    "print('\\t- I evaluate the model by feeding the network with each possible state')\n",
    "print('\\t- This gives us the optimal move (the highest value in the output) based on states and rewards')\n",
    "print('')\n",
    "\n",
    "# load dataset\n",
    "states = np.array([0, 1, 2, 3, 4]).reshape((1, 5))\n",
    "rewards = np.array([1, 0, 0, 0, 2]).reshape((1, 5))\n",
    "actions = np.array([0, 1, 2]).reshape(1, 3)\n",
    "\n",
    "# Set learning parameters\n",
    "num_iterations = 10000\n",
    "discount_factor = 0.7\n",
    "learning_rate = 0.5\n",
    "\n",
    "# establish the feed-forward part of the network\n",
    "inputs = tf.placeholder(shape=[1, 5], dtype=tf.float32)\n",
    "W = tf.Variable(tf.random_uniform([5, 3], 0.1, 0.9))\n",
    "y = tf.matmul(inputs, W)\n",
    "predict = tf.argmax(y, 1)\n",
    "\n",
    "# obtain the loss by taking the sum of squares difference between the actual value and predicted Q values\n",
    "y_ = tf.placeholder(shape=[1, 3], dtype=tf.float32)\n",
    "loss = tf.reduce_sum(tf.square((discount_factor * y_) - y))\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "# train the network\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    identity_matrix = np.identity(5)\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        # random state:\n",
    "        rand_index = random.randint(0, 4)\n",
    "        a, allQ = sess.run([predict, y], feed_dict={inputs: identity_matrix[rand_index: rand_index+1]})\n",
    "        curr_action = a[0]\n",
    "        \n",
    "        # perform 5 steps from the random start state:\n",
    "        for j in range(5):\n",
    "            # obtain a new state and reward from the environment based on the predicted action\n",
    "            if curr_action == 0 and rand_index-1 >= 0:\n",
    "                # action = left\n",
    "                rand_index -= 1\n",
    "                new_state = identity_matrix[rand_index: rand_index+1]\n",
    "                reward = rewards[0][rand_index]\n",
    "                \n",
    "            elif curr_action == 2 and rand_index+1 <= 4:\n",
    "                # action = right\n",
    "                rand_index += 1\n",
    "                new_state = identity_matrix[rand_index: rand_index+1]\n",
    "                reward = rewards[0][rand_index]\n",
    "            else:\n",
    "                # action = stay\n",
    "                new_state = identity_matrix[rand_index: rand_index+1]\n",
    "                reward = rewards[0][rand_index]\n",
    "            \n",
    "            # obtain the Q values from feeding the new state through the network\n",
    "            a, new_Q = sess.run([predict, y], feed_dict={inputs: new_state})\n",
    "            \n",
    "            # obtain the maximum new_Q value\n",
    "            max_new_Q = np.max(new_Q)\n",
    "            curr_action = a[0]\n",
    "            \n",
    "            # set target value\n",
    "            target_Q = allQ\n",
    "            target_Q[0, curr_action] = reward + learning_rate * max_new_Q\n",
    "            \n",
    "            # train the network using the target and predicted Q values\n",
    "            _, W1 = sess.run([train_step, W], feed_dict={\n",
    "                inputs: identity_matrix[rand_index: rand_index+1],\n",
    "                y_: target_Q\n",
    "            })\n",
    "\n",
    "    print('########## Evaluate #############')\n",
    "    for i in range(5):\n",
    "        one_hot_vector = np.zeros((1, 5))\n",
    "        one_hot_vector[0][i] = 1\n",
    "        print('STATE {0}:'.format(i))\n",
    "        print('')\n",
    "        print('Input:')\n",
    "        print(one_hot_vector)\n",
    "        print('')\n",
    "        print('Output:')\n",
    "        eval_a, eval_q_out = sess.run([predict, y], feed_dict={inputs: one_hot_vector})\n",
    "        print('Action: {0}'.format(eval_a[0]))\n",
    "        print('Q values: {0}'.format(eval_q_out))\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}