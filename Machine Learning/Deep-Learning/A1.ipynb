{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sKGsmsab5I1S",
    "colab_type": "text"
   },
   "source": [
    "# <center>Assignment 1</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q5VD__J65I1U",
    "colab_type": "text"
   },
   "source": [
    "There are 2 main parts asked in this assignment - Tensorflow Basics and Neural Networks. You can choose to code in Python2 or Python3. All the imports made in this notebook are as below; if these imports work, you are (mostly) set to complete the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "B4teB1gh5I1V",
    "colab_type": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     }
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import random \n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vWzfE86tusct",
    "colab_type": "text"
   },
   "source": [
    "## Tensorflow - Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gtgV4CJM5I1Z",
    "colab_type": "text"
   },
   "source": [
    "### I. Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "waXA1j7A5I1a",
    "colab_type": "text"
   },
   "source": [
    "<b>1a. Creating Sample Data </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "LrDL6noo5I1b",
    "colab_type": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     }
    }
   },
   "outputs": [],
   "source": [
    "x = np.random.randn(100, 3)  # 100 data points of dimension 3\n",
    "w = np.array([[1], [2], [3]])\n",
    "b = 10\n",
    "y = np.dot(x, w) + b\n",
    "n_samples = x.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J6m9Ibn65QcP",
    "colab_type": "text"
   },
   "source": [
    "**1b. Plot Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "lS4Hlv8k5UeZ",
    "colab_type": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1196db208>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1194bba58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x.T[0], y.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qo8hagle5I1c",
    "colab_type": "text"
   },
   "source": [
    "<b>2. Creating Placeholders</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "9aM6fyvd5I1e",
    "colab_type": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     }
    }
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(dtype=tf.float32, shape=[None, 3])\n",
    "Y_Expected = tf.placeholder(dtype=tf.float32, shape=[None, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IIcGDu2s5I1f",
    "colab_type": "text"
   },
   "source": [
    "<b>3. Creating Variables</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "OJd_anpk5I1h",
    "colab_type": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     }
    }
   },
   "outputs": [],
   "source": [
    "b = tf.Variable(\n",
    "    dtype=tf.float32,\n",
    "    initial_value=np.zeros(shape=(1, 1)),\n",
    "    name=\"b\")\n",
    "W = tf.Variable(\n",
    "    dtype=tf.float32,\n",
    "    initial_value=np.zeros(shape=w.shape),\n",
    "    name=\"W\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cZItc9VZ5I1k",
    "colab_type": "text"
   },
   "source": [
    "<b> 4. Creating Compute Graph </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "HwpRYAc85I1k",
    "colab_type": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     }
    }
   },
   "outputs": [],
   "source": [
    "# Define the equation to compute the output variable.\n",
    "Y = tf.matmul(X, W) + b\n",
    "\n",
    "# Define the loss function\n",
    "cost = tf.divide(tf.square(Y_Expected - Y, name=\"loss\"), n_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoqEq-Mj5I1m",
    "colab_type": "text"
   },
   "source": [
    "<b> 5. Training and optimizer </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "TkF3TNpt5I1o",
    "colab_type": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [[ 0.23367128]\n [ 2.43590236]\n [ 1.22140634]\n [ 2.24416351]\n [ 0.97417033]\n [ 0.70235223]\n [ 1.91396332]\n [ 1.26079285]\n [ 2.15064168]\n [ 1.04374194]\n [ 0.39531854]\n [ 0.8921324 ]\n [ 0.86004019]\n [ 0.79801732]\n [ 0.1375742 ]\n [ 1.36387289]\n [ 0.84946024]\n [ 0.99483854]\n [ 0.74835676]\n [ 1.22701526]\n [ 1.67737103]\n [ 0.74737138]\n [ 0.04879889]\n [ 0.28373623]\n [ 0.85103679]\n [ 2.41333532]\n [ 0.85047269]\n [ 1.63011587]\n [ 0.01177646]\n [ 0.62563634]\n [ 2.29529428]\n [ 1.48668718]\n [ 0.84382033]\n [ 1.92180526]\n [ 1.94121122]\n [ 0.12570754]\n [ 1.41417587]\n [ 0.50727695]\n [ 1.36370623]\n [ 0.06240673]\n [ 2.56590915]\n [ 2.1882937 ]\n [ 1.08353364]\n [ 0.55915254]\n [ 0.34213009]\n [ 0.03622762]\n [ 4.28922796]\n [ 1.78130412]\n [ 0.94636041]\n [ 1.39680326]\n [ 1.4681716 ]\n [ 1.13248563]\n [ 1.36686432]\n [ 1.08252215]\n [ 1.05948818]\n [ 1.31031764]\n [ 0.29236573]\n [ 0.21215859]\n [ 1.3595196 ]\n [ 1.65362179]\n [ 1.23698652]\n [ 0.51965922]\n [ 1.91630661]\n [ 0.98855966]\n [ 0.76204568]\n [ 0.22956489]\n [ 2.08056712]\n [ 0.98740828]\n [ 1.49948156]\n [ 1.06743872]\n [ 0.80282128]\n [ 0.31795841]\n [ 0.57862341]\n [ 1.72492862]\n [ 1.02184904]\n [ 0.7711817 ]\n [ 0.639467  ]\n [ 0.25286317]\n [ 0.91286856]\n [ 0.90101463]\n [ 0.75147688]\n [ 0.0986005 ]\n [ 0.47094259]\n [ 1.3514514 ]\n [ 1.27653754]\n [ 0.36935958]\n [ 0.94917089]\n [ 0.83297044]\n [ 0.46937427]\n [ 1.27691531]\n [ 1.67013752]\n [ 1.64851868]\n [ 1.44634771]\n [ 0.31731275]\n [ 0.66239393]\n [ 0.98631388]\n [ 1.51748812]\n [ 1.40656602]\n [ 4.12419605]\n [ 2.9853251 ]]\n1 [[ 0.13897994]\n [ 1.54101944]\n [ 0.78202266]\n [ 1.39257443]\n [ 0.66707879]\n [ 0.43117753]\n [ 1.23429441]\n [ 0.85194421]\n [ 1.38603771]\n [ 0.65763152]\n [ 0.27822134]\n [ 0.58412963]\n [ 0.56801653]\n [ 0.5620833 ]\n [ 0.08941425]\n [ 0.8755483 ]\n [ 0.5619604 ]\n [ 0.64560091]\n [ 0.45922467]\n [ 0.78933942]\n [ 1.04092979]\n [ 0.5280813 ]\n [ 0.02443619]\n [ 0.16477549]\n [ 0.57689172]\n [ 1.52689707]\n [ 0.59770906]\n [ 1.01331282]\n [ 0.01078152]\n [ 0.40666553]\n [ 1.51345658]\n [ 0.92336023]\n [ 0.54357237]\n [ 1.22911847]\n [ 1.20446324]\n [ 0.07008135]\n [ 0.88833076]\n [ 0.36359864]\n [ 0.81596762]\n [ 0.03838399]\n [ 1.69615138]\n [ 1.38616979]\n [ 0.69366342]\n [ 0.3552247 ]\n [ 0.20258412]\n [ 0.0193612 ]\n [ 2.78038931]\n [ 1.12688601]\n [ 0.60758793]\n [ 0.88233328]\n [ 0.91848218]\n [ 0.73440415]\n [ 0.91458023]\n [ 0.67601097]\n [ 0.68830317]\n [ 0.80553335]\n [ 0.15809439]\n [ 0.12065304]\n [ 0.90471411]\n [ 1.07701838]\n [ 0.80445802]\n [ 0.36416799]\n [ 1.20345449]\n [ 0.68803334]\n [ 0.48985472]\n [ 0.13017403]\n [ 1.3185873 ]\n [ 0.63145441]\n [ 0.97397661]\n [ 0.71374547]\n [ 0.53435087]\n [ 0.21015424]\n [ 0.36886203]\n [ 1.06322408]\n [ 0.64666522]\n [ 0.46127892]\n [ 0.4254328 ]\n [ 0.14901799]\n [ 0.63540441]\n [ 0.54557425]\n [ 0.4554112 ]\n [ 0.06541691]\n [ 0.30153263]\n [ 0.86249834]\n [ 0.80550599]\n [ 0.22825651]\n [ 0.55770546]\n [ 0.49816307]\n [ 0.30735976]\n [ 0.82963288]\n [ 1.09691465]\n [ 1.00931609]\n [ 0.85559839]\n [ 0.20267479]\n [ 0.42650622]\n [ 0.60588688]\n [ 0.95049948]\n [ 0.86762965]\n [ 2.68754649]\n [ 1.89204335]]\n2 [[ 0.08235977]\n [ 0.97568917]\n [ 0.5013203 ]\n [ 0.86453909]\n [ 0.45644197]\n [ 0.2642628 ]\n [ 0.7967456 ]\n [ 0.57556552]\n [ 0.89411902]\n [ 0.41461059]\n [ 0.19513409]\n [ 0.3825981 ]\n [ 0.37540886]\n [ 0.39442554]\n [ 0.05791643]\n [ 0.56241089]\n [ 0.37202606]\n [ 0.41923678]\n [ 0.28145012]\n [ 0.50812882]\n [ 0.64618766]\n [ 0.37212694]\n [ 0.01176805]\n [ 0.09509011]\n [ 0.39080808]\n [ 0.96705073]\n [ 0.41879612]\n [ 0.62971812]\n [ 0.00931882]\n [ 0.26443559]\n [ 0.99887103]\n [ 0.57392037]\n [ 0.35050038]\n [ 0.78643572]\n [ 0.74795043]\n [ 0.03861386]\n [ 0.55827671]\n [ 0.25931224]\n [ 0.48732135]\n [ 0.02356911]\n [ 1.12227416]\n [ 0.87906235]\n [ 0.44433457]\n [ 0.22576074]\n [ 0.11950773]\n [ 0.01010365]\n [ 1.80456829]\n [ 0.71294832]\n [ 0.38991386]\n [ 0.55762422]\n [ 0.57509893]\n [ 0.47662777]\n [ 0.61256027]\n [ 0.4222025 ]\n [ 0.44753307]\n [ 0.49492946]\n [ 0.08384851]\n [ 0.06799549]\n [ 0.6021533 ]\n [ 0.70234841]\n [ 0.5235858 ]\n [ 0.25473806]\n [ 0.75573981]\n [ 0.47833106]\n [ 0.31508628]\n [ 0.07312635]\n [ 0.83684015]\n [ 0.40398511]\n [ 0.63306659]\n [ 0.47738543]\n [ 0.35555986]\n [ 0.13891725]\n [ 0.23520301]\n [ 0.65548903]\n [ 0.40933368]\n [ 0.27516335]\n [ 0.28304955]\n [ 0.08730008]\n [ 0.44116372]\n [ 0.3301678 ]\n [ 0.27538517]\n [ 0.04334447]\n [ 0.1929768 ]\n [ 0.55098003]\n [ 0.50860417]\n [ 0.14084798]\n [ 0.32622352]\n [ 0.29707727]\n [ 0.20153596]\n [ 0.5396232 ]\n [ 0.72089583]\n [ 0.61719805]\n [ 0.50451517]\n [ 0.12944837]\n [ 0.27486938]\n [ 0.37192068]\n [ 0.59552878]\n [ 0.53504419]\n [ 1.75293195]\n [ 1.20022535]]\n3 [[ 0.04861109]\n [ 0.61826754]\n [ 0.32177785]\n [ 0.53698671]\n [ 0.31209347]\n [ 0.16167244]\n [ 0.51479828]\n [ 0.38877895]\n [ 0.57733727]\n [ 0.26156196]\n [ 0.13643183]\n [ 0.25068656]\n [ 0.24828325]\n [ 0.27583873]\n [ 0.03738735]\n [ 0.36148933]\n [ 0.24645838]\n [ 0.27242023]\n [ 0.17226438]\n [ 0.32732686]\n [ 0.40128076]\n [ 0.26158354]\n [ 0.00537494]\n [ 0.05448573]\n [ 0.26459202]\n [ 0.61312145]\n [ 0.29262769]\n [ 0.39121354]\n [ 0.00772433]\n [ 0.17201732]\n [ 0.65985566]\n [ 0.35701093]\n [ 0.22622892]\n [ 0.50340319]\n [ 0.46487153]\n [ 0.02098334]\n [ 0.35102063]\n [ 0.18410921]\n [ 0.29045886]\n [ 0.01444711]\n [ 0.74325037]\n [ 0.55811948]\n [ 0.28479251]\n [ 0.14353925]\n [ 0.07020944]\n [ 0.00511644]\n [ 1.17267954]\n [ 0.45109302]\n [ 0.250108  ]\n [ 0.35258594]\n [ 0.36042041]\n [ 0.30957481]\n [ 0.41066992]\n [ 0.26371661]\n [ 0.29122758]\n [ 0.30390492]\n [ 0.04341755]\n [ 0.03791998]\n [ 0.40084276]\n [ 0.45858735]\n [ 0.34104976]\n [ 0.17789346]\n [ 0.4745523 ]\n [ 0.33219588]\n [ 0.20280138]\n [ 0.04063525]\n [ 0.53186148]\n [ 0.25856373]\n [ 0.41175854]\n [ 0.31938943]\n [ 0.2365305 ]\n [ 0.09184054]\n [ 0.15001507]\n [ 0.40420482]\n [ 0.25916713]\n [ 0.16365381]\n [ 0.18832991]\n [ 0.05080289]\n [ 0.30559558]\n [ 0.19969696]\n [ 0.16612743]\n [ 0.02868463]\n [ 0.12344553]\n [ 0.35232216]\n [ 0.3213467 ]\n [ 0.08677335]\n [ 0.18986668]\n [ 0.1766046 ]\n [ 0.1323234 ]\n [ 0.35137886]\n [ 0.474076  ]\n [ 0.37691277]\n [ 0.29644588]\n [ 0.08267625]\n [ 0.1773067 ]\n [ 0.2281228 ]\n [ 0.37323225]\n [ 0.32985049]\n [ 1.14435697]\n [ 0.76206708]]\n4 [[ 0.02856477]\n [ 0.39211139]\n [ 0.20679952]\n [ 0.33370769]\n [ 0.21325412]\n [ 0.09871688]\n [ 0.33294421]\n [ 0.26256883]\n [ 0.37314525]\n [ 0.16511756]\n [ 0.09511792]\n [ 0.16431415]\n [ 0.16431892]\n [ 0.19231072]\n [ 0.02405305]\n [ 0.23249106]\n [ 0.16338564]\n [ 0.17713504]\n [ 0.1052839 ]\n [ 0.21100296]\n [ 0.24928622]\n [ 0.18346369]\n [ 0.00227544]\n [ 0.03096723]\n [ 0.17904148]\n [ 0.38914526]\n [ 0.20395374]\n [ 0.24295887]\n [ 0.00620224]\n [ 0.11194304]\n [ 0.43629569]\n [ 0.22227058]\n [ 0.14616415]\n [ 0.32236657]\n [ 0.28919944]\n [ 0.01121567]\n [ 0.22081514]\n [ 0.13018975]\n [ 0.17274672]\n [ 0.0088396 ]\n [ 0.49267903]\n [ 0.35477269]\n [ 0.18264432]\n [ 0.0913009 ]\n [ 0.04105922]\n [ 0.00249153]\n [ 0.76299232]\n [ 0.28542873]\n [ 0.1603532 ]\n [ 0.22305189]\n [ 0.22609383]\n [ 0.20123075]\n [ 0.27557543]\n [ 0.16474153]\n [ 0.18967152]\n [ 0.1864866 ]\n [ 0.02180902]\n [ 0.02088964]\n [ 0.26687852]\n [ 0.29979774]\n [ 0.2223271 ]\n [ 0.12403924]\n [ 0.29795855]\n [ 0.23048237]\n [ 0.1306157 ]\n [ 0.0222944 ]\n [ 0.33852619]\n [ 0.16555768]\n [ 0.26799506]\n [ 0.21374582]\n [ 0.15731108]\n [ 0.06072714]\n [ 0.09570666]\n [ 0.24931106]\n [ 0.16412924]\n [ 0.09701613]\n [ 0.12531637]\n [ 0.02934083]\n [ 0.21123853]\n [ 0.12071505]\n [ 0.09995652]\n [ 0.01896154]\n [ 0.07892964]\n [ 0.22551571]\n [ 0.203169  ]\n [ 0.05336798]\n [ 0.10988552]\n [ 0.10462416]\n [ 0.08699597]\n [ 0.22905533]\n [ 0.31195784]\n [ 0.2298408 ]\n [ 0.17350951]\n [ 0.0528026 ]\n [ 0.11447941]\n [ 0.1398049 ]\n [ 0.23398191]\n [ 0.20328608]\n [ 0.74772149]\n [ 0.48431578]]\n5 [[ 0.01670283]\n [ 0.24889544]\n [ 0.13307694]\n [ 0.20749339]\n [ 0.14562817]\n [ 0.06014936]\n [ 0.21553743]\n [ 0.17730597]\n [ 0.24140166]\n [ 0.10430494]\n [ 0.06614288]\n [ 0.10773966]\n [ 0.10882355]\n [ 0.13369831]\n [ 0.01542138]\n [ 0.14961895]\n [ 0.10838814]\n [ 0.1152538 ]\n [ 0.06424672]\n [ 0.13611178]\n [ 0.1549232 ]\n [ 0.12840809]\n [ 0.00085605]\n [ 0.01743686]\n [ 0.12109122]\n [ 0.2472605 ]\n [ 0.14182208]\n [ 0.15083033]\n [ 0.0048573 ]\n [ 0.07287815]\n [ 0.28873363]\n [ 0.13850807]\n [ 0.09453031]\n [ 0.2065205 ]\n [ 0.18009031]\n [ 0.0058756 ]\n [ 0.13897711]\n [ 0.09172712]\n [ 0.1024973 ]\n [ 0.00539839]\n [ 0.3268711 ]\n [ 0.22578746]\n [ 0.11720473]\n [ 0.05809871]\n [ 0.0238901 ]\n [ 0.00115064]\n [ 0.49704048]\n [ 0.18061179]\n [ 0.10275681]\n [ 0.14117807]\n [ 0.14197052]\n [ 0.13090786]\n [ 0.18508796]\n [ 0.10292413]\n [ 0.12363287]\n [ 0.11435428]\n [ 0.0105279 ]\n [ 0.011342  ]\n [ 0.1777163 ]\n [ 0.19623102]\n [ 0.14504769]\n [ 0.08636629]\n [ 0.18705863]\n [ 0.15976731]\n [ 0.08417951]\n [ 0.01204791]\n [ 0.21579334]\n [ 0.10605023]\n [ 0.17454222]\n [ 0.14308709]\n [ 0.1046016 ]\n [ 0.04016155]\n [ 0.06107546]\n [ 0.15381366]\n [ 0.10396685]\n [ 0.05730602]\n [ 0.0833939 ]\n [ 0.01679962]\n [ 0.14573067]\n [ 0.07292929]\n [ 0.05997131]\n [ 0.01252105]\n [ 0.05044217]\n [ 0.14449491]\n [ 0.12853976]\n [ 0.03276269]\n [ 0.06319426]\n [ 0.06174508]\n [ 0.05727155]\n [ 0.14948022]\n [ 0.20540585]\n [ 0.13993584]\n [ 0.10111687]\n [ 0.03372275]\n [ 0.07398412]\n [ 0.08560221]\n [ 0.14672816]\n [ 0.12524228]\n [ 0.48898423]\n [ 0.30808821]]\n6 [[  9.71334055e-03]\n [  1.58126473e-01]\n [  8.57476145e-02]\n [  1.29089057e-01]\n [  9.93911996e-02]\n [  3.65658179e-02]\n [  1.39666051e-01]\n [  1.19715512e-01]\n [  1.56320617e-01]\n [  6.59352615e-02]\n [  4.58855480e-02]\n [  7.06697702e-02]\n [  7.21191913e-02]\n [  9.27099138e-02]\n [  9.85282566e-03]\n [  9.63468105e-02]\n [  7.19522238e-02]\n [  7.50398785e-02]\n [  3.91388051e-02]\n [  8.78626183e-02]\n [  9.63189229e-02]\n [  8.97036716e-02]\n [  2.61345587e-04]\n [  9.71252285e-03]\n [  8.18598196e-02]\n [  1.57284200e-01]\n [  9.84089598e-02]\n [  9.35979187e-02]\n [  3.72834085e-03]\n [  4.74652164e-02]\n [  1.91245064e-01]\n [  8.63938704e-02]\n [  6.11987337e-02]\n [  1.32359013e-01]\n [  1.12262793e-01]\n [  3.00235115e-03]\n [  8.75146315e-02]\n [  6.44150972e-02]\n [  6.06603250e-02]\n [  3.29031795e-03]\n [  2.17051655e-01]\n [  1.43875450e-01]\n [  7.52570555e-02]\n [  3.69870961e-02]\n [  1.38214985e-02]\n [  4.92538908e-04]\n [  3.24182063e-01]\n [  1.14288948e-01]\n [  6.58138320e-02]\n [  8.94029811e-02]\n [  8.92392620e-02]\n [  8.52274671e-02]\n [  1.24420598e-01]\n [  6.43100441e-02]\n [  8.06543231e-02]\n [  7.00695440e-02]\n [  4.81411349e-03]\n [  6.05165493e-03]\n [  1.18363008e-01]\n [  1.28598362e-01]\n [  9.47047994e-02]\n [  6.00570291e-02]\n [  1.17419370e-01]\n [  1.10655099e-01]\n [  5.42884208e-02]\n [  6.39285287e-03]\n [  1.37768477e-01]\n [  6.79602548e-02]\n [  1.13752834e-01]\n [  9.58139971e-02]\n [  6.95396811e-02]\n [  2.65660360e-02]\n [  3.89862098e-02]\n [  9.49230492e-02]\n [  6.58727288e-02]\n [  3.37156132e-02]\n [  5.55014946e-02]\n [  9.52351373e-03]\n [  1.00356638e-01]\n [  4.40344326e-02]\n [  3.58689316e-02]\n [  8.26008245e-03]\n [  3.22203562e-02]\n [  9.26771611e-02]\n [  8.13806131e-02]\n [  2.00734921e-02]\n [  3.60817425e-02]\n [  3.62855159e-02]\n [  3.77532244e-02]\n [  9.76570919e-02]\n [  1.35330394e-01]\n [  8.50525498e-02]\n [  5.86452261e-02]\n [  2.15370934e-02]\n [  4.78590280e-02]\n [  5.23634106e-02]\n [  9.20394361e-02]\n [  7.71324113e-02]\n [  3.20051849e-01]\n [  1.96172386e-01]]\n7 [[  5.61406929e-03]\n [  1.00549176e-01]\n [  5.53239882e-02]\n [  8.03587884e-02]\n [  6.77990541e-02]\n [  2.21735761e-02]\n [  9.05889049e-02]\n [  8.08222294e-02]\n [  1.01322271e-01]\n [  4.17100564e-02]\n [  3.17634828e-02]\n [  4.63713929e-02]\n [  4.78265323e-02]\n [  6.41352385e-02]\n [  6.27264101e-03]\n [  6.20809458e-02]\n [  4.77967300e-02]\n [  4.88894470e-02]\n [  2.37997267e-02]\n [  5.67564145e-02]\n [  5.99090159e-02]\n [  6.25562072e-02]\n [  4.96045795e-05]\n [  5.34174778e-03]\n [  5.53151481e-02]\n [  1.00164041e-01]\n [  6.81518987e-02]\n [  5.80561720e-02]\n [  2.81493925e-03]\n [  3.09267305e-02]\n [  1.26780167e-01]\n [  5.39422892e-02]\n [  3.96605283e-02]\n [  8.48629773e-02]\n [  7.00583309e-02]\n [  1.48632098e-03]\n [  5.51373810e-02]\n [  4.51001488e-02]\n [  3.58004011e-02]\n [  2.00131629e-03]\n [  1.44249514e-01]\n [  9.17952955e-02]\n [  4.83519994e-02]\n [  2.35576481e-02]\n [  7.94538483e-03]\n [  1.87397018e-04]\n [  2.11693227e-01]\n [  7.23210126e-02]\n [  4.21296246e-02]\n [  5.66451848e-02]\n [  5.61538972e-02]\n [  5.55309951e-02]\n [  8.37085471e-02]\n [  4.01871763e-02]\n [  5.26602492e-02]\n [  4.28996980e-02]\n [  2.03591352e-03]\n [  3.16081266e-03]\n [  7.88462609e-02]\n [  8.43774900e-02]\n [  6.18834049e-02]\n [  4.17120643e-02]\n [  7.36936629e-02]\n [  7.65794963e-02]\n [  3.50349396e-02]\n [  3.31687811e-03]\n [  8.80927593e-02]\n [  4.35692407e-02]\n [  7.41839856e-02]\n [  6.41775578e-02]\n [  4.62221950e-02]\n [  1.75768230e-02]\n [  2.48930026e-02]\n [  5.85978888e-02]\n [  4.17462550e-02]\n [  1.97490864e-02]\n [  3.69423404e-02]\n [  5.33651467e-03]\n [  6.89951256e-02]\n [  2.65724547e-02]\n [  2.13795956e-02]\n [  5.44419698e-03]\n [  2.05704402e-02]\n [  5.95035926e-02]\n [  5.15601970e-02]\n [  1.22728217e-02]\n [  2.04325095e-02]\n [  2.12234799e-02]\n [  2.49196570e-02]\n [  6.38702288e-02]\n [  8.92154500e-02]\n [  5.15983626e-02]\n [  3.38299721e-02]\n [  1.37547003e-02]\n [  3.09890676e-02]\n [  3.19978222e-02]\n [  5.77516444e-02]\n [  4.74846885e-02]\n [  2.09657609e-01]\n [  1.25032440e-01]]\n8 [[  3.22237704e-03]\n [  6.39947057e-02]\n [  3.57422121e-02]\n [  5.00551946e-02]\n [  4.62263338e-02]\n [  1.34095298e-02]\n [  5.88131063e-02]\n [  5.45595773e-02]\n [  6.57362714e-02]\n [  2.64047310e-02]\n [  2.19441596e-02]\n [  3.04386802e-02]\n [  3.17376219e-02]\n [  4.42710556e-02]\n [  3.97876790e-03]\n [  4.00267467e-02]\n [  3.17717120e-02]\n [  3.18731405e-02]\n [  1.44436648e-02]\n [  3.66883948e-02]\n [  3.72792780e-02]\n [  4.35544103e-02]\n [  7.09002563e-07]\n [  2.89392285e-03]\n [  3.73634920e-02]\n [  6.38621822e-02]\n [  4.71130498e-02]\n [  3.59929428e-02]\n [  2.09621363e-03]\n [  2.01593153e-02]\n [  8.41146931e-02]\n [  3.37161198e-02]\n [  2.57289745e-02]\n [  5.44321015e-02]\n [  4.37710658e-02]\n [  7.05813407e-04]\n [  3.47572044e-02]\n [  3.14908326e-02]\n [  2.10645348e-02]\n [  1.21465884e-03]\n [  9.59447026e-02]\n [  5.86422682e-02]\n [  3.10848355e-02]\n [  1.50112482e-02]\n [  4.53450764e-03]\n [  5.78934014e-05]\n [  1.38400972e-01]\n [  4.57635298e-02]\n [  2.69532856e-02]\n [  3.59089673e-02]\n [  3.53743471e-02]\n [  3.62103805e-02]\n [  5.63634075e-02]\n [  2.51155086e-02]\n [  3.44111174e-02]\n [  2.62422655e-02]\n [  7.61775882e-04]\n [  1.60750165e-03]\n [  5.25319763e-02]\n [  5.54288328e-02]\n [  4.04683985e-02]\n [  2.89386418e-02]\n [  4.62421291e-02]\n [  5.29582836e-02]\n [  2.26251725e-02]\n [  1.67305511e-03]\n [  5.64184822e-02]\n [  2.79439110e-02]\n [  4.84108590e-02]\n [  4.29994315e-02]\n [  3.07184812e-02]\n [  1.16321528e-02]\n [  1.58989541e-02]\n [  3.61857042e-02]\n [  2.64623780e-02]\n [  1.15114478e-02]\n [  2.45922748e-02]\n [  2.94976658e-03]\n [  4.73611169e-02]\n [  1.60257928e-02]\n [  1.26949679e-02]\n [  3.58525384e-03]\n [  1.31258145e-02]\n [  3.82444486e-02]\n [  3.26908007e-02]\n [  7.48637971e-03]\n [  1.14613036e-02]\n [  1.23483827e-02]\n [  1.64702144e-02]\n [  4.18181457e-02]\n [  5.88495582e-02]\n [  3.12393121e-02]\n [  1.93971470e-02]\n [  8.78452137e-03]\n [  2.00852100e-02]\n [  1.95312090e-02]\n [  3.62481773e-02]\n [  2.92205885e-02]\n [  1.37454599e-01]\n [  7.97690079e-02]]\n9 [[  1.83508615e-03]\n [  4.07667682e-02]\n [  2.31222827e-02]\n [  3.11996117e-02]\n [  3.15037332e-02]\n [  8.08532257e-03]\n [  3.82196493e-02]\n [  3.68278325e-02]\n [  4.26888801e-02]\n [  1.67282391e-02]\n [  1.51327774e-02]\n [  1.99876372e-02]\n [  2.10748482e-02]\n [  3.04979086e-02]\n [  2.51421798e-03]\n [  2.58233976e-02]\n [  2.11333539e-02]\n [  2.07931567e-02]\n [  8.74680560e-03]\n [  2.37326268e-02]\n [  2.32085008e-02]\n [  3.02795768e-02]\n [  1.04114997e-05]\n [  1.53955515e-03]\n [  2.52287332e-02]\n [  4.07650769e-02]\n [  3.25150862e-02]\n [  2.23025400e-02]\n [  1.54289487e-03]\n [  1.31462868e-02]\n [  5.58526963e-02]\n [  2.10975464e-02]\n [  1.67084709e-02]\n [  3.49270254e-02]\n [  2.73807403e-02]\n [  3.16575984e-04]\n [  2.19220575e-02]\n [  2.19335929e-02]\n [  1.23529397e-02]\n [  7.35543203e-04]\n [  6.38664663e-02]\n [  3.75116467e-02]\n [  1.99963469e-02]\n [  9.56996437e-03]\n [  2.56660860e-03]\n [  1.11478366e-05]\n [  9.05897617e-02]\n [  2.89574452e-02]\n [  1.72336977e-02]\n [  2.27758400e-02]\n [  2.23099738e-02]\n [  2.36304514e-02]\n [  3.79805975e-02]\n [  1.56978983e-02]\n [  2.25047488e-02]\n [  1.60376970e-02]\n [  2.28814315e-04]\n [  7.90036924e-04]\n [  3.50061692e-02]\n [  3.64549533e-02]\n [  2.64847223e-02]\n [  2.00562421e-02]\n [  2.90101487e-02]\n [  3.65979187e-02]\n [  1.46211600e-02]\n [  8.13644088e-04]\n [  3.61912958e-02]\n [  1.79298930e-02]\n [  3.16124298e-02]\n [  2.88182423e-02]\n [  2.04121098e-02]\n [  7.70005537e-03]\n [  1.01574846e-02]\n [  2.23536678e-02]\n [  1.67779587e-02]\n [  6.67303428e-03]\n [  1.63731799e-02]\n [  1.60412712e-03]\n [  3.24642919e-02]\n [  9.65954922e-03]\n [  7.50648556e-03]\n [  2.35922914e-03]\n [  8.37088842e-03]\n [  2.46066675e-02]\n [  2.07424425e-02]\n [  4.55535762e-03]\n [  6.35839952e-03]\n [  7.14215636e-03]\n [  1.08998707e-02]\n [  2.74094064e-02]\n [  3.88419703e-02]\n [  1.88714117e-02]\n [  1.10457018e-02]\n [  5.61037520e-03]\n [  1.30308429e-02]\n [  1.19074015e-02]\n [  2.27583423e-02]\n [  1.79734137e-02]\n [  9.01903063e-02]\n [  5.09421267e-02]]\n10 [[  1.03566749e-03]\n [  2.59938184e-02]\n [  1.49783744e-02]\n [  1.94601566e-02]\n [  2.14612484e-02]\n [  4.85915411e-03]\n [  2.48605553e-02]\n [  2.48571895e-02]\n [  2.77480409e-02]\n [  1.06060477e-02]\n [  1.04181673e-02]\n [  1.31297698e-02]\n [  1.40034799e-02]\n [  2.09707543e-02]\n [  1.58250693e-03]\n [  1.66704394e-02]\n [  1.40662286e-02]\n [  1.35738049e-02]\n [  5.28453058e-03]\n [  1.53626809e-02]\n [  1.44557133e-02]\n [  2.10219249e-02]\n [  3.31367119e-05]\n [  8.00949347e-04]\n [  1.70294493e-02]\n [  2.60527730e-02]\n [  2.24059559e-02]\n [  1.38114318e-02]\n [  1.12433743e-03]\n [  8.57666042e-03]\n [  3.71157974e-02]\n [  1.32170962e-02]\n [  1.08618308e-02]\n [  2.24199630e-02]\n [  1.71497948e-02]\n [  1.30668908e-04]\n [  1.38343954e-02]\n [  1.52421379e-02]\n [  7.21771317e-03]\n [  4.44347999e-04]\n [  4.25462238e-02]\n [  2.40267199e-02]\n [  1.28713036e-02]\n [  6.10405207e-03]\n [  1.43901852e-03]\n [  1.69478241e-07]\n [  5.93635850e-02]\n [  1.83222145e-02]\n [  1.10122506e-02]\n [  1.44537417e-02]\n [  1.40873874e-02]\n [  1.54329753e-02]\n [  2.56123729e-02]\n [  9.81261209e-03]\n [  1.47301741e-02]\n [  9.79140494e-03]\n [  4.10639914e-05]\n [  3.71009199e-04]\n [  2.33315770e-02]\n [  2.40039006e-02]\n [  1.73464566e-02]\n [  1.38870087e-02]\n [  1.81950517e-02]\n [  2.52754539e-02]\n [  9.45527758e-03]\n [  3.76749551e-04]\n [  2.32539698e-02]\n [  1.15093598e-02]\n [  2.06563044e-02]\n [  1.93195250e-02]\n [  1.35619184e-02]\n [  5.09857573e-03]\n [  6.49132254e-03]\n [  1.38143012e-02]\n [  1.06401173e-02]\n [  3.84437712e-03]\n [  1.09026292e-02]\n [  8.55255814e-04]\n [  2.22236495e-02]\n [  5.81897609e-03]\n [  4.41781152e-03]\n [  1.55135174e-03]\n [  5.33543481e-03]\n [  1.58489570e-02]\n [  1.31711969e-02]\n [  2.76442314e-03]\n [  3.48187960e-03]\n [  4.10333183e-03]\n [  7.22275302e-03]\n [  1.79845635e-02]\n [  2.56513078e-02]\n [  1.13723865e-02]\n [  6.24096952e-03]\n [  3.58323567e-03]\n [  8.46251845e-03]\n [  7.25010876e-03]\n [  1.42931230e-02]\n [  1.10500120e-02]\n [  5.92250861e-02]\n [  3.25653218e-02]]\n11 [[  5.78448235e-04]\n [  1.65897869e-02]\n [  9.71596316e-03]\n [  1.21466098e-02]\n [  1.46144722e-02]\n [  2.90979212e-03]\n [  1.61862299e-02]\n [  1.67766251e-02]\n [  1.80532839e-02]\n [  6.72979886e-03]\n [  7.16135930e-03]\n [  8.62808712e-03]\n [  9.31078102e-03]\n [  1.43950526e-02]\n [  9.91972513e-04]\n [  1.07684247e-02]\n [  9.36840195e-03]\n [  8.86685774e-03]\n [  3.18462006e-03]\n [  9.95159335e-03]\n [  9.00858734e-03]\n [  1.45762321e-02]\n [  5.17003973e-05]\n [  4.05161816e-04]\n [  1.14915036e-02]\n [  1.66704636e-02]\n [  1.54180042e-02]\n [  8.54768138e-03]\n [  8.12279060e-04]\n [  5.59788151e-03]\n [  2.46835891e-02]\n [  8.29044264e-03]\n [  7.06845708e-03]\n [  1.43970205e-02]\n [  1.07561173e-02]\n [  4.72546744e-05]\n [  8.73550028e-03]\n [  1.05700241e-02]\n [  4.20029741e-03]\n [  2.67759518e-04]\n [  2.83645671e-02]\n [  1.54100703e-02]\n [  8.29023402e-03]\n [  3.89535003e-03]\n [  7.97972374e-04]\n [  2.33449509e-06]\n [  3.89452614e-02]\n [  1.15921758e-02]\n [  7.03223050e-03]\n [  9.17747896e-03]\n [  8.90637748e-03]\n [  1.00871269e-02]\n [  1.72842126e-02]\n [  6.13439968e-03]\n [  9.64935403e-03]\n [  5.97143872e-03]\n [  2.15084299e-07]\n [  1.63523437e-04]\n [  1.55533664e-02]\n [  1.58236101e-02]\n [  1.13699855e-02]\n [  9.60700121e-03]\n [  1.14086606e-02]\n [  1.74453743e-02]\n [  6.11888804e-03]\n [  1.62763332e-04]\n [  1.49661684e-02]\n [  7.39109050e-03]\n [  1.35059794e-02]\n [  1.29553564e-02]\n [  9.00963694e-03]\n [  3.37702129e-03]\n [  4.14965814e-03]\n [  8.54066387e-03]\n [  6.74916804e-03]\n [  2.19928659e-03]\n [  7.26105087e-03]\n [  4.44954348e-04]\n [  1.51947020e-02]\n [  3.50340921e-03]\n [  2.58647022e-03]\n [  1.01944839e-03]\n [  3.39871971e-03]\n [  1.02191987e-02]\n [  8.37003253e-03]\n [  1.67270016e-03]\n [  1.87735679e-03]\n [  2.33954424e-03]\n [  4.79224930e-03]\n [  1.18130334e-02]\n [  1.69498567e-02]\n [  6.83507323e-03]\n [  3.49467667e-03]\n [  2.28862208e-03]\n [  5.50126890e-03]\n [  4.40828316e-03]\n [  8.97942856e-03]\n [  6.79004891e-03]\n [  3.89217027e-02]\n [  2.08388083e-02]]\n12 [[  3.19176615e-04]\n [  1.05979564e-02]\n [  6.31099008e-03]\n [  7.58732855e-03]\n [  9.94854886e-03]\n [  1.73555361e-03]\n [  1.05484156e-02]\n [  1.13223838e-02]\n [  1.17566315e-02]\n [  4.27368423e-03]\n [  4.91569098e-03]\n [  5.67196123e-03]\n [  6.19457103e-03]\n [  9.86556709e-03]\n [  6.19111874e-04]\n [  6.96029468e-03]\n [  6.24348596e-03]\n [  5.79592353e-03]\n [  1.91380386e-03]\n [  6.45093061e-03]\n [  5.61704952e-03]\n [  1.00950589e-02]\n [  6.17423866e-05]\n [  1.97650661e-04]\n [  7.75236310e-03]\n [  1.06801130e-02]\n [  1.05955610e-02]\n [  5.28638903e-03]\n [  5.82423119e-04]\n [  3.65529559e-03]\n [  1.64279435e-02]\n [  5.20689413e-03]\n [  4.60471213e-03]\n [  9.24849417e-03]\n [  6.75559416e-03]\n [  1.33923677e-05]\n [  5.51910140e-03]\n [  7.31599145e-03]\n [  2.43343436e-03]\n [  1.60922253e-04]\n [  1.89238247e-02]\n [  9.89698339e-03]\n [  5.34296827e-03]\n [  2.48714676e-03]\n [  4.36812465e-04]\n [  7.55642213e-06]\n [  2.55784113e-02]\n [  7.33346632e-03]\n [  4.48759552e-03]\n [  5.83050540e-03]\n [  5.63805411e-03]\n [  6.59815362e-03]\n [  1.16720954e-02]\n [  3.83531186e-03]\n [  6.32621162e-03]\n [  3.63751268e-03]\n [  1.18966618e-05]\n [  6.55742333e-05]\n [  1.03701660e-02]\n [  1.04428576e-02]\n [  7.45830964e-03]\n [  6.64070435e-03]\n [  7.15118600e-03]\n [  1.20341768e-02]\n [  3.96258757e-03]\n [  6.32778974e-05]\n [  9.64830443e-03]\n [  4.74843429e-03]\n [  8.83640442e-03]\n [  8.69013648e-03]\n [  5.98484417e-03]\n [  2.23745732e-03]\n [  2.65354547e-03]\n [  5.28263161e-03]\n [  4.28201724e-03]\n [  1.24813546e-03]\n [  4.83661098e-03]\n [  2.24404153e-04]\n [  1.03770420e-02]\n [  2.10812874e-03]\n [  1.50540681e-03]\n [  6.69519184e-04]\n [  2.16370169e-03]\n [  6.59635663e-03]\n [  5.32321632e-03]\n [  1.00889604e-03]\n [  9.93401278e-04]\n [  1.32228376e-03]\n [  3.18362983e-03]\n [  7.76745146e-03]\n [  1.12063885e-02]\n [  4.09605308e-03]\n [  1.93659216e-03]\n [  1.46180554e-03]\n [  3.57984542e-03]\n [  2.67634448e-03]\n [  5.64292446e-03]\n [  4.17006668e-03]\n [  2.55981199e-02]\n [  1.33485505e-02]]\n13 [[  1.73604785e-04]\n [  6.77669607e-03]\n [  4.10485920e-03]\n [  4.74308617e-03]\n [  6.77010277e-03]\n [  1.03062496e-03]\n [  6.88070618e-03]\n [  7.64110405e-03]\n [  7.66319456e-03]\n [  2.71621789e-03]\n [  3.36980936e-03]\n [  3.73005331e-03]\n [  4.12390847e-03]\n [  6.75136223e-03]\n [  3.84628773e-04]\n [  4.50166129e-03]\n [  4.16351669e-03]\n [  3.79105005e-03]\n [  1.14660407e-03]\n [  4.18462558e-03]\n [  3.50432401e-03]\n [  6.98392931e-03]\n [  6.40170474e-05]\n [  9.18374790e-05]\n [  5.22858044e-03]\n [  6.85085170e-03]\n [  7.27263559e-03]\n [  3.26698157e-03]\n [  4.14853654e-04]\n [  2.38789851e-03]\n [  1.09414700e-02]\n [  3.27466126e-03]\n [  3.00287642e-03]\n [  5.94329135e-03]\n [  4.24921792e-03]\n [  2.06338314e-06]\n [  3.48905264e-03]\n [  5.05480310e-03]\n [  1.40281883e-03]\n [  9.64419232e-05]\n [  1.26342224e-02]\n [  6.36492530e-03]\n [  3.44566652e-03]\n [  1.58886600e-03]\n [  2.35463216e-04]\n [  1.19290853e-05]\n [  1.68178100e-02]\n [  4.63877572e-03]\n [  2.86171772e-03]\n [  3.70621122e-03]\n [  3.57379974e-03]\n [  4.31929203e-03]\n [  7.88741373e-03]\n [  2.39811069e-03]\n [  4.15089913e-03]\n [  2.21301150e-03]\n [  3.46964516e-05]\n [  2.24987998e-05]\n [  6.91555673e-03]\n [  6.89945091e-03]\n [  4.89608617e-03]\n [  4.58684238e-03]\n [  4.48092818e-03]\n [  8.29702523e-03]\n [  2.56800512e-03]\n [  2.05485630e-05]\n [  6.23051636e-03]\n [  3.05193802e-03]\n [  5.78492181e-03]\n [  5.83076756e-03]\n [  3.97524051e-03]\n [  1.48292736e-03]\n [  1.69737067e-03]\n [  3.26903164e-03]\n [  2.71731149e-03]\n [  7.01847544e-04]\n [  3.22225271e-03]\n [  1.08653687e-04]\n [  7.07938150e-03]\n [  1.26784865e-03]\n [  8.70396732e-04]\n [  4.39464406e-04]\n [  1.37659267e-03]\n [  4.26249439e-03]\n [  3.38820345e-03]\n [  6.06400776e-04]\n [  5.13603853e-04]\n [  7.39805400e-04]\n [  2.11761077e-03]\n [  5.11267502e-03]\n [  7.41317822e-03]\n [  2.44672853e-03]\n [  1.06015150e-03]\n [  9.33742675e-04]\n [  2.33188411e-03]\n [  1.62222935e-03]\n [  3.54727032e-03]\n [  2.55951611e-03]\n [  1.68480985e-02]\n [  8.55930615e-03]]\n14 [[  9.28143054e-05]\n [  4.33742441e-03]\n [  2.67356285e-03]\n [  2.96747591e-03]\n [  4.60578641e-03]\n [  6.09024486e-04]\n [  4.49242629e-03]\n [  5.15661528e-03]\n [  4.99957940e-03]\n [  1.72781874e-03]\n [  2.30729720e-03]\n [  2.45391345e-03]\n [  2.74710124e-03]\n [  4.61388007e-03]\n [  2.37789136e-04]\n [  2.91332207e-03]\n [  2.77817901e-03]\n [  2.48131435e-03]\n [  6.84657542e-04]\n [  2.71640695e-03]\n [  2.18753982e-03]\n [  4.82674688e-03]\n [  6.08107257e-05]\n [  3.98369884e-05]\n [  3.52563988e-03]\n [  4.40006796e-03]\n [  4.98623680e-03]\n [  2.01734947e-03]\n [  2.93771649e-04]\n [  1.56065670e-03]\n [  7.29251048e-03]\n [  2.06237682e-03]\n [  1.96033437e-03]\n [  3.82065494e-03]\n [  2.67681805e-03]\n [  3.57833918e-09]\n [  2.20703986e-03]\n [  3.48682236e-03]\n [  8.04218405e-04]\n [  5.76266793e-05]\n [  8.44088942e-03]\n [  4.09905659e-03]\n [  2.22351309e-03]\n [  1.01557735e-03]\n [  1.24591199e-04]\n [  1.43968073e-05]\n [  1.10696899e-02]\n [  2.93382467e-03]\n [  1.82354287e-03]\n [  2.35721702e-03]\n [  2.26842845e-03]\n [  2.82967743e-03]\n [  5.33329649e-03]\n [  1.49960304e-03]\n [  2.72578932e-03]\n [  1.34454330e-03]\n [  5.29882163e-05]\n [  5.67638654e-06]\n [  4.61267540e-03]\n [  4.56336187e-03]\n [  3.21650598e-03]\n [  3.16600478e-03]\n [  2.80665257e-03]\n [  5.71761746e-03]\n [  1.66542607e-03]\n [  4.56837142e-06]\n [  4.03027888e-03]\n [  1.96240400e-03]\n [  3.78958229e-03]\n [  3.91334621e-03]\n [  2.64024409e-03]\n [  9.83183854e-04]\n [  1.08609663e-03]\n [  2.02401122e-03]\n [  1.72473607e-03]\n [  3.90462810e-04]\n [  2.14714254e-03]\n [  4.97588480e-05]\n [  4.82491823e-03]\n [  7.62107607e-04]\n [  4.99459682e-04]\n [  2.88318930e-04]\n [  8.75245605e-04]\n [  2.75740772e-03]\n [  2.15835962e-03]\n [  3.63087020e-04]\n [  2.57873646e-04]\n [  4.09044849e-04]\n [  1.41026895e-03]\n [  3.36871366e-03]\n [  4.90660826e-03]\n [  1.45633414e-03]\n [  5.72018675e-04]\n [  5.96474914e-04]\n [  1.52052671e-03]\n [  9.81569756e-04]\n [  2.23060581e-03]\n [  1.56999531e-03]\n [  1.10971592e-02]\n [  5.49407769e-03]]\n15 [[  4.85912060e-05]\n [  2.77887285e-03]\n [  1.74371165e-03]\n [  1.85815874e-03]\n [  3.13252350e-03]\n [  3.57921002e-04]\n [  2.93580862e-03]\n [  3.47991008e-03]\n [  3.26476898e-03]\n [  1.10004237e-03]\n [  1.57804671e-03]\n [  1.61497784e-03]\n [  1.83107657e-03]\n [  3.14912410e-03]\n [  1.46237086e-04]\n [  1.88658282e-03]\n [  1.85492064e-03]\n [  1.62513449e-03]\n [  4.07310785e-04]\n [  1.76456943e-03]\n [  1.36639620e-03]\n [  3.33275367e-03]\n [  5.44212526e-05]\n [  1.55688012e-05]\n [  2.37686187e-03]\n [  2.82959640e-03]\n [  3.41507979e-03]\n [  1.24461378e-03]\n [  2.06952493e-04]\n [  1.02046714e-03]\n [  4.86384192e-03]\n [  1.30078790e-03]\n [  1.28109229e-03]\n [  2.45697564e-03]\n [  1.68896129e-03]\n [  1.05725246e-06]\n [  1.39694568e-03]\n [  2.40162411e-03]\n [  4.58183436e-04]\n [  3.43258507e-05]\n [  5.64309582e-03]\n [  2.64349882e-03]\n [  1.43575727e-03]\n [  6.49509078e-04]\n [  6.44370230e-05]\n [  1.50769811e-05]\n [  7.29387812e-03]\n [  1.85521645e-03]\n [  1.16109091e-03]\n [  1.50008325e-03]\n [  1.44187803e-03]\n [  1.85522472e-03]\n [  3.60845542e-03]\n [  9.37829900e-04]\n [  1.79139932e-03]\n [  8.15699168e-04]\n [  6.28651178e-05]\n [  5.72079102e-07]\n [  3.07723740e-03]\n [  3.02147842e-03]\n [  2.11467571e-03]\n [  2.18387949e-03]\n [  1.75719801e-03]\n [  3.93826328e-03]\n [  1.08086993e-03]\n [  2.48250217e-07]\n [  2.61148927e-03]\n [  1.26236689e-03]\n [  2.48401333e-03]\n [  2.62720580e-03]\n [  1.75349007e-03]\n [  6.52092858e-04]\n [  6.95193419e-04]\n [  1.25386298e-03]\n [  1.09494943e-03]\n [  2.14523636e-04]\n [  1.43102731e-03]\n [  2.10211674e-05]\n [  3.28537729e-03]\n [  4.57881397e-04]\n [  2.84134498e-04]\n [  1.89074432e-04]\n [  5.56112092e-04]\n [  1.78572850e-03]\n [  1.37606543e-03]\n [  2.16487839e-04]\n [  1.24626327e-04]\n [  2.23020543e-04]\n [  9.40337428e-04]\n [  2.22188537e-03]\n [  3.24928993e-03]\n [  8.63416295e-04]\n [  3.03310429e-04]\n [  3.81056336e-04]\n [  9.92493704e-04]\n [  5.92800556e-04]\n [  1.40309031e-03]\n [  9.62374907e-04]\n [  7.31448270e-03]\n [  3.53019382e-03]]\n16 [[  2.47808530e-05]\n [  1.78208726e-03]\n [  1.13879598e-03]\n [  1.16455753e-03]\n [  2.12998246e-03]\n [  2.09055506e-04]\n [  1.92029262e-03]\n [  2.34838133e-03]\n [  2.13383115e-03]\n [  7.00983685e-04]\n [  1.07817526e-03]\n [  1.06325268e-03]\n [  1.22123375e-03]\n [  2.14684661e-03]\n [  8.94247860e-05]\n [  1.22244051e-03]\n [  1.23921630e-03]\n [  1.06506329e-03]\n [  2.41326503e-04]\n [  1.14705611e-03]\n [  8.54031881e-04]\n [  2.29918235e-03]\n [  4.66456549e-05]\n [  5.09475467e-06]\n [  1.60211104e-03]\n [  1.82198745e-03]\n [  2.33672210e-03]\n [  7.67139136e-04]\n [  1.45110796e-04]\n [  6.67560031e-04]\n [  3.24619212e-03]\n [  8.21680529e-04]\n [  8.38081760e-04]\n [  1.58055557e-03]\n [  1.06741744e-03]\n [  2.61739660e-06]\n [  8.84754059e-04]\n [  1.65188254e-03]\n [  2.59198539e-04]\n [  2.03770815e-05]\n [  3.77507182e-03]\n [  1.70717994e-03]\n [  9.27682384e-04]\n [  4.15627088e-04]\n [  3.23799250e-05]\n [  1.44612895e-05]\n [  4.81093768e-03]\n [  1.17291696e-03]\n [  7.38680072e-04]\n [  9.55163734e-04]\n [  9.17815370e-04]\n [  1.21725770e-03]\n [  2.44285236e-03]\n [  5.86552487e-04]\n [  1.17824681e-03]\n [  4.94077743e-04]\n [  6.51278460e-05]\n [  1.09795565e-07]\n [  2.05329177e-03]\n [  2.00266694e-03]\n [  1.39130536e-03]\n [  1.50551042e-03]\n [  1.09962479e-03]\n [  2.71146838e-03]\n [  7.01999117e-04]\n [  3.45449450e-07]\n [  1.69504166e-03]\n [  8.12395883e-04]\n [  1.62922766e-03]\n [  1.76424102e-03]\n [  1.16451201e-03]\n [  4.32654662e-04]\n [  4.45129379e-04]\n [  7.77220004e-04]\n [  6.95273862e-04]\n [  1.16118994e-04]\n [  9.53943934e-04]\n [  7.81805102e-06]\n [  2.23517534e-03]\n [  2.74969439e-04]\n [  1.60030075e-04]\n [  1.23941107e-04]\n [  3.53091251e-04]\n [  1.15773967e-03]\n [  8.78060819e-04]\n [  1.28476633e-04]\n [  5.71949276e-05]\n [  1.19568416e-04]\n [  6.27735280e-04]\n [  1.46694412e-03]\n [  2.15290464e-03]\n [  5.09647711e-04]\n [  1.57432412e-04]\n [  2.43455652e-04]\n [  6.48491085e-04]\n [  3.57271754e-04]\n [  8.82848864e-04]\n [  5.89484815e-04]\n [  4.82457364e-03]\n [  2.27066362e-03]]\n17 [[  1.22218717e-05]\n [  1.14397693e-03]\n [  7.44742109e-04]\n [  7.30522501e-04]\n [  1.44796818e-03]\n [  1.21255347e-04]\n [  1.25718804e-03]\n [  1.58478215e-03]\n [  1.39589084e-03]\n [  4.47097351e-04]\n [  7.35950598e-04]\n [  7.00281933e-04]\n [  8.14985717e-04]\n [  1.46195141e-03]\n [  5.43467031e-05]\n [  7.92593579e-04]\n [  8.28369753e-04]\n [  6.98466087e-04]\n [  1.42333738e-04]\n [  7.46163714e-04]\n [  5.34151855e-04]\n [  1.58487319e-03]\n [  3.86929350e-05]\n [  1.15046544e-06]\n [  1.07972894e-03]\n [  1.17469439e-03]\n [  1.59743440e-03]\n [  4.72352200e-04]\n [  1.01327889e-04]\n [  4.36904142e-04]\n [  2.16796692e-03]\n [  5.19864669e-04]\n [  5.48847136e-04]\n [  1.01710658e-03]\n [  6.75763178e-04]\n [  3.76170419e-06]\n [  5.60720742e-04]\n [  1.13474461e-03]\n [  1.45456797e-04]\n [  1.20532550e-05]\n [  2.52701412e-03]\n [  1.10405032e-03]\n [  5.99786872e-04]\n [  2.66124232e-04]\n [  1.56750975e-05]\n [  1.30649341e-05]\n [  3.17645585e-03]\n [  7.41388474e-04]\n [  4.69541701e-04]\n [  6.08544447e-04]\n [  5.85093629e-04]\n [  7.99276808e-04]\n [  1.65468617e-03]\n [  3.66878463e-04]\n [  7.75577792e-04]\n [  2.98753555e-04]\n [  6.19910570e-05]\n [  1.05671313e-06]\n [  1.37032627e-03]\n [  1.32875575e-03]\n [  9.16048011e-04]\n [  1.03728543e-03]\n [  6.87770313e-04]\n [  1.86606834e-03]\n [  4.56270645e-04]\n [  1.62116703e-06]\n [  1.10209920e-03]\n [  5.23044146e-04]\n [  1.06923166e-03]\n [  1.18506560e-03]\n [  7.73343141e-04]\n [  2.87174364e-04]\n [  2.85111077e-04]\n [  4.82077041e-04]\n [  4.41573124e-04]\n [  6.17345213e-05]\n [  6.36049081e-04]\n [  2.30624005e-06]\n [  1.51947071e-03]\n [  1.65056292e-04]\n [  8.90855590e-05]\n [  8.12197177e-05]\n [  2.24022879e-04]\n [  7.51424814e-04]\n [  5.60770393e-04]\n [  7.58508613e-05]\n [  2.43775503e-05]\n [  6.28031339e-05]\n [  4.19544202e-04]\n [  9.69470537e-04]\n [  1.42719864e-03]\n [  2.99353851e-04]\n [  7.95613305e-05]\n [  1.55556976e-04]\n [  4.24164900e-04]\n [  2.14842221e-04]\n [  5.55678154e-04]\n [  3.60795471e-04]\n [  3.18443705e-03]\n [  1.46205351e-03]]\n18 [[  5.76626235e-06]\n [  7.35086796e-04]\n [  4.87696641e-04]\n [  4.58693918e-04]\n [  9.84134967e-04]\n [  6.97749128e-05]\n [  8.23797833e-04]\n [  1.06949371e-03]\n [  9.13959462e-04]\n [  2.85431626e-04]\n [  5.01915871e-04]\n [  4.61393967e-04]\n [  5.44191978e-04]\n [  9.94537724e-04]\n [  3.28059614e-05]\n [  5.14209212e-04]\n [  5.54056198e-04]\n [  4.58346767e-04]\n [  8.35241735e-05]\n [  4.85718920e-04]\n [  3.34319891e-04]\n [  1.09166361e-03]\n [  3.12836710e-05]\n [  6.76844536e-08]\n [  7.27566425e-04]\n [  7.58336915e-04]\n [  1.09113427e-03]\n [  2.90516473e-04]\n [  7.04913109e-05]\n [  2.86082912e-04]\n [  1.44879578e-03]\n [  3.29451781e-04]\n [  3.59810743e-04]\n [  6.54732983e-04]\n [  4.28568048e-04]\n [  4.30653472e-06]\n [  3.55590135e-04]\n [  7.78584566e-04]\n [  8.08784389e-05]\n [  7.10232644e-06]\n [  1.69259252e-03]\n [  7.15011149e-04]\n [  3.88039392e-04]\n [  1.70501269e-04]\n [  7.21600418e-06]\n [  1.13018414e-05]\n [  2.09937175e-03]\n [  4.68500773e-04]\n [  2.98190047e-04]\n [  3.87937936e-04]\n [  3.73550312e-04]\n [  5.25218784e-04]\n [  1.12140703e-03]\n [  2.29495723e-04]\n [  5.10923041e-04]\n [  1.80311748e-04]\n [  5.56846571e-05]\n [  2.08556639e-06]\n [  9.14709177e-04]\n [  8.82508815e-04]\n [  6.03571883e-04]\n [  7.14315160e-04]\n [  4.29923501e-04]\n [  1.28375622e-03]\n [  2.96775484e-04]\n [  2.79791539e-06]\n [  7.17798539e-04]\n [  3.36895115e-04]\n [  7.02140620e-04]\n [  7.96243839e-04]\n [  5.13564970e-04]\n [  1.90687540e-04]\n [  1.82683463e-04]\n [  2.99215288e-04]\n [  2.80500273e-04]\n [  3.21042971e-05]\n [  4.24182595e-04]\n [  3.88291994e-07]\n [  1.03217468e-03]\n [  9.90417684e-05]\n [  4.89128288e-05]\n [  5.32074700e-05]\n [  1.42025572e-04]\n [  4.88248566e-04]\n [  3.58451653e-04]\n [  4.45232290e-05]\n [  9.26846042e-06]\n [  3.21551088e-05]\n [  2.80720735e-04]\n [  6.41327293e-04]\n [  9.46600223e-04]\n [  1.74867397e-04]\n [  3.88494082e-05]\n [  9.94056027e-05]\n [  2.77721614e-04]\n [  1.28877989e-04]\n [  3.49865819e-04]\n [  2.20642803e-04]\n [  2.10328866e-03]\n [  9.42379760e-04]]\n19 [[  2.55825580e-06]\n [  4.72808344e-04]\n [  3.19795392e-04]\n [  2.88297073e-04]\n [  6.68755616e-04]\n [  3.97858457e-05]\n [  5.40283509e-04]\n [  7.21764402e-04]\n [  5.98932384e-04]\n [  1.82394855e-04]\n [  3.42029292e-04]\n [  3.04114830e-04]\n [  3.63586907e-04]\n [  6.75921794e-04]\n [  1.96556120e-05]\n [  3.33807431e-04]\n [  3.70790629e-04]\n [  3.00969754e-04]\n [  4.87369034e-05]\n [  3.16400372e-04]\n [  2.09397593e-04]\n [  7.51419575e-04]\n [  2.47763419e-05]\n [  7.81325156e-08]\n [  4.90210310e-04]\n [  4.90189181e-04]\n [  7.44721212e-04]\n [  1.78462185e-04]\n [  4.88734840e-05]\n [  1.87413345e-04]\n [  9.68787761e-04]\n [  2.09135484e-04]\n [  2.36128064e-04]\n [  4.21595847e-04]\n [  2.72293924e-04]\n [  4.34799676e-06]\n [  2.25654512e-04]\n [  5.33625192e-04]\n [  4.44901452e-05]\n [  4.16794455e-06]\n [  1.13435264e-03]\n [  4.63711680e-04]\n [  2.51206482e-04]\n [  1.09305794e-04]\n [  3.09275515e-06]\n [  9.46063301e-06]\n [  1.38883770e-03]\n [  2.95974285e-04]\n [  1.89190512e-04]\n [  2.47450895e-04]\n [  2.38861685e-04]\n [  3.45385051e-04]\n [  7.60381517e-04]\n [  1.43567464e-04]\n [  3.36839061e-04]\n [  1.08604982e-04]\n [  4.79557748e-05]\n [  2.76277251e-06]\n [  6.10701332e-04]\n [  5.86704933e-04]\n [  3.97968601e-04]\n [  4.91670449e-04]\n [  2.68577045e-04]\n [  8.82843218e-04]\n [  1.93178013e-04]\n [  3.50122741e-06]\n [  4.68298502e-04]\n [  2.17087421e-04]\n [  4.61353018e-04]\n [  5.35144121e-04]\n [  3.41045816e-04]\n [  1.26672385e-04]\n [  1.17095260e-04]\n [  1.85849931e-04]\n [  1.78217670e-04]\n [  1.62396609e-05]\n [  2.82954163e-04]\n [  1.19237023e-10]\n [  7.00670644e-04]\n [  5.94114426e-05]\n [  2.64158680e-05]\n [  3.48480098e-05]\n [  8.99689621e-05]\n [  3.17599188e-04]\n [  2.29328187e-04]\n [  2.59644221e-05]\n [  2.88390993e-06]\n [  1.59344236e-05]\n [  1.88041857e-04]\n [  4.24656086e-04]\n [  6.28151058e-04]\n [  1.01513251e-04]\n [  1.81205214e-05]\n [  6.35300094e-05]\n [  1.82029238e-04]\n [  7.71051928e-05]\n [  2.20351067e-04]\n [  1.34810529e-04]\n [  1.39010325e-03]\n [  6.08055212e-04]]\n20 [[  1.03612911e-06]\n [  3.04414250e-04]\n [  2.09974853e-04]\n [  1.81383904e-04]\n [  4.54365916e-04]\n [  2.24463565e-05]\n [  3.54648422e-04]\n [  4.87102894e-04]\n [  3.92829505e-04]\n [  1.16664291e-04]\n [  2.32899038e-04]\n [  2.00523529e-04]\n [  2.43057031e-04]\n [  4.58967639e-04]\n [  1.16784358e-05]\n [  2.16826142e-04]\n [  2.48282682e-04]\n [  1.97756584e-04]\n [  2.82554956e-05]\n [  2.06246550e-04]\n [  1.31253342e-04]\n [  5.16885659e-04]\n [  1.92932603e-05]\n [  3.86392685e-07]\n [  3.30248498e-04]\n [  3.17269529e-04]\n [  5.07922494e-04]\n [  1.09483342e-04]\n [  3.37807433e-05]\n [  1.22833575e-04]\n [  6.48199697e-04]\n [  1.32989691e-04]\n [  1.55121932e-04]\n [  2.71557947e-04]\n [  1.73327193e-04]\n [  4.05328865e-06]\n [  1.43293350e-04]\n [  3.65363871e-04]\n [  2.41689359e-05]\n [  2.43492855e-06]\n [  7.60660332e-04]\n [  3.01158405e-04]\n [  1.62729266e-04]\n [  7.01186800e-05]\n [  1.18810453e-06]\n [  7.71771283e-06]\n [  9.19648039e-04]\n [  1.86920166e-04]\n [  1.19912795e-04]\n [  1.57930568e-04]\n [  1.52977140e-04]\n [  2.27293480e-04]\n [  5.15836757e-04]\n [  8.98179569e-05]\n [  2.22240764e-04]\n [  6.52710805e-05]\n [  4.00008830e-05]\n [  3.03832167e-06]\n [  4.07807500e-04]\n [  3.90425150e-04]\n [  2.62587360e-04]\n [  3.38270591e-04]\n [  1.67666454e-04]\n [  6.06922258e-04]\n [  1.25838691e-04]\n [  3.72986506e-06]\n [  3.06040427e-04]\n [  1.39947690e-04]\n [  3.03317065e-04]\n [  3.59756465e-04]\n [  2.26480435e-04]\n [  8.41826404e-05]\n [  7.50820400e-05]\n [  1.15523711e-04]\n [  1.13251241e-04]\n [  7.92507672e-06]\n [  1.88789330e-04]\n [  1.54568681e-07]\n [  4.75329114e-04]\n [  3.56272503e-05]\n [  1.39820777e-05]\n [  2.28190675e-05]\n [  5.69449330e-05]\n [  2.06822180e-04]\n [  1.46849532e-04]\n [  1.50305441e-05]\n [  5.74534226e-07]\n [  7.56205918e-06]\n [  1.26097730e-04]\n [  2.81449815e-04]\n [  4.17037867e-04]\n [  5.85151101e-05]\n [  7.92776154e-06]\n [  4.06069266e-05]\n [  1.19430806e-04]\n [  4.59962648e-05]\n [  1.38824005e-04]\n [  8.22876609e-05]\n [  9.19324171e-04]\n [  3.92750138e-04]]\n21 [[  3.61723721e-07]\n [  1.96190609e-04]\n [  1.38052012e-04]\n [  1.14239861e-04]\n [  3.08658637e-04]\n [  1.25075803e-05]\n [  2.32993669e-04]\n [  3.28742462e-04]\n [  2.57864478e-04]\n [  7.46941587e-05]\n [  1.58481169e-04]\n [  1.32267014e-04]\n [  1.62573589e-04]\n [  3.11395706e-04]\n [  6.87392730e-06]\n [  1.40926393e-04]\n [  1.66340353e-04]\n [  1.30019442e-04]\n [  1.62627275e-05]\n [  1.34536080e-04]\n [  8.23361115e-05]\n [  3.55338416e-04]\n [  1.48126428e-05]\n [  6.75873025e-07]\n [  2.22465445e-04]\n [  2.05614269e-04]\n [  3.46183078e-04]\n [  6.70692680e-05]\n [  2.32840375e-05]\n [  8.05460077e-05]\n [  4.33950947e-04]\n [  8.47189949e-05]\n [  1.02009661e-04]\n [  1.74965768e-04]\n [  1.10541630e-04]\n [  3.57540421e-06]\n [  9.10550007e-05]\n [  2.49924837e-04]\n [  1.29354730e-05]\n [  1.41569046e-06]\n [  5.10354119e-04]\n [  1.95862129e-04]\n [  1.05481216e-04]\n [  4.50100888e-05]\n [  3.77786137e-07]\n [  6.16676107e-06]\n [  6.09523500e-04]\n [  1.18005162e-04]\n [  7.59206450e-05]\n [  1.00855163e-04]\n [  9.81308403e-05]\n [  1.49687578e-04]\n [  3.50097747e-04]\n [  5.61953675e-05]\n [  1.46740917e-04]\n [  3.91328649e-05]\n [  3.25391266e-05]\n [  2.99872181e-06]\n [  2.72372621e-04]\n [  2.60052911e-04]\n [  1.73382447e-04]\n [  2.32634222e-04]\n [  1.04591789e-04]\n [  4.17107978e-04]\n [  8.20335190e-05]\n [  3.60712170e-06]\n [  2.00339913e-04]\n [  9.02559404e-05]\n [  1.99530812e-04]\n [  2.41913542e-04]\n [  1.50402513e-04]\n [  5.59696819e-05]\n [  4.81607203e-05]\n [  7.18679585e-05]\n [  7.19795717e-05]\n [  3.68615838e-06]\n [  1.25991734e-04]\n [  4.17094128e-07]\n [  3.22269654e-04]\n [  2.13609583e-05]\n [  7.21779770e-06]\n [  1.49399111e-05]\n [  3.60102313e-05]\n [  1.34832677e-04]\n [  9.41216931e-05]\n [  8.62783054e-06]\n [  1.36258311e-08]\n [  3.38040854e-06]\n [  8.46496696e-05]\n [  1.86711608e-04]\n [  2.77010084e-04]\n [  3.34561628e-05]\n [  3.15156512e-06]\n [  2.59590761e-05]\n [  7.84407457e-05]\n [  2.73503811e-05]\n [  8.74869729e-05]\n [  5.01761351e-05]\n [  6.08346832e-04]\n [  2.53940700e-04]]\n22 [[  9.47985583e-08]\n [  1.26567218e-04]\n [  9.08803631e-05]\n [  7.20297467e-05]\n [  2.09646081e-04]\n [  6.86805288e-06]\n [  1.53201341e-04]\n [  2.21876951e-04]\n [  1.69412160e-04]\n [  4.78712755e-05]\n [  1.07773725e-04]\n [  8.72783639e-05]\n [  1.08799868e-04]\n [  2.11106788e-04]\n [  4.00333101e-06]\n [  9.16511271e-05]\n [  1.11502268e-04]\n [  8.55390754e-05]\n [  9.28298232e-06]\n [  8.78173305e-05]\n [  5.16925138e-05]\n [  2.44143594e-04]\n [  1.12365324e-05]\n [  8.51954269e-07]\n [  1.49846310e-04]\n [  1.33425579e-04]\n [  2.35799904e-04]\n [  4.10224457e-05]\n [  1.60087438e-05]\n [  5.28425335e-05]\n [  2.90682307e-04]\n [  5.40697001e-05]\n [  6.71520756e-05]\n [  1.12764610e-04]\n [  7.06411229e-05]\n [  3.02893682e-06]\n [  5.79010193e-05]\n [  1.70811487e-04]\n [  6.79974346e-06]\n [  8.18876060e-07]\n [  3.42593907e-04]\n [  1.27560517e-04]\n [  6.84177721e-05]\n [  2.89111740e-05]\n [  7.98748445e-08]\n [  4.84402017e-06]\n [  4.04340622e-04]\n [  7.44684949e-05]\n [  4.80139061e-05]\n [  6.44462125e-05]\n [  6.30497525e-05]\n [  9.86511295e-05]\n [  2.37716347e-04]\n [  3.51608760e-05]\n [  9.69628891e-05]\n [  2.33996871e-05]\n [  2.59401295e-05]\n [  2.75722709e-06]\n [  1.81954616e-04]\n [  1.73372406e-04]\n [  1.14558105e-04]\n [  1.59925126e-04]\n [  6.51894370e-05]\n [  2.86567054e-04]\n [  5.35185245e-05]\n [  3.26981103e-06]\n [  1.31364810e-04]\n [  5.82338580e-05]\n [  1.31332024e-04]\n [  1.62717100e-04]\n [  9.98825417e-05]\n [  3.72284594e-05]\n [  3.09037132e-05]\n [  4.47475068e-05]\n [  4.57559800e-05]\n [  1.60227978e-06]\n [  8.41030414e-05]\n [  6.23685651e-07]\n [  2.18373607e-04]\n [  1.28061456e-05]\n [  3.60857075e-06]\n [  9.77978561e-06]\n [  2.27514684e-05]\n [  8.79961517e-05]\n [  6.03812659e-05]\n [  4.90497496e-06]\n [  9.35401587e-08]\n [  1.38404084e-06]\n [  5.68837750e-05]\n [  1.23973485e-04]\n [  1.84083401e-04]\n [  1.89515049e-05]\n [  1.06929883e-06]\n [  1.65974870e-05]\n [  5.15705397e-05]\n [  1.62058586e-05]\n [  5.51536177e-05]\n [  3.05605972e-05]\n [  4.02815611e-04]\n [  1.64363533e-04]]\n23 [[  1.10850671e-08]\n [  8.17349282e-05]\n [  5.99049672e-05]\n [  4.54661422e-05]\n [  1.42379256e-04]\n [  3.70522548e-06]\n [  1.00818783e-04]\n [  1.49757601e-04]\n [  1.11391528e-04]\n [  3.07126211e-05]\n [  7.32481276e-05]\n [  5.76140083e-05]\n [  7.28511732e-05]\n [  1.43014942e-04]\n [  2.30283786e-06]\n [  5.96410100e-05]\n [  7.47832019e-05]\n [  5.63112408e-05]\n [  5.24829829e-06]\n [  5.73623729e-05]\n [  3.24803987e-05]\n [  1.67656573e-04]\n [  8.43632006e-06]\n [  9.12043049e-07]\n [  1.00927966e-04]\n [  8.66948831e-05]\n [  1.60522672e-04]\n [  2.50488520e-05]\n [  1.09811372e-05]\n [  3.46852175e-05]\n [  1.94817141e-04]\n [  3.45740991e-05]\n [  4.42512937e-05]\n [  7.26965955e-05]\n [  4.52323766e-05]\n [  2.48789343e-06]\n [  3.68454057e-05]\n [  1.16646785e-04]\n [  3.49676770e-06]\n [  4.70991068e-07]\n [  2.30097139e-04]\n [  8.31933066e-05]\n [  4.44062207e-05]\n [  1.85838071e-05]\n [  3.16058735e-09]\n [  3.75098375e-06]\n [  2.68464530e-04]\n [  4.69754887e-05]\n [  3.03290653e-05]\n [  4.12058980e-05]\n [  4.05777610e-05]\n [  6.50616785e-05]\n [  1.61476215e-04]\n [  2.20012753e-05]\n [  6.41189545e-05]\n [  1.39521389e-05]\n [  2.03385716e-05]\n [  2.41154680e-06]\n [  1.21574805e-04]\n [  1.15687770e-04]\n [  7.57445887e-05]\n [  1.09902845e-04]\n [  4.05984174e-05]\n [  1.96829627e-04]\n [  3.49406801e-05]\n [  2.82991073e-06]\n [  8.62815796e-05]\n [  3.75889322e-05]\n [  8.64925387e-05]\n [  1.09475361e-04]\n [  6.63340106e-05]\n [  2.47742046e-05]\n [  1.98384769e-05]\n [  2.78876541e-05]\n [  2.90919543e-05]\n [  6.28666328e-07]\n [  5.61553388e-05]\n [  7.34237119e-07]\n [  1.47893807e-04]\n [  7.67683377e-06]\n [  1.73028502e-06]\n [  6.40175767e-06]\n [  1.43608249e-05]\n [  5.74924597e-05]\n [  3.87733598e-05]\n [  2.75754405e-06]\n [  3.25347713e-07]\n [  4.91465471e-07]\n [  3.82643411e-05]\n [  8.23897790e-05]\n [  1.22390047e-04]\n [  1.06191592e-05]\n [  2.64619644e-07]\n [  1.06135667e-05]\n [  3.39411235e-05]\n [  9.56520034e-06]\n [  3.47796413e-05]\n [  1.85912086e-05]\n [  2.66879302e-04]\n [  1.06496409e-04]]\n24 [[  9.34235245e-10]\n [  5.28356031e-05]\n [  3.95383904e-05]\n [  2.87330004e-05]\n [  9.66851148e-05]\n [  1.95624625e-06]\n [  6.64023828e-05]\n [  1.01085156e-04]\n [  7.33011839e-05]\n [  1.97239551e-05]\n [  4.97575020e-05]\n [  3.80475576e-05]\n [  4.88088335e-05]\n [  9.68202003e-05]\n [  1.30612784e-06]\n [  3.88339540e-05]\n [  5.01828872e-05]\n [  3.70935850e-05]\n [  2.93466087e-06]\n [  3.74954398e-05]\n [  2.04281278e-05]\n [  1.15077222e-04]\n [  6.27785448e-06]\n [  8.85463010e-07]\n [  6.79767400e-05]\n [  5.64043148e-05]\n [  1.09218061e-04]\n [  1.52662415e-05]\n [  7.51728066e-06]\n [  2.27787696e-05]\n [  1.30640023e-04]\n [  2.21509363e-05]\n [  2.91897704e-05]\n [  4.68787985e-05]\n [  2.90230691e-05]\n [  1.99397482e-06]\n [  2.34633917e-05]\n [  7.96004679e-05]\n [  1.74915317e-06]\n [  2.69176951e-07]\n [  1.54618727e-04]\n [  5.43351052e-05]\n [  2.88404517e-05]\n [  1.19539664e-05]\n [  8.65475425e-09]\n [  2.86943327e-06]\n [  1.78398506e-04]\n [  2.96189919e-05]\n [  1.91346171e-05]\n [  2.63619804e-05]\n [  2.61586756e-05]\n [  4.29403262e-05]\n [  1.09732944e-04]\n [  1.37675179e-05]\n [  4.24318932e-05]\n [  8.29275814e-06]\n [  1.57272461e-05]\n [  2.03233549e-06]\n [  8.12493745e-05]\n [  7.72627100e-05]\n [  5.01153518e-05]\n [  7.55024157e-05]\n [  2.52583468e-05]\n [  1.35156239e-04]\n [  2.28302306e-05]\n [  2.36556161e-06]\n [  5.67615643e-05]\n [  2.42740716e-05]\n [  5.69945987e-05]\n [  7.36739821e-05]\n [  4.40573785e-05]\n [  1.64939120e-05]\n [  1.27403637e-05]\n [  1.73971566e-05]\n [  1.85000335e-05]\n [  2.07457347e-07]\n [  3.75053678e-05]\n [  7.58707984e-07]\n [  1.00117148e-04]\n [  4.60308911e-06]\n [  7.83073460e-07]\n [  4.19075559e-06]\n [  9.05571596e-06]\n [  3.76053067e-05]\n [  2.49210971e-05]\n [  1.53007818e-06]\n [  5.21460436e-07]\n [  1.33203955e-07]\n [  2.57655647e-05]\n [  5.48028802e-05]\n [  8.14093437e-05]\n [  5.87509066e-06]\n [  2.49715324e-08]\n [  6.78880542e-06]\n [  2.23614934e-05]\n [  5.62176001e-06]\n [  2.19395879e-05]\n [  1.12952703e-05]\n [  1.76918897e-04]\n [  6.90708985e-05]]\n25 [[  1.39168064e-08]\n [  3.41882587e-05]\n [  2.61274708e-05]\n [  1.81798423e-05]\n [  6.56491611e-05]\n [  1.00501120e-06]\n [  4.37691997e-05]\n [  6.82348837e-05]\n [  4.82746254e-05]\n [  1.26801842e-05]\n [  3.37829588e-05]\n [  2.51348411e-05]\n [  3.27167108e-05]\n [  6.55070617e-05]\n [  7.28324494e-07]\n [  2.53005419e-05]\n [  3.36915618e-05]\n [  2.44491730e-05]\n [  1.61922480e-06]\n [  2.45246811e-05]\n [  1.28594402e-05]\n [  7.89517362e-05]\n [  4.63516608e-06]\n [  8.05340221e-07]\n [  4.57817878e-05]\n [  3.67435932e-05]\n [  7.42743941e-05]\n [  9.28588815e-06]\n [  5.13552004e-06]\n [  1.49666485e-05]\n [  8.76476042e-05]\n [  1.42191457e-05]\n [  1.92742045e-05]\n [  3.02377484e-05]\n [  1.86603538e-05]\n [  1.56716453e-06]\n [  1.49518937e-05]\n [  5.42809867e-05]\n [  8.44313263e-07]\n [  1.52830125e-07]\n [  1.03949067e-04]\n [  3.55350930e-05]\n [  1.87436635e-05]\n [  7.69454800e-06]\n [  3.48678810e-08]\n [  2.17246225e-06]\n [  1.18650423e-04]\n [  1.86644738e-05]\n [  1.20550767e-05]\n [  1.68760544e-05]\n [  1.68925126e-05]\n [  2.83600202e-05]\n [  7.45985817e-05]\n [  8.61550961e-06]\n [  2.81005869e-05]\n [  4.91173660e-06]\n [  1.20193417e-05]\n [  1.66468305e-06]\n [  5.43097995e-05]\n [  5.16445289e-05]\n [  3.31809206e-05]\n [  5.18530833e-05]\n [  1.56985170e-05]\n [  9.27849032e-05]\n [  1.49283023e-05]\n [  1.92489370e-06]\n [  3.74020638e-05]\n [  1.56818951e-05]\n [  3.75760719e-05]\n [  4.95941640e-05]\n [  2.92619479e-05]\n [  1.09861940e-05]\n [  8.18517947e-06]\n [  1.08642034e-05]\n [  1.17661057e-05]\n [  4.78198565e-08]\n [  2.50550565e-05]\n [  7.22273683e-07]\n [  6.77441931e-05]\n [  2.76023661e-06]\n [  3.25565338e-07]\n [  2.74307240e-06]\n [  5.70414068e-06]\n [  2.46230138e-05]\n [  1.60335567e-05]\n [  8.35747358e-07]\n [  6.31694547e-07]\n [  1.74461547e-08]\n [  1.73653498e-05]\n [  3.64839143e-05]\n [  5.41749469e-05]\n [  3.20119511e-06]\n [  6.25042063e-09]\n [  4.34292724e-06]\n [  1.47468318e-05]\n [  3.28811620e-06]\n [  1.38440573e-05]\n [  6.85306577e-06]\n [  1.17349264e-04]\n [  4.48458050e-05]]\n26 [[  2.91085698e-08]\n [  2.21437567e-05]\n [  1.72867512e-05]\n [  1.15168605e-05]\n [  4.45716068e-05]\n [  4.98712552e-07]\n [  2.88742649e-05]\n [  4.60635601e-05]\n [  3.18174680e-05]\n [  8.16091415e-06]\n [  2.29264788e-05]\n [  1.66106984e-05]\n [  2.19404810e-05]\n [  4.42931705e-05]\n [  3.97948639e-07]\n [  1.64935245e-05]\n [  2.26301745e-05]\n [  1.61253356e-05]\n [  8.79728987e-07]\n [  1.60518921e-05]\n [  8.10217352e-06]\n [  5.41440641e-05]\n [  3.39893222e-06]\n [  6.99279497e-07]\n [  3.08321796e-05]\n [  2.39668188e-05]\n [  5.04860036e-05]\n [  5.63578806e-06]\n [  3.50265509e-06]\n [  9.83847804e-06]\n [  5.88321491e-05]\n [  9.14576503e-06]\n [  1.27393423e-05]\n [  1.95093817e-05]\n [  1.20233090e-05]\n [  1.21187099e-06]\n [  9.53572817e-06]\n [  3.69914305e-05]\n [  3.88529742e-07]\n [  8.60963212e-08]\n [  6.99135926e-05]\n [  2.32706934e-05]\n [  1.21885541e-05]\n [  4.95643417e-06]\n [  5.79400528e-08]\n [  1.63022594e-06]\n [  7.89703772e-05]\n [  1.17556401e-05]\n [  7.58410488e-06]\n [  1.08102049e-05]\n [  1.09265329e-05]\n [  1.87428377e-05]\n [  5.07316036e-05]\n [  5.39171288e-06]\n [  1.86224715e-05]\n [  2.89785476e-06]\n [  9.09392475e-06]\n [  1.33280207e-06]\n [  3.63090112e-05]\n [  3.45471890e-05]\n [  2.19815975e-05]\n [  3.56010714e-05]\n [  9.74537033e-06]\n [  6.36798504e-05]\n [  9.76860520e-06]\n [  1.53291069e-06]\n [  2.46836244e-05]\n [  1.01356336e-05]\n [  2.47879725e-05]\n [  3.33922035e-05]\n [  1.94369968e-05]\n [  7.32116268e-06]\n [  5.26075883e-06]\n [  6.79129016e-06]\n [  7.48411003e-06]\n [  3.16595106e-09]\n [  1.67423332e-05]\n [  6.49786330e-07]\n [  4.58192226e-05]\n [  1.65583560e-06]\n [  1.18263742e-07]\n [  1.79555684e-06]\n [  3.58885131e-06]\n [  1.61398930e-05]\n [  1.03247503e-05]\n [  4.47947286e-07]\n [  6.61291097e-07]\n [  6.24313545e-10]\n [  1.17148029e-05]\n [  2.43079121e-05]\n [  3.60669110e-05]\n [  1.71226782e-06]\n [  5.29120747e-08]\n [  2.77880577e-06]\n [  9.73525039e-06]\n [  1.91274012e-06]\n [  8.73855242e-06]\n [  4.15150907e-06]\n [  7.78758549e-05]\n [  2.91465058e-05]]\n27 [[  3.94430195e-08]\n [  1.43575726e-05]\n [  1.14515769e-05]\n [  7.30517331e-06]\n [  3.02597782e-05]\n [  2.36188640e-07]\n [  1.90629326e-05]\n [  3.10975229e-05]\n [  2.09870741e-05]\n [  5.25747828e-06]\n [  1.55530015e-05]\n [  1.09820849e-05]\n [  1.47219389e-05]\n [  2.99343537e-05]\n [  2.12065302e-07]\n [  1.07582173e-05]\n [  1.52089106e-05]\n [  1.06415464e-05]\n [  4.68996944e-07]\n [  1.05137588e-05]\n [  5.11026519e-06]\n [  3.71168207e-05]\n [  2.47707476e-06]\n [  5.86231295e-07]\n [  2.07648518e-05]\n [  1.56524511e-05]\n [  3.43021093e-05]\n [  3.41204372e-06]\n [  2.38496295e-06]\n [  6.47083289e-06]\n [  3.95096140e-05]\n [  5.89498723e-06]\n [  8.42842746e-06]\n [  1.25900124e-05]\n [  7.76374782e-06]\n [  9.24334017e-07]\n [  6.08544451e-06]\n [  2.51946403e-05]\n [  1.67150290e-07]\n [  4.81227289e-08]\n [  4.70447958e-05]\n [  1.52610246e-05]\n [  7.93152049e-06]\n [  3.19522564e-06]\n [  7.12280084e-08]\n [  1.21355129e-06]\n [  5.26015610e-05]\n [  7.39929919e-06]\n [  4.76406785e-06]\n [  6.92866934e-06]\n [  7.08010521e-06]\n [  1.23958562e-05]\n [  3.45124427e-05]\n [  3.37409892e-06]\n [  1.23502341e-05]\n [  1.70254793e-06]\n [  6.82015025e-06]\n [  1.04742048e-06]\n [  2.42797105e-05]\n [  2.31292142e-05]\n [  1.45722952e-05]\n [  2.44369148e-05]\n [  6.04223396e-06]\n [  4.36973023e-05]\n [  6.39753580e-06]\n [  1.19925323e-06]\n [  1.63166151e-05]\n [  6.55382155e-06]\n [  1.63605600e-05]\n [  2.24897540e-05]\n [  1.29114751e-05]\n [  4.88113710e-06]\n [  3.38268819e-06]\n [  4.25041344e-06]\n [  4.76156993e-06]\n [  2.68163602e-09]\n [  1.11903855e-05]\n [  5.60881801e-07]\n [  3.09774478e-05]\n [  9.93761319e-07]\n [  3.34229213e-08]\n [  1.17550803e-06]\n [  2.25539907e-06]\n [  1.05905874e-05]\n [  6.65577954e-06]\n [  2.34569256e-07]\n [  6.32907927e-07]\n [  1.60397029e-08]\n [  7.91004913e-06]\n [  1.62096967e-05]\n [  2.40219433e-05]\n [  8.95361893e-07]\n [  1.02007114e-07]\n [  1.77853735e-06]\n [  6.43328531e-06]\n [  1.10609824e-06]\n [  5.51777839e-06]\n [  2.51073516e-06]\n [  5.17089720e-05]\n [  1.89614693e-05]]\n28 [[  4.38396448e-08]\n [  9.31846535e-06]\n [  7.59566501e-06]\n [  4.63958168e-06]\n [  2.05420802e-05]\n [  1.04766841e-07]\n [  1.25947499e-05]\n [  2.09958162e-05]\n [  1.38539945e-05]\n [  3.39128837e-06]\n [  1.05468716e-05]\n [  7.26347571e-06]\n [  9.88339889e-06]\n [  2.02191804e-05]\n [  1.09527129e-07]\n [  7.02186117e-06]\n [  1.02257027e-05]\n [  7.02742182e-06]\n [  2.44321996e-07]\n [  6.89056469e-06]\n [  3.22615551e-06]\n [  2.54350343e-05]\n [  1.79530139e-06]\n [  4.78250968e-07]\n [  1.39849308e-05]\n [  1.02360746e-05]\n [  2.32983057e-05]\n [  2.06057575e-06]\n [  1.62177423e-06]\n [  4.25828193e-06]\n [  2.65454273e-05]\n [  3.80725010e-06]\n [  5.58203374e-06]\n [  8.12662256e-06]\n [  5.02439434e-06]\n [  6.96769632e-07]\n [  3.88693434e-06]\n [  1.71506199e-05]\n [  6.49827498e-08]\n [  2.66645657e-08]\n [  3.16702426e-05]\n [  1.00217949e-05]\n [  5.16473801e-06]\n [  2.06153413e-06]\n [  7.50186189e-08]\n [  8.96851475e-07]\n [  3.50670671e-05]\n [  4.65438370e-06]\n [  2.98816190e-06]\n [  4.44367106e-06]\n [  4.59572630e-06]\n [  8.20401692e-06]\n [  2.34864947e-05]\n [  2.11181623e-06]\n [  8.19636989e-06]\n [  9.95473215e-07]\n [  5.07625919e-06]\n [  8.10655195e-07]\n [  1.62388915e-05]\n [  1.54973877e-05]\n [  9.66693187e-06]\n [  1.67696599e-05]\n [  3.74166211e-06]\n [  2.99781987e-05]\n [  4.19280559e-06]\n [  9.24563267e-07]\n [  1.08020540e-05]\n [  4.23941037e-06]\n [  1.08045615e-05]\n [  1.51502054e-05]\n [  8.57748182e-06]\n [  3.25620181e-06]\n [  2.17618890e-06]\n [  2.66318739e-06]\n [  3.02984972e-06]\n [  1.44620662e-08]\n [  7.48202319e-06]\n [  4.69127599e-07]\n [  2.09364280e-05]\n [  5.97014207e-07]\n [  4.96694152e-09]\n [  7.69754479e-07]\n [  1.41574719e-06]\n [  6.95731615e-06]\n [  4.29456941e-06]\n [  1.19348499e-07]\n [  5.69917177e-07]\n [  3.55119134e-08]\n [  5.34597166e-06]\n [  1.08177310e-05]\n [  1.60068375e-05]\n [  4.54995899e-07]\n [  1.33482544e-07]\n [  1.13873205e-06]\n [  4.25552707e-06]\n [  6.35186097e-07]\n [  3.48500771e-06]\n [  1.51583754e-06]\n [  3.43523934e-05]\n [  1.23488926e-05]]\n29 [[  4.35804495e-08]\n [  6.05349123e-06]\n [  5.04365198e-06]\n [  2.95036557e-06]\n [  1.39450158e-05]\n [  4.21587991e-08]\n [  8.32794831e-06]\n [  1.41767423e-05]\n [  9.15153396e-06]\n [  2.18985679e-06]\n [  7.14980570e-06]\n [  4.80579001e-06]\n [  6.63807668e-06]\n [  1.36516974e-05]\n [  5.42920162e-08]\n [  4.58550949e-06]\n [  6.87905413e-06]\n [  4.64327968e-06]\n [  1.23569365e-07]\n [  4.51917595e-06]\n [  2.03900277e-06]\n [  1.74242159e-05]\n [  1.29492571e-06]\n [  3.81841517e-07]\n [  9.41887993e-06]\n [  6.70162080e-06]\n [  1.58173880e-05]\n [  1.24075450e-06]\n [  1.10138922e-06]\n [  2.80350150e-06]\n [  1.78439077e-05]\n [  2.46410832e-06]\n [  3.70008729e-06]\n [  5.24698771e-06]\n [  3.25843916e-06]\n [  5.20118419e-07]\n [  2.48451011e-06]\n [  1.16688234e-05]\n [  2.12903615e-08]\n [  1.46057797e-08]\n [  2.13283565e-05]\n [  6.58951512e-06]\n [  3.36534572e-06]\n [  1.33104095e-06]\n [  7.20192119e-08]\n [  6.58850468e-07]\n [  2.33913834e-05]\n [  2.92584537e-06]\n [  1.87076660e-06]\n [  2.85193209e-06]\n [  2.98849159e-06]\n [  5.43342503e-06]\n [  1.59885276e-05]\n [  1.32170521e-06]\n [  5.44321028e-06]\n [  5.79024686e-07]\n [  3.75292325e-06]\n [  6.19325021e-07]\n [  1.08635750e-05]\n [  1.03910461e-05]\n [  6.41684755e-06]\n [  1.15052126e-05]\n [  2.31348713e-06]\n [  2.05619672e-05]\n [  2.75010552e-06]\n [  7.04072590e-07]\n [  7.16205113e-06]\n [  2.74378340e-06]\n [  7.13858981e-06]\n [  1.02086333e-05]\n [  5.69890290e-06]\n [  2.17323532e-06]\n [  1.40058273e-06]\n [  1.67084102e-06]\n [  1.92807056e-06]\n [  2.57613380e-08]\n [  5.00389388e-06]\n [  3.82785004e-07]\n [  1.41458795e-05]\n [  3.58918669e-07]\n [  3.38422970e-11]\n [  5.04148829e-07]\n [  8.87528245e-07]\n [  4.57489614e-06]\n [  2.77387971e-06]\n [  5.84231188e-08]\n [  4.91331775e-07]\n [  4.92908292e-08]\n [  3.61582079e-06]\n [  7.22497361e-06]\n [  1.06701873e-05]\n [  2.22758899e-07]\n [  1.45664714e-07]\n [  7.29179305e-07]\n [  2.81789107e-06]\n [  3.62010553e-07]\n [  2.20201036e-06]\n [  9.13500855e-07]\n [  2.28320532e-05]\n [  8.04959654e-06]]\n"
     ]
    }
   ],
   "source": [
    "# This part has been done for you already!\n",
    "# Just run it after you finish coding the above sections.\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "train_op = optimizer.minimize(cost)\n",
    "for epoch in range(30):\n",
    "    epoch_cost, _ = sess.run([cost, train_op], feed_dict={X: x, Y_Expected: y})\n",
    "    print(epoch, epoch_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nnMq3Dxx5I1q",
    "colab_type": "text"
   },
   "source": [
    "<b> 5. Print out parameters </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "Htz0trvg5I1s",
    "colab_type": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W: [[ 1.00719106]\n [ 1.99761581]\n [ 2.98960853]]\nb: [[ 9.98534489]]\n"
     ]
    }
   ],
   "source": [
    "# Replace the None with the correct operation. \n",
    "# You should get W close to [[1],[2],[3]] and b close to 10. \n",
    "print(\"W:\", sess.run(W))\n",
    "print(\"b:\", sess.run(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1xXx4-CO5I1v",
    "colab_type": "text"
   },
   "source": [
    "### II. Matrix Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "0ZylKoT7v9J2",
    "colab_type": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     }
    }
   },
   "outputs": [],
   "source": [
    "def ndmatmul():\n",
    "    \"\"\"\n",
    "      # 3d x 2d Matmul operation. \n",
    "      You may find some of these functions useful: einsum, tile, expand_dims.\n",
    "      :return a: Placeholder for 3d tensor [float64]\n",
    "              b: Placeholder for 2d tensor [float64]\n",
    "              c: Matrix Product\n",
    "    \"\"\"\n",
    "    a = tf.placeholder(dtype=tf.float64, name=\"a\", shape=(None, None, None))\n",
    "    b = tf.placeholder(dtype=tf.float64, name=\"b\", shape=(None, None))\n",
    "    c = tf.einsum('ijk,kl', a, b)\n",
    "    return a, b, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "oHHeXiBOxyI2",
    "colab_type": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     }
    }
   },
   "outputs": [],
   "source": [
    "A, B, C = ndmatmul()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Rldu3MIfx3Yi",
    "colab_type": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 2, 3)\n(3, 1)\n(5, 2, 1)\n[[[-0.51783421]\n  [ 1.98033132]]\n\n [[-1.12404125]\n  [ 1.00805567]]\n\n [[-0.40371716]\n  [ 1.43240346]]\n\n [[ 0.49588387]\n  [-1.49477015]]\n\n [[-0.26736846]\n  [ 0.38907473]]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "a = np.random.randn(5, 2, 3)\n",
    "b = np.random.randn(3, 1)\n",
    "c = np.matmul(a, b)\n",
    "print(a.shape)\n",
    "print(b.shape)\n",
    "print(c.shape)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "BX2VDVzx01vp",
    "colab_type": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-0.51783421]\n  [ 1.98033132]]\n\n [[-1.12404125]\n  [ 1.00805567]]\n\n [[-0.40371716]\n  [ 1.43240346]]\n\n [[ 0.49588387]\n  [-1.49477015]]\n\n [[-0.26736846]\n  [ 0.38907473]]]\nCorrect!\n"
     ]
    }
   ],
   "source": [
    "# Will give error if function not implemented. Your output should match Numpy's output.\n",
    "sess = tf.InteractiveSession()\n",
    "c_tensor = sess.run(C, feed_dict={A: a, B: b})\n",
    "print(c_tensor)\n",
    "if (c_tensor - c < 10 ** -10).all():\n",
    "    print(\"Correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M_B3PwLX2mYc",
    "colab_type": "text"
   },
   "source": [
    "### III. Experiments with Feed-forward NN on MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cX231VZk5I13",
    "colab_type": "text"
   },
   "source": [
    "In this Qn, you will experiment with Feed-forward Neural nets while training on the MNIST dataset. Read more about it <a href = \"https://en.wikipedia.org/wiki/MNIST_database\">here</a>. A random sample of the images has been shown to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "GX5Q-nJv5I14",
    "colab_type": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\nExtracting MNIST_data/t10k-images-idx3-ubyte.gz\nExtracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data shape:  (55000, 784)\ntrain_labels shape:  (55000, 10)\neval_data shape:  (10000, 784)\neval_labels shape:  (10000, 10)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAADkAAAD8CAYAAADNPQyCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQv4/8XtUQl9tTOEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11b315be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load MNIST Data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "train_data = mnist.train.images  # Returns np.array\n",
    "train_labels = np.asarray(mnist.train.labels, dtype=np.int32)\n",
    "eval_data = mnist.test.images  # Returns np.array\n",
    "eval_labels = np.asarray(mnist.test.labels, dtype=np.int32)\n",
    "\n",
    "print(\"train_data shape: \", train_data.shape)\n",
    "print(\"train_labels shape: \", train_labels.shape)\n",
    "print(\"eval_data shape: \", eval_data.shape)\n",
    "print(\"eval_labels shape: \", eval_labels.shape)\n",
    "\n",
    "# Randomly choose 10 images from first 50 images of Train Data.\n",
    "for index, idx in enumerate(random.sample(range(50), 10)): \n",
    "    plt.subplot(10, 1, index+1)\n",
    "    plt.imshow(train_data[idx].reshape(28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W6I1p3uH5I18",
    "colab_type": "text"
   },
   "source": [
    "Fill in the following snippet as per the instructions. \n",
    "* For initialising placeholders, use None to accommodate variable batch_size. \n",
    "* Do not change the seed; use it for comparing epoch-wise loss with your friends.\n",
    "* You can use the following <a href =\"https://www.tensorflow.org/versions/r1.1/get_started/mnist/beginners\">tutorial</a> for reference. Note that they use softmax in their example, while you are required to code Feedforward neural network. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "IqWoBVW9276e",
    "colab_type": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     }
    }
   },
   "outputs": [],
   "source": [
    "def initializer_1(shape):\n",
    "    # Do not change the seed. \n",
    "    np.random.seed(1)\n",
    "    return np.random.randn(*shape)\n",
    "\n",
    "\n",
    "def initializer_2(shape):\n",
    "    # Do not change the seed.\n",
    "    np.random.seed(1)\n",
    "    return 0.01 * np.random.randn(*shape)\n",
    "\n",
    "\n",
    "class MNIST_ANN:\n",
    "    def __init__(self, hidden_units, activations, initializer):\n",
    "        \"\"\"\n",
    "        Initialises the weights and builds the compute graph. \n",
    "        Uses AdamOptimizer with default parameters for gradient descent.\n",
    "        :param hidden_units - list of number of hidden units. \n",
    "               Eg: [10,20] => Layer 1 has 10 hidden units and Layer 2 has 20.\n",
    "        :param activations - list of activations for each of the hidden layers.\n",
    "               Eg: [tf.nn.sigmoid, tf.nn.tanh]\n",
    "        :param initializer - the reference to the function used for initializing the weights\n",
    "        \"\"\"\n",
    "        # Define constants\n",
    "        num_input_units = 784\n",
    "        num_output_units = 10\n",
    "        num_hidden_layers = len(hidden_units)\n",
    "        \n",
    "        # Define the placeholders\n",
    "        self.input = tf.placeholder(\n",
    "            dtype=tf.float32,\n",
    "            shape=[None, num_input_units],\n",
    "            name=\"input\")\n",
    "        self.expected_output = tf.placeholder(\n",
    "            dtype=tf.float32,\n",
    "            shape=[None, num_output_units],\n",
    "            name=\"labels\")\n",
    "        \n",
    "        # Initialise the weights and biases \n",
    "        weights = []\n",
    "        biases = []\n",
    "        \n",
    "        # Populate weights and biases for the input layer\n",
    "        weights.append(tf.cast(tf.Variable(initial_value=initializer((num_input_units, hidden_units[0]))), tf.float32))\n",
    "        biases.append(tf.cast(tf.Variable(tf.zeros([hidden_units[0]])), tf.float32))\n",
    "        \n",
    "        # Populate weights and biases for all hidden layers\n",
    "        if num_hidden_layers > 1:\n",
    "            for i in range(1, num_hidden_layers):\n",
    "                num_hidden_units = hidden_units[i-1]\n",
    "                num_hidden_units_next_layer = hidden_units[i]\n",
    "                w = tf.cast(tf.Variable(initial_value=initializer((num_hidden_units, num_hidden_units_next_layer))), tf.float32)\n",
    "                b = tf.cast(tf.Variable(tf.zeros([num_hidden_units_next_layer])), tf.float32)\n",
    "                weights.append(w)\n",
    "                biases.append(b)\n",
    "        \n",
    "        # Populate weights and biases for the output layer\n",
    "        weights.append(tf.cast(tf.Variable(initial_value=initializer((hidden_units[-1], num_output_units))), tf.float32))\n",
    "        biases.append(tf.cast(tf.Variable(tf.zeros([hidden_units[-1]])), tf.float32))\n",
    "        \n",
    "        # Build the graph for computing output:\n",
    "        nn_layers = []\n",
    "        \n",
    "        # Construct the input\n",
    "        y = tf.matmul(self.input, weights[0] + biases[0])\n",
    "        nn_layers.append(activations[0](y))\n",
    "        \n",
    "        # Construct all hidden layers\n",
    "        for i in range(1, num_hidden_layers):\n",
    "            y = tf.matmul(nn_layers[i-1], weights[i] + biases[i])\n",
    "            nn_layers.append(activations[i](y))\n",
    "        \n",
    "        # Construct the output\n",
    "        self.output = tf.matmul(nn_layers[-1], weights[-1]) + biases[-1]\n",
    "        \n",
    "        # Define the loss and accuracy\n",
    "        self.cost = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                labels=self.expected_output,\n",
    "                logits=self.output))\n",
    "        self.accuracy = tf.reduce_mean(\n",
    "            tf.cast(\n",
    "                tf.equal(tf.argmax(self.output, 1), tf.argmax(self.expected_output, 1)),\n",
    "                tf.float32))\n",
    "        \n",
    "        # Instantiate the optimizer\n",
    "        optimizer = tf.train.AdamOptimizer()\n",
    "        self.train_op = optimizer.minimize(self.cost)\n",
    "        self.session = tf.Session()\n",
    "        \n",
    "        # Initialize all variables\n",
    "        self.session.run(tf.global_variables_initializer())\n",
    "    \n",
    "    def train(self, \n",
    "              train_data, \n",
    "              train_labels, \n",
    "              eval_data, \n",
    "              eval_labels, \n",
    "              batch_size, \n",
    "              epochs=100):\n",
    "        \"\"\"\n",
    "        Trains the model.\n",
    "        :param train_data - the data to be used for training.\n",
    "        :param train_labels - the labels to be used for training.\n",
    "        :param eval_data - the data to be used for evaluation.\n",
    "        :param eval_labels - the labels to be used for evaluation.\n",
    "        :param batch_size - The size of the batches of training data.\n",
    "        :param epochs - the number of iterations to train the model. Default is 100\n",
    "        \"\"\"\n",
    "        sess = self.session\n",
    "\n",
    "        # Slice the data and labels into batches depending on the batch_size.\n",
    "        batches = int(train_data.shape[0] / batch_size)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            cost_epoch = 0\n",
    "            for _ in range(batches):\n",
    "                input_batch, label_batch = mnist.train.next_batch(batch_size)\n",
    "                \n",
    "                # Forward Propagate, compute cost and back-propagate.\n",
    "                cost, _ = sess.run([self.cost, self.train_op], \n",
    "                                   feed_dict={\n",
    "                                       self.input: input_batch,\n",
    "                                       self.expected_output: label_batch\n",
    "                                   })\n",
    "                cost_epoch += cost\n",
    "            if epoch % 10 == 0:\n",
    "                print(\"Train accuracy: {}\".format(self.compute_accuracy(train_data, train_labels)))        \n",
    "                print(\"Test accuracy: {}\".format(self.compute_accuracy(eval_data, eval_labels)))\n",
    "            print(\"Epoch {}: {}\".format(epoch, cost_epoch))\n",
    "        print(\"Train accuracy: {}\".format(self.compute_accuracy(train_data, train_labels)))\n",
    "        print(\"Test accuracy: {}\".format(self.compute_accuracy(eval_data, eval_labels)))\n",
    "\n",
    "    def compute_accuracy(self, data, labels):\n",
    "        \"\"\"\n",
    "        Compute the accuracy given input data and labels.\n",
    "        \"\"\"\n",
    "        return self.session.run(\n",
    "            self.accuracy,\n",
    "            feed_dict={\n",
    "                self.input: data, \n",
    "                self.expected_output: labels\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "_TTASFUA5I1_",
    "colab_type": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.7586908936500549\nTest accuracy: 0.7705000042915344\nEpoch 0: 7219.651976570487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 3580.0010548606515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 2859.8273340277374\nTrain accuracy: 0.8599272966384888\nTest accuracy: 0.8636000156402588\n"
     ]
    }
   ],
   "source": [
    "ann = MNIST_ANN([10], [tf.nn.sigmoid], initializer_1)\n",
    "ann.train(train_data, \n",
    "          train_labels, \n",
    "          eval_data, \n",
    "          eval_labels, \n",
    "          batch_size=10, \n",
    "          epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "maX6s20Q5I2B",
    "colab_type": "text"
   },
   "source": [
    "The expected output for the above snippet is\n",
    "<pre>\n",
    "Train accuracy: 0.780763626099\n",
    "Test accuracy: 0.791599988937\n",
    "Epoch 0: 6768.86486949\n",
    "Epoch 1: 3275.00310887\n",
    "Epoch 2: 2590.16959983\n",
    "Train accuracy: 0.873399972916\n",
    "Test accuracy: 0.876900017262\n",
    "</pre>\n",
    "If you get any other output and you feel you are correct, you can proceed (However, I cannot think of any case where you can get a different output). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iJv-fmOh5I2B",
    "colab_type": "text"
   },
   "source": [
    "### Answer the following questions by running code snippets. Unless asked explicitly (like in Q1 and Q4), you need to just show the system performance and need not comment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mmfIKKtm5I2D",
    "colab_type": "text"
   },
   "source": [
    "**1. Use 1 hidden layer of 10 hidden units with sigmoid activation and batch_size=10 for this question. Observe the network performance for initializer_1 and initializer_2 and explain the behavior. Why does this happen? What is your guess for tanh and relu? Why?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "IQ-60H-X5I2D",
    "colab_type": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializer_1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.31594544649124146\nTest accuracy: 0.3165000081062317\nEpoch 0: 12436.835572779179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 8550.750813364983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 7436.668827295303\nTrain accuracy: 0.49878183007240295\nTest accuracy: 0.49779999256134033\n\n\n\nInitializer_2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.7216727137565613\nTest accuracy: 0.7305999994277954\nEpoch 0: 5574.391864523292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 4272.362072907388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 4019.4721032604575\nTrain accuracy: 0.7613999843597412\nTest accuracy: 0.7705000042915344\n\n\n\nWhy does this happen?\nInitializer_2 provides better accuracy scores (approximately 5% better) because the weights are initialized with much smaller values than Initializer_1.\nThe sigmoid function y = 1 / (1 + exp(-x)) computes bigger values for smaller weights, which means they are initialized with greater values and have more impact from the start of the training.\nInitializer_2 uses 0.01 * the values for Initializer_1, (1% of Initializer_1), which is significantly smaller.\n\nWhat is your guess for tanh and relu? Why?\nMy guess for tanh is that it will experience similar results to the sigmoid function, but with a greater difference in accuracy scores bwtween initializers.\nThis is because tanh ranges values from (-1, 1) compared to sigmoid which ranges values from (0, 1).\nMy guess for relu\n"
     ]
    }
   ],
   "source": [
    "print('Initializer_1')\n",
    "ann = MNIST_ANN([10], [tf.nn.relu], initializer_1)\n",
    "ann.train(train_data, \n",
    "          train_labels, \n",
    "          eval_data, \n",
    "          eval_labels, \n",
    "          batch_size=10, \n",
    "          epochs=3)\n",
    "\n",
    "print('\\n\\n\\nInitializer_2')\n",
    "ann = MNIST_ANN([10], [tf.nn.relu], initializer_2)\n",
    "ann.train(train_data, \n",
    "          train_labels, \n",
    "          eval_data, \n",
    "          eval_labels, \n",
    "          batch_size=10, \n",
    "          epochs=3)\n",
    "\n",
    "print('\\n\\n\\nWhy does this happen?')\n",
    "print('Initializer_2 provides better accuracy scores (approximately 5% better) because the weights are initialized with much smaller values than Initializer_1.')\n",
    "print('The sigmoid function y = 1 / (1 + exp(-x)) computes bigger values for smaller weights, which means they are initialized with greater values and have more impact from the start of the training.')\n",
    "print('Initializer_2 uses 0.01 * the values for Initializer_1, (1% of Initializer_1), which is significantly smaller.')\n",
    "print('\\nWhat is your guess for tanh and relu? Why?')\n",
    "print('My guess for tanh is that it will experience similar results to the sigmoid function, but with a greater difference in accuracy scores bwtween initializers.')\n",
    "print('This is because tanh ranges values from (-1, 1) compared to sigmoid which ranges values from (0, 1).')\n",
    "print('My guess for relu is that it will perform worse overall because small values are very close to 0 with this function.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eysUpIUF5I2H",
    "colab_type": "text"
   },
   "source": [
    "<b>2. Play around with different configurations of the system. Spend some time on <a href=\"https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.52239&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false\"> Tensorflow Playground </a> to get a feel. Just demonstrate the performance of the system and make observations. No need to make any comments. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "CYr-ppoI5I2H",
    "colab_type": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.32685455679893494\nTest accuracy: 0.3294000029563904\nEpoch 0: 1096.077220439911\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 855.1526888608932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 718.1305743455887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 469.9484614431858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 229.08677345514297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 176.48015482723713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 157.23617093265057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 145.03893394768238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: 135.87377650290728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 127.94035453349352\nTrain accuracy: 0.9404181838035583\nTest accuracy: 0.9298999905586243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.21712727844715118\nTest accuracy: 0.21610000729560852\nEpoch 0: 3764.7966343164444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 3203.714378118515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 2828.993523001671\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 2591.0728338956833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 2321.091254889965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 2207.15582126379\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 2112.893650174141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 1919.1118484139442\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: 1801.310665100813\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 1721.4665396213531\nTrain accuracy: 0.6510000228881836\nTest accuracy: 0.6416000127792358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.29041817784309387\nTest accuracy: 0.29249998927116394\nEpoch 0: 2245.612993478775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 1767.3207231760025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 1258.6704075932503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 964.8486541509628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 786.3613228201866\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 652.5153859853745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 522.1321328729391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 450.7464482486248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: 408.10645987838507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 380.8429092913866\nTrain accuracy: 0.9154727458953857\nTest accuracy: 0.9111999869346619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.8652545213699341\nTest accuracy: 0.868399977684021\nEpoch 0: 656.1265763640404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 286.5229874551296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 215.94161753356457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 186.35879562795162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 170.94488133490086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 159.78453147411346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 152.29967929422855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 146.15340925008059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: 140.9633423164487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 136.5981752127409\nTrain accuracy: 0.9345818161964417\nTest accuracy: 0.9258999824523926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.8727454543113708\nTest accuracy: 0.8748000264167786\nEpoch 0: 658.9400433897972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 278.80368283391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 210.99788933992386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 184.6673841625452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 169.64718374609947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 160.75818066298962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 153.21435844153166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 147.3923533782363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: 143.43707471340895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 139.90269585698843\nTrain accuracy: 0.9313636422157288\nTest accuracy: 0.9251999855041504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.23810909688472748\nTest accuracy: 0.23229999840259552\nEpoch 0: 1070.0751485824585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 870.5943779945374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 680.8393391370773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 524.651781141758\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 420.07327950000763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 333.6220278441906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 271.0481206178665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 226.09967443346977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: 198.44065894186497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 181.0732659995556\nTrain accuracy: 0.9211454391479492\nTest accuracy: 0.9142000079154968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.8461999893188477\nTest accuracy: 0.8381999731063843\nEpoch 0: 2320.984729871154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 975.9889228120446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 723.2590276002884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 630.1693148836493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 577.7614377569407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 553.5789951011539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 535.2894643619657\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 516.8542717974633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: 503.4455648716539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 500.4842035844922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.9386000037193298\nTest accuracy: 0.9296000003814697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.8571454286575317\nTest accuracy: 0.8579000234603882\nEpoch 0: 544.0135030448437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 235.92216446995735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 203.86444337666035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 192.11368913948536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 184.66637952625751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 180.45343346893787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 176.86863332986832\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 173.45876142382622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: 171.4989409521222\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-62f70d040154>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m           \u001b[0meval_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m           epochs=10)\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0mann\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMNIST_ANN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializer_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-e94ead1bf084>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_data, train_labels, eval_data, eval_labels, batch_size, epochs)\u001b[0m\n\u001b[1;32m    122\u001b[0m                                    feed_dict={\n\u001b[1;32m    123\u001b[0m                                        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minput_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m                                        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpected_output\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlabel_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m                                    })\n\u001b[1;32m    126\u001b[0m                 \u001b[0mcost_epoch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mons/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mons/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mons/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mons/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mons/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "ann = MNIST_ANN([20, 10], [tf.nn.sigmoid, tf.nn.tanh], initializer_2)\n",
    "ann.train(train_data, \n",
    "          train_labels, \n",
    "          eval_data, \n",
    "          eval_labels, \n",
    "          batch_size=100, \n",
    "          epochs=20)\n",
    "\n",
    "ann = MNIST_ANN([10, 10], [tf.nn.sigmoid, tf.nn.sigmoid], initializer_2)\n",
    "ann.train(train_data, \n",
    "          train_labels, \n",
    "          eval_data, \n",
    "          eval_labels, \n",
    "          batch_size=30, \n",
    "          epochs=10)\n",
    "\n",
    "ann = MNIST_ANN([20, 10], [tf.nn.sigmoid, tf.nn.sigmoid], initializer_2)\n",
    "ann.train(train_data, \n",
    "          train_labels, \n",
    "          eval_data, \n",
    "          eval_labels, \n",
    "          batch_size=50, \n",
    "          epochs=10)\n",
    "\n",
    "ann = MNIST_ANN([10], [tf.nn.tanh], initializer_2)\n",
    "ann.train(train_data, \n",
    "          train_labels, \n",
    "          eval_data, \n",
    "          eval_labels, \n",
    "          batch_size=100, \n",
    "          epochs=10)\n",
    "\n",
    "ann = MNIST_ANN([10], [tf.nn.tanh], initializer_2)\n",
    "ann.train(train_data, \n",
    "          train_labels, \n",
    "          eval_data, \n",
    "          eval_labels, \n",
    "          batch_size=100, \n",
    "          epochs=10)\n",
    "\n",
    "ann = MNIST_ANN([10, 10], [tf.nn.tanh, tf.nn.sigmoid], initializer_2)\n",
    "ann.train(train_data, \n",
    "          train_labels, \n",
    "          eval_data, \n",
    "          eval_labels, \n",
    "          batch_size=100, \n",
    "          epochs=10)\n",
    "\n",
    "ann = MNIST_ANN([20, 10], [tf.nn.tanh, tf.nn.tanh], initializer_2)\n",
    "ann.train(train_data, \n",
    "          train_labels, \n",
    "          eval_data, \n",
    "          eval_labels, \n",
    "          batch_size=25, \n",
    "          epochs=10)\n",
    "\n",
    "ann = MNIST_ANN([10], [tf.nn.relu], initializer_2)\n",
    "ann.train(train_data, \n",
    "          train_labels, \n",
    "          eval_data, \n",
    "          eval_labels, \n",
    "          batch_size=100, \n",
    "          epochs=10)\n",
    "\n",
    "ann = MNIST_ANN([20, 10], [tf.nn.relu, tf.nn.sigmoid], initializer_2)\n",
    "ann.train(train_data, \n",
    "          train_labels, \n",
    "          eval_data, \n",
    "          eval_labels, \n",
    "          batch_size=10, \n",
    "          epochs=10)\n",
    "\n",
    "ann = MNIST_ANN([10, 10], [tf.nn.relu, tf.nn.tanh], initializer_2)\n",
    "ann.train(train_data, \n",
    "          train_labels, \n",
    "          eval_data, \n",
    "          eval_labels, \n",
    "          batch_size=50, \n",
    "          epochs=10)\n",
    "\n",
    "ann = MNIST_ANN([20, 10], [tf.nn.relu, tf.nn.relu], initializer_2)\n",
    "ann.train(train_data, \n",
    "          train_labels, \n",
    "          eval_data, \n",
    "          eval_labels, \n",
    "          batch_size=20, \n",
    "          epochs=10)\n",
    "\n",
    "ann = MNIST_ANN([10, 10], [tf.nn.relu, tf.nn.relu], initializer_2)\n",
    "ann.train(train_data, \n",
    "          train_labels, \n",
    "          eval_data, \n",
    "          eval_labels, \n",
    "          batch_size=10, \n",
    "          epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GE-NMOxC5I2J",
    "colab_type": "text"
   },
   "source": [
    "<b>4. List the problems you faced while experimenting [Loss did not decrease, ran into NaNs, etc]. What conclusions did you make? </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "aRZAGPJ-5I2L",
    "colab_type": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     }
    }
   },
   "outputs": [],
   "source": [
    "print('I ran into issues with my code when testing hidden unit sizes = 16')\n",
    "print('Bigger batch sizes computes faster than small batch sizes.')\n",
    "print('I experienced bad results with the inputs [10, 10], [sigmoid, sigmoid], batch_size=30 and epochs=10.')\n",
    "print('I experienced good results with [20, 10], [sigmoid, tanh], batch_size=100 and epochs=10.')\n",
    "print('I experienced better results with Initializer_2 than with Initializer_1 on a consistent basis.')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "A1.ipynb",
   "version": "0.3.2",
   "views": {},
   "default_view": {},
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}