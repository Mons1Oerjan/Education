{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IQaTiatZGmoI"
   },
   "source": [
    "# <center>Assignment 3</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2b-ZK9tuGmoK"
   },
   "source": [
    "This assignment is based on embeddings and CNNs. You can choose to code in Python2 or Python3. All the imports made in this notebook are as below; if these imports work, you are (mostly) set to complete the assignment. You will learn the following:\n",
    "* Making use of embeddings in Tensorflow\n",
    "* Coding CNNs in TF\n",
    "* Intuitions behind working of CNN\n",
    "* Intuitions behind embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Zjqi3WjaGmoL"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import random \n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if sys.version_info >= (3, 0):\n",
    "  from builtins import map as m\n",
    "  def map(f,l):\n",
    "    return list(m(f,l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "In8BxTz4GmoO"
   },
   "source": [
    "## Quick Review questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5WuBRadWGmoO"
   },
   "source": [
    "Q1) If the input volume has dimensions 10 x 10 x 32 (Height x Width x Channels), how many weights will be there in a filter that considers an area of 5 x 5?\n",
    "\n",
    "<b>Answer:</b> Num_weights = 32 * 5 * 5 * D_out, where D_out = the number of output channels (dimensions).\n",
    "\n",
    "Q2) If input volume has dimensions 10 x 10 x 32 and after convolution we get an output volume of 8 x 8 x 64, how many filters were used? \n",
    "\n",
    "<b>Answer:</b> 64 filters (The number of filters = the number of channels for the output volume)\n",
    "\n",
    "Q3) What is inverted-dropout? Why is it done? \n",
    "\n",
    "<b>Answer:</b> Dropout is a regularization technique that complements methods such as L1, L2, and MaxNorm. Dropout is implemented by assigning a probability (hyperparameter) to each neuron in a NN which determines if the neuron is alive, or is set to 0. <b>Inverted dropout</b> is a version of dropout which performs scaling and dropping during the training rather than during testing. It is always preferable to use inverted dropout since it leaves the forward pass at test time alone (no changes made to it)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vWzfE86tusct"
   },
   "source": [
    "## Sentiment Classification - dataset analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "atbvZSXlGmoQ"
   },
   "source": [
    "We will use movie review dataset taken from http://www.cs.cornell.edu/people/pabo/movie-review-data/. The exact dataset we will use is the Sentence-polarity dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "6l_qzOlhGmoR",
    "outputId": "1f2332b1-06fd-40ec-ddd3-225bc11ebb7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews of class_neg = 5331\n",
      "\tMax number of tokens in a sentence = 56\n",
      "\tMin number of tokens in a sentence = 1\n",
      "Number of reviews of class_pos = 5331\n",
      "\tMax number of tokens in a sentence = 59\n",
      "\tMin number of tokens in a sentence = 2\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "for file_, label in zip([\"class_neg.txt\", \"class_pos.txt\"], [0, 1]):\n",
    "    lines = open(file_).readlines()\n",
    "    lines = map(lambda x: x.strip().replace(\"-\", \" \").split(), lines)\n",
    "    for line in lines:\n",
    "        data.append([line, label])\n",
    "    print(\"Number of reviews of {} = {}\".format(file_[:-4], len(lines)))\n",
    "    print(\"\\tMax number of tokens in a sentence = {}\".format(max(map(lambda x: len(x), lines))))\n",
    "    print(\"\\tMin number of tokens in a sentence = {}\".format(min(map(lambda x: len(x), lines))))\n",
    "random.Random(5).shuffle(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sKqVN0XTGmoV"
   },
   "source": [
    "Observe that the lengths of sentences are different. In case, we need to vectorize the operations, we need all sentences to be of equal length. Therefore, we will pad all sentences to be of equal length and substitute the padded parts of sentence with zeros. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "UJSIi3EOGmoV",
    "outputId": "179ed366-f127-4820-80b6-190a3d8fad05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "an awkwardly garish showcase that diverges from anything remotely probing or penetrating .\n"
     ]
    }
   ],
   "source": [
    "# See some randomly sampled sentences\n",
    "print(\" \".join(data[random.randint(0, len(data))][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x70n4lA7GmoY"
   },
   "source": [
    "We will work with the sentence as given and not remove any stop-words or punctuation marks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "959vht4CGmoZ",
    "outputId": "5ead240b-6bd1-4bfb-e48c-125902935212"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words :  19757\n",
      "train\n",
      "\tNumber of positive examples :  4263\n",
      "\tNumber of negative examples :  4266\n",
      "test\n",
      "\tNumber of positive examples :  1068\n",
      "\tNumber of negative examples :  1065\n"
     ]
    }
   ],
   "source": [
    "sents = map(lambda x: x[0], data)  # all sentences\n",
    "all_words = set()\n",
    "for sent in sents:\n",
    "    all_words |= set(sent)\n",
    "all_words = sorted(list(all_words))\n",
    "vocab = {\n",
    "    all_words[i]: i for i in range(len(all_words))\n",
    "}\n",
    "print(\"Number of words : \", len(vocab))\n",
    "train = data[:int(0.8 * len(data))]\n",
    "test = data[int(0.8 * len(data)):]\n",
    "train_data = []\n",
    "train_targets = []\n",
    "test_data = []\n",
    "test_targets = []\n",
    "for list_all, list_data, list_target, label_list in zip([train, test], [train_data, test_data], [train_targets, test_targets], [\"train\",\"test\"]):\n",
    "    for datum, label in list_all:\n",
    "        list_data.append([vocab[w] for w in datum])\n",
    "        list_target.append([label])\n",
    "    print(label_list)\n",
    "    print(\"\\tNumber of positive examples : \", list_target.count([1]))\n",
    "    print(\"\\tNumber of negative examples : \", list_target.count([0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E5NZIsmiGmoc"
   },
   "source": [
    "For implementation purposes, we will need an index for the padded word and we will use the index 19757.\n",
    "Note: For a dataset of this <i>small</i> size, we will need to do K-Fold Cross-validation to evaluate the performance. However, we will work with this train-test split for the rest of this assignment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cIWGatDXGmoc"
   },
   "source": [
    "## Simple Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BQEg38m6Gmod"
   },
   "source": [
    "<img src=\"https://web.cs.dal.ca/~sastry/cnn_simple.jpg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZCvl-t5EGmoe"
   },
   "source": [
    "The above image shows the architecture of the simple model that we will implement for text classification. We are interested in the following hyperparameters apart from the number of filters (which we will set to 1 for this problem):\n",
    "* The span of the filter/the number of words considered for making the prediction.\n",
    "* The size of the stride.\n",
    "* The number of activations selected for feeding into softmax classifier.\n",
    "\n",
    "Before continuing:\n",
    "\n",
    "* Can you reason how the machine is classifying (in the above example)? The values of the activations are color-coded. Is this the only possible way the machine can work? \n",
    "\n",
    "    (Your answer might look like : ... filter is ... template matching ... )\n",
    "\n",
    "<b>Answer</b>: The filter gets applied to 4 locations in the input matrix, and outputs 4 numerical values based on the input words. Each row in the matrix represents a word (with zero-padding to keep the same length for each word). The filter convolution generates feature maps detecting specific features (e.g. does the sentence contain \"not great\"?), which we then apply max-pooling to. The reason for applying max-pooling is to get a fixed size output matrix which is required for classification problems. Also, by only keeping the max of some groups of feature maps we keep information about whether or not a feature appeared in the sentence while loosing information about exactly where in the sentence the feature appeared. However, this is okay for classification purposes since we are attempting to classify whether a movie review is a positive review or a negative view. \n",
    "    \n",
    "* Why might order of activations need to be retained?\n",
    "\n",
    "<b>Answer</b>: The order of activations might need to be retained to know the location of the sentences, or the locations of specific features within sentences. The meaning of a text can differ if two sentences (or two words within a sentence) gets swapped around, and therefore it might be important to retain the order.\n",
    "\n",
    "* In the code, we will add an additional row of zeros to represent the padded words. Will the zeros of the padded words be updated during back-prop? Why or why not?\n",
    "\n",
    "<b>Answer</b>: No."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JrDDcU5GGmoe"
   },
   "source": [
    "First, we will write code which can select k top elements in the order they appeared. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "8SViy9l_Gmog"
   },
   "outputs": [],
   "source": [
    "def k_max_pool(A, k):\n",
    "    \"\"\"\n",
    "    A = 2 dimensional array (assume that the length of last dimension of A will be always more than k)\n",
    "    k = number of elements.\n",
    "    Return: For every row of A, return the top k elements in the order they appear.\n",
    "    \"\"\"\n",
    "    assert len(A.get_shape()) == 2\n",
    "    def func(row):\n",
    "        _, indices = tf.nn.top_k(row, k=k, sorted=False)\n",
    "        indices = tf.contrib.framework.sort(indices)\n",
    "        return tf.gather(row, indices)\n",
    "    return tf.map_fn(func, A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "MayRnDIAGmoi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "A = tf.placeholder(shape=[None, None], dtype=tf.float64)\n",
    "top = k_max_pool(A, 5)\n",
    "sess = tf.Session()\n",
    "for i in range(1, 6):\n",
    "    np.random.seed(5)\n",
    "    l = np.random.randn(i * 10, i * 10)\n",
    "    top_elements = sess.run(top, feed_dict={A: l})\n",
    "    l = l.tolist()\n",
    "    top_elements2 = np.array(map(lambda x: [x[i] for i in range(len(x)) if x[i] > sorted(x, reverse=True)[5]], l))\n",
    "    # Note that this test assumes that the 6th largest element and 5th largest element are different.\n",
    "    print(((top_elements - top_elements2) < 10**-10).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "KN66FnNcGmok"
   },
   "outputs": [],
   "source": [
    "def initializer(shape):\n",
    "    xavier = tf.contrib.layers.xavier_initializer(seed=1)\n",
    "    return xavier(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "HeXmr83YGmom"
   },
   "outputs": [],
   "source": [
    "class CNN_simple:\n",
    "    def __init__(self, num_words, embedding_size=30, span=2, k=5):\n",
    "        self.num_words = num_words\n",
    "\n",
    "        # The batch of text documents. Let's assume that it is always padded to length 100\n",
    "        self.input = tf.placeholder(shape=[None, 100], dtype=tf.int32)\n",
    "        self.expected_output = tf.placeholder(shape=[None, 1], dtype=tf.float32)\n",
    "\n",
    "        embedding_matrix = tf.Variable(initializer((num_words, embedding_size)), name=\"embeddings\")\n",
    "        \n",
    "        # Add an additional row of zeros to denote padded words.\n",
    "        zero_padding_row = tf.zeros((1, embedding_size))\n",
    "        self.embedding_matrix = tf.concat(values=[embedding_matrix, zero_padding_row], axis=0)\n",
    "        \n",
    "        # Extract the vectors from the embedding matrix. The dimensions should be None x 100 x embedding_size\n",
    "        vectors = tf.nn.embedding_lookup(self.embedding_matrix, self.input) # None x 100 x embedding_size\n",
    "        \n",
    "        # In order to use conv2d, we need vectors to be 4 dimensional.\n",
    "        # Convention: NHWC = None (Batch Size) x Height(of image) x Width(of image) x Channel(Depth - e.g. RGB).\n",
    "        # For text classification we consider Height = 1, width = number of words, channel = embedding_size.\n",
    "        vectors2d = tf.expand_dims(vectors, 1) # None x 1 x 100 x embedding_size\n",
    "        \n",
    "        # Conv2d needs a filter bank.\n",
    "        # The dimensions of the filter bank = Height, Width, in-channels, out-channels(Number-of-Filters).\n",
    "        # We are creating a single filter of size = span. \n",
    "        # height = 1, width = span, in-channels = embedding_size, out-channels = 1. \n",
    "        single_filter = tf.Variable(initializer((1, span, embedding_size, 1)), name=\"filter\")\n",
    "        bias = tf.Variable(0.0, name=\"bias\")  # You need a bias for each filter.\n",
    "        conv_span = tf.nn.conv2d(\n",
    "            input=vectors2d,\n",
    "            filter=single_filter,\n",
    "            strides=[1, 1, 1, 1],  # Note: the first and last elements SHOULD be 1.\n",
    "            padding=\"VALID\"  # We are ok with input size being reduced during the process of convolution.\n",
    "        )  # Shape = (1, span, embedding_size, 1)\n",
    "        \n",
    "        acts = tf.nn.leaky_relu(conv_span + bias)\n",
    "        \n",
    "        # Now, let us extract the top k activations. \n",
    "        # But, we need to first convert acts into 2-dimensional.\n",
    "        acts_2d = tf.squeeze(acts, [1, 3])\n",
    "        \n",
    "        # Use k_max_pool to extract top-k activations\n",
    "        input_fully_connected = k_max_pool(acts_2d, k)  # None x k\n",
    "        \n",
    "        # Initialize the weight and bias needed for softmax classifier.\n",
    "        self.softmax_weight = tf.Variable(tf.truncated_normal([k, 2], stddev=0.1))\n",
    "        self.softmax_bias = tf.Variable(tf.constant(0.1, shape=[2]))\n",
    "        \n",
    "        # Write out the equation for computing the logits.\n",
    "        Wx_plus_b = tf.add(tf.matmul(input_fully_connected, self.softmax_weight), self.softmax_bias)\n",
    "        self.output = tf.nn.softmax(Wx_plus_b, axis=1)  # Shape = (1, 2)\n",
    "        \n",
    "        # Compute the cross-entropy cost. \n",
    "        # You might either sum or take mean of all the costs across all the examples. \n",
    "        # It is your choice as the test case is on Stochastic Training.\n",
    "        one_hot_expected_output = tf.one_hot(tf.cast(self.expected_output, dtype=tf.int32), 2)\n",
    "        self.cost = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "                logits=Wx_plus_b,\n",
    "                labels=one_hot_expected_output\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        correct_prediction = tf.equal(tf.reshape(tf.argmax(self.output, 1), [-1, 1]), tf.cast(self.expected_output, dtype=tf.int64))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))        \n",
    "        \n",
    "        optimizer = tf.train.AdamOptimizer()\n",
    "        self.train_op = optimizer.minimize(self.cost)\n",
    "        self.session = tf.Session()\n",
    "        self.session.run(tf.global_variables_initializer())\n",
    "\n",
    "    def pad(self, data, pad_word, pad_length=100):\n",
    "        for datum in data:\n",
    "            datum.extend([pad_word]*(pad_length-len(datum)))\n",
    "        return data\n",
    "    \n",
    "    def train(self, train_data, test_data, train_targets, test_targets, batch_size=1, epochs=1, verbose=False):\n",
    "        sess = self.session\n",
    "        self.pad(train_data, self.num_words)\n",
    "        self.pad(test_data, self.num_words)\n",
    "        print(\"Starting training...\")\n",
    "        for epoch in range(epochs):\n",
    "            cost_epoch = 0\n",
    "            c = 0\n",
    "            for datum, target in zip([train_data[i:i+batch_size] for i in range(0, len(train_data), batch_size)],\n",
    "                                   [train_targets[i:i+batch_size] for i in range(0, len(train_targets), batch_size)]):\n",
    "                _, cost = sess.run([self.train_op, self.cost], feed_dict={self.input: datum, self.expected_output: target})\n",
    "                cost_epoch += cost\n",
    "                c += 1\n",
    "                if c % 100 == 0 and verbose:\n",
    "                    print(\"\\t{} batches finished. Cost : {}\".format(c, cost_epoch/c))\n",
    "            print(\"Epoch {}: {}\".format(epoch, cost_epoch/len(train_data)))\n",
    "            print(\"\\tTrain accuracy: {}\".format(self.compute_accuracy(train_data, train_targets)))\n",
    "            print(\"\\tTest accuracy: {}\".format(self.compute_accuracy(test_data, test_targets)))\n",
    "    \n",
    "    def compute_accuracy(self, data, targets):\n",
    "        return self.session.run(self.accuracy, feed_dict={self.input: data, self.expected_output: targets})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "KN8RoSLoGmoo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "\t100 batches finished. Cost : 0.6929708349704743\n",
      "\t200 batches finished. Cost : 0.6922829195857048\n",
      "\t300 batches finished. Cost : 0.6925293787320455\n",
      "\t400 batches finished. Cost : 0.6923463428020478\n",
      "\t500 batches finished. Cost : 0.6932781388759613\n",
      "\t600 batches finished. Cost : 0.6933880699674289\n",
      "\t700 batches finished. Cost : 0.6927246058838709\n",
      "\t800 batches finished. Cost : 0.6925811886787414\n",
      "\t900 batches finished. Cost : 0.6934241186247931\n",
      "\t1000 batches finished. Cost : 0.6938184412121773\n",
      "\t1100 batches finished. Cost : 0.6937404790249738\n",
      "\t1200 batches finished. Cost : 0.6936372148990632\n",
      "\t1300 batches finished. Cost : 0.6933954641452202\n",
      "\t1400 batches finished. Cost : 0.6932443425059318\n",
      "\t1500 batches finished. Cost : 0.6931555347839992\n",
      "\t1600 batches finished. Cost : 0.6928893229737878\n",
      "\t1700 batches finished. Cost : 0.6930368027967565\n",
      "\t1800 batches finished. Cost : 0.69309016396602\n",
      "\t1900 batches finished. Cost : 0.6929770465901024\n",
      "\t2000 batches finished. Cost : 0.6928122484982013\n",
      "\t2100 batches finished. Cost : 0.6930015282687687\n",
      "\t2200 batches finished. Cost : 0.6930013479969718\n",
      "\t2300 batches finished. Cost : 0.6925860702732335\n",
      "\t2400 batches finished. Cost : 0.6923356125503779\n",
      "\t2500 batches finished. Cost : 0.6924229054689407\n",
      "\t2600 batches finished. Cost : 0.6923690290175951\n",
      "\t2700 batches finished. Cost : 0.691725693322994\n",
      "\t2800 batches finished. Cost : 0.6912746175697871\n",
      "\t2900 batches finished. Cost : 0.6912026980005461\n",
      "\t3000 batches finished. Cost : 0.6913955191572507\n",
      "\t3100 batches finished. Cost : 0.6904910628449532\n",
      "\t3200 batches finished. Cost : 0.6896143928542733\n",
      "\t3300 batches finished. Cost : 0.689718788410678\n",
      "\t3400 batches finished. Cost : 0.6888196336960092\n",
      "\t3500 batches finished. Cost : 0.6882401984333992\n",
      "\t3600 batches finished. Cost : 0.6876716894325283\n",
      "\t3700 batches finished. Cost : 0.6873728453065898\n",
      "\t3800 batches finished. Cost : 0.68683173285111\n",
      "\t3900 batches finished. Cost : 0.6859929390404469\n",
      "\t4000 batches finished. Cost : 0.6849499305188655\n",
      "\t4100 batches finished. Cost : 0.6836761166627814\n",
      "\t4200 batches finished. Cost : 0.6819886887818575\n",
      "\t4300 batches finished. Cost : 0.681775723983382\n",
      "\t4400 batches finished. Cost : 0.6820776582509279\n",
      "\t4500 batches finished. Cost : 0.681512765844663\n",
      "\t4600 batches finished. Cost : 0.6804973928824715\n",
      "\t4700 batches finished. Cost : 0.680126761813113\n",
      "\t4800 batches finished. Cost : 0.6796042365736018\n",
      "\t4900 batches finished. Cost : 0.6788929743882345\n",
      "\t5000 batches finished. Cost : 0.6782211693614721\n",
      "\t5100 batches finished. Cost : 0.6780841160171173\n",
      "\t5200 batches finished. Cost : 0.6776232842470591\n",
      "\t5300 batches finished. Cost : 0.6770113731920719\n",
      "\t5400 batches finished. Cost : 0.6758918873938146\n",
      "\t5500 batches finished. Cost : 0.6750466578358953\n",
      "\t5600 batches finished. Cost : 0.6747022083933865\n",
      "\t5700 batches finished. Cost : 0.6730688979594331\n",
      "\t5800 batches finished. Cost : 0.6729156674316217\n",
      "\t5900 batches finished. Cost : 0.6724824275543629\n",
      "\t6000 batches finished. Cost : 0.6708145522537331\n",
      "\t6100 batches finished. Cost : 0.6699804804645112\n",
      "\t6200 batches finished. Cost : 0.6695987073884856\n",
      "\t6300 batches finished. Cost : 0.6688857187791949\n",
      "\t6400 batches finished. Cost : 0.6680571440025233\n",
      "\t6500 batches finished. Cost : 0.6674083824811073\n",
      "\t6600 batches finished. Cost : 0.6668277778882872\n",
      "\t6700 batches finished. Cost : 0.6658109825988536\n",
      "\t6800 batches finished. Cost : 0.6650138621344505\n",
      "\t6900 batches finished. Cost : 0.6646374427780941\n",
      "\t7000 batches finished. Cost : 0.6636251984392958\n",
      "\t7100 batches finished. Cost : 0.6632954654685208\n",
      "\t7200 batches finished. Cost : 0.6629331393260509\n",
      "\t7300 batches finished. Cost : 0.6620870407487023\n",
      "\t7400 batches finished. Cost : 0.6613828422300316\n",
      "\t7500 batches finished. Cost : 0.6599605852882068\n",
      "\t7600 batches finished. Cost : 0.6593298737567507\n",
      "\t7700 batches finished. Cost : 0.6585979943703134\n",
      "\t7800 batches finished. Cost : 0.6576034331918718\n",
      "\t7900 batches finished. Cost : 0.6565995706551815\n",
      "\t8000 batches finished. Cost : 0.656772860799916\n",
      "\t8100 batches finished. Cost : 0.6558332235826019\n",
      "\t8200 batches finished. Cost : 0.6556956110958283\n",
      "\t8300 batches finished. Cost : 0.654694405312549\n",
      "\t8400 batches finished. Cost : 0.6543290126381354\n",
      "\t8500 batches finished. Cost : 0.6531739046963698\n",
      "Epoch 0: 0.6526045810589793\n",
      "\tTrain accuracy: 0.7716027498245239\n",
      "\tTest accuracy: 0.6952648758888245\n"
     ]
    }
   ],
   "source": [
    "c=CNN_simple(len(vocab))\n",
    "c.train(train_data, test_data, train_targets, test_targets, epochs=1, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9qAcWbzyGmop"
   },
   "source": [
    "The expected output for the above snippet is\n",
    "<pre>\n",
    "Starting training...\n",
    "\t100 batches finished. Cost : 0.688363179564\n",
    "\t200 batches finished. Cost : 0.695461705327\n",
    "\t300 batches finished. Cost : 0.695902070602\n",
    "\t400 batches finished. Cost : 0.697339072227\n",
    "\t500 batches finished. Cost : 0.698220448136\n",
    "    ...\n",
    "Epoch 0: 0.675099702418\n",
    "\tTrain accuracy: 0.718958854675\n",
    "\tTest accuracy: 0.664322555065   \n",
    "</pre>\n",
    "If you get any other output and you feel you are correct, you can proceed (However, I cannot think of any case where you can get a different output). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rtV-0gInGmoq"
   },
   "source": [
    "## ConvNet "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xi_un8GvGmor"
   },
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FtrPWOepGmor"
   },
   "source": [
    "<img src=\"https://web.cs.dal.ca/~sastry/cnn.png\" style=\"height:40%;width:40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WLMzzdMBGmos"
   },
   "source": [
    "Essentially, there are 2 kind of hyper-parameters - the filter size and number of filters of each size. In the image shown, there are 3 filter-sizes - 2,3,4 and number of filters of each size is 2. Once the convolution is obtained, 1-max pooling is done - it basically involves extracting 1 activation from the list of activations which is the maximum activation. The reason we need to do this is to construct the inputs to the softmax layer which are of a fixed size.\n",
    "Read more at https://arxiv.org/pdf/1510.03820.pdf. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "sqoRWB9wGmot"
   },
   "outputs": [],
   "source": [
    "class CNN:\n",
    "    def __init__(self, num_words, embedding_size = 30):\n",
    "        self.num_words = num_words\n",
    "\n",
    "        # The batch of text documents. Let's assume that it is always padded to length 100. \n",
    "        # We could use [None,None], but we'll use [None,100] for simplicity. \n",
    "        self.input = tf.placeholder(shape=[None, 100], dtype=tf.int32)\n",
    "        self.expected_output = tf.placeholder(shape=[None, 1], dtype=tf.float32)\n",
    "        \n",
    "        embedding_matrix = tf.Variable(initializer((num_words, embedding_size)), name=\"embeddings\")\n",
    "        # Add an additional row of zeros to denote padded words.\n",
    "        zero_padding_row = tf.zeros((1, embedding_size))\n",
    "        self.embedding_matrix = tf.concat(values=[embedding_matrix, zero_padding_row], axis=0)\n",
    "        \n",
    "        # Extract the vectors from the embedding matrix. The dimensions should be None x 100 x embedding_size. \n",
    "        # Use embedding lookup\n",
    "        vectors = tf.nn.embedding_lookup(self.embedding_matrix, self.input) # None x 100 x embedding_size\n",
    "        \n",
    "        # In order to use conv2d, we need vectors to be 4 dimensional.\n",
    "        # The convention is NHWC - None (Batch Size) x Height(Height of image) x Width(Width of image) x Channel(Depth - similar to RGB).\n",
    "        # For text, let's consider Height = 1, width = number of words, channel = embedding_size.\n",
    "        # Use expand-dims to modify. \n",
    "        vectors2d = tf.expand_dims(vectors, 1) # None x 1 x 100 x embedding_size\n",
    "        \n",
    "        # Create 50 filters with span of 3 words. You need 1 bias for each filter.\n",
    "        # The dimensions of the filter bank = Height, Width, in-channels, out-channels(Number-of-Filters).\n",
    "        filter_tri = tf.Variable(initializer((1, 3, embedding_size, 50)), name=\"weight3\")\n",
    "        bias_tri = tf.Variable(tf.zeros((1, 50)), name=\"bias3\")  # we need a bias for each filter\n",
    "        conv1 = tf.nn.conv2d(\n",
    "            input=vectors2d,\n",
    "            filter=filter_tri,\n",
    "            strides=[1, 1, 98, 1],\n",
    "            padding=\"VALID\"\n",
    "        )  # Shape = (?, 1, 1, 50)\n",
    "        A1 = tf.nn.leaky_relu(conv1+bias_tri)\n",
    "        A1_2d = tf.squeeze(A1, [1, 2])\n",
    "\n",
    "        # Create 50 filters with span of 4 words. You need 1 bias for each filter.\n",
    "        filter_4 = tf.Variable(initializer((1, 4, embedding_size, 50)), name=\"weight4\")  \n",
    "        bias_4 = tf.Variable(tf.zeros((1, 50)), name=\"bias4\")  # we need a bias for each filter\n",
    "        conv2 = tf.nn.conv2d(\n",
    "            input=vectors2d,\n",
    "            filter=filter_4,\n",
    "            strides=[1, 1, 97, 1],\n",
    "            padding=\"VALID\"\n",
    "        )  # Shape = (?, 1, 1, 50)\n",
    "        A2 = tf.nn.leaky_relu(conv2+bias_4)\n",
    "        A2_2d = tf.squeeze(A2, [1, 2])\n",
    "\n",
    "        # Create 50 filters with span of 5 words. You need 1 bias for each filter.\n",
    "        filter_5 = tf.Variable(initializer((1, 5, embedding_size, 50)), name=\"weight5\")  \n",
    "        bias_5 = tf.Variable(tf.zeros((1, 50)), name=\"bias5\")  # we need a bias for each filter\n",
    "        conv3 = tf.nn.conv2d(\n",
    "            input=vectors2d,\n",
    "            filter=filter_5,\n",
    "            strides=[1, 1, 96, 1],\n",
    "            padding=\"VALID\"\n",
    "        )  # Shape = (?, 1, 1, 50)\n",
    "        A3 = tf.nn.leaky_relu(conv3+bias_5)\n",
    "        A3_2d = tf.squeeze(A3, [1, 2])\n",
    "        \n",
    "        # Now extract the maximum activations for each of the filters. The shapes are listed alongside. \n",
    "        max_A1 = k_max_pool(A1_2d, 50)  # None x 50\n",
    "        max_A2 = k_max_pool(A2_2d, 50)  # None x 50\n",
    "        max_A3 = k_max_pool(A3_2d, 50)  # None x 50\n",
    "        \n",
    "        concat = tf.concat([max_A1, max_A2, max_A3], axis=1) # None x 150\n",
    "        \n",
    "        # Initialize the weight and bias needed for softmax classifier. \n",
    "        self.softmax_weight = tf.Variable(tf.truncated_normal([150, 2], stddev=0.1))\n",
    "        self.softmax_bias = tf.Variable(tf.constant(0.1, shape=[2]))\n",
    "        \n",
    "        # Write out the equation for computing the logits.\n",
    "        Wx_plus_b = tf.add(tf.matmul(concat, self.softmax_weight), self.softmax_bias)\n",
    "        self.output = tf.nn.softmax(Wx_plus_b, axis=1) # Shape = ?\n",
    "        \n",
    "        # Compute the cross-entropy cost. \n",
    "        # You might either sum or take mean of all the costs across all the examples. \n",
    "        # It is your choice as the test case is on Stochastic Training. \n",
    "        one_hot_expected_output = tf.one_hot(tf.cast(self.expected_output, dtype=tf.int32), 2)\n",
    "        self.cost = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "                logits=Wx_plus_b,\n",
    "                labels=one_hot_expected_output\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        correct_prediction = tf.equal(tf.reshape(tf.argmax(self.output, 1), [-1, 1]), tf.cast(self.expected_output, dtype=tf.int64))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        \n",
    "        optimizer = tf.train.AdamOptimizer()\n",
    "        self.train_op = optimizer.minimize(self.cost)\n",
    "        self.session = tf.Session()\n",
    "        self.session.run(tf.global_variables_initializer())\n",
    "\n",
    "    def pad(self, data, pad_word, pad_length=100):\n",
    "        for datum in data:\n",
    "            datum.extend([pad_word]*(pad_length-len(datum)))\n",
    "        return data\n",
    "    \n",
    "    def train(self, train_data, test_data, train_targets, test_targets, batch_size=1, epochs=1, verbose=False):\n",
    "        sess = self.session\n",
    "        self.pad(train_data, self.num_words)\n",
    "        self.pad(test_data, self.num_words)\n",
    "        print(\"Starting training...\")\n",
    "        for epoch in range(epochs):\n",
    "            cost_epoch = 0\n",
    "            c = 0\n",
    "            for datum, target in zip([train_data[i:i+batch_size] for i in range(0, len(train_data), batch_size)],\n",
    "                                   [train_targets[i:i+batch_size] for i in range(0, len(train_targets), batch_size)]):\n",
    "                _, cost = sess.run([self.train_op, self.cost], feed_dict={self.input: datum, self.expected_output: target})\n",
    "                cost_epoch += cost\n",
    "                c += 1\n",
    "                if c % 100 == 0 and verbose:\n",
    "                    print(\"\\t{} batches finished. Cost : {}\".format(c, cost_epoch/c))\n",
    "            print(\"Epoch {}: {}\".format(epoch, cost_epoch/len(train_data)))\n",
    "            print(\"\\tTrain accuracy: {}\".format(self.compute_accuracy(train_data, train_targets)))\n",
    "            print(\"\\tTest accuracy: {}\".format(self.compute_accuracy(test_data, test_targets)))\n",
    "            self.display_distances()\n",
    "            self.display_most_similar()\n",
    "    \n",
    "    def compute_accuracy(self, data, targets):\n",
    "        return self.session.run(self.accuracy, feed_dict={self.input: data, self.expected_output: targets})\n",
    "    \n",
    "    def get_distance(self, word1, word2):\n",
    "        \"\"\"\n",
    "        word1 = a word in the dataset\n",
    "        word2 = another word in the dataset\n",
    "        Return: The cosine distance between word1 and word2.\n",
    "        \"\"\"\n",
    "        assert word1 in vocab and word2 in vocab\n",
    "        word1_embedding = self.embedding_matrix[vocab[word1]]\n",
    "        word2_embedding = self.embedding_matrix[vocab[word2]]\n",
    "        word1_normalized = tf.nn.l2_normalize(word1_embedding, axis=0)\n",
    "        word2_normalized = tf.nn.l2_normalize(word2_embedding, axis=0)\n",
    "        cosine_distance = tf.losses.cosine_distance(\n",
    "            word1_normalized,\n",
    "            word2_normalized,\n",
    "            axis=0\n",
    "        )\n",
    "        return self.session.run(cosine_distance)\n",
    "    \n",
    "    def get_most_similar(self, word):\n",
    "        \"\"\"\n",
    "        word = a word in the dataset\n",
    "        Return: the top 10 most similar words to the inputted word.\n",
    "        \"\"\"\n",
    "        assert word in vocab\n",
    "        n = 10\n",
    "        word_embedding = self.embedding_matrix[vocab[word]]\n",
    "        word_normalized = tf.nn.l2_normalize(word_embedding, axis=0)\n",
    "        word_normalized = tf.expand_dims(word_normalized, 0)\n",
    "        matrix_normalized = tf.nn.l2_normalize(self.embedding_matrix, axis=1)\n",
    "        cosine_similarity = tf.matmul(word_normalized, tf.transpose(matrix_normalized))\n",
    "        top_10_values, top_10_indices = self.session.run(tf.nn.top_k(cosine_similarity, k=n+1))\n",
    "        top_10_words = []\n",
    "        for i in range(1, len(top_10_indices[0])):\n",
    "            # the most similar word will be itself.\n",
    "            # Therefore, we skip the first index and only append the next 10.\n",
    "            top_10_words.append(all_words[top_10_indices[0][i]])\n",
    "        return top_10_words\n",
    "    \n",
    "    def display_distances(self):\n",
    "        print('\\n')\n",
    "        print(\"\\tcos_dist good vs bad: {}\".format(self.get_distance('good', 'bad')))\n",
    "        print(\"\\tcos_dist terrible vs horrible: {}\".format(self.get_distance('terrible', 'horrible')))\n",
    "        print(\"\\tcos_dist he vs she: {}\".format(self.get_distance('he', 'she')))\n",
    "        print(\"\\tcos_dist movie vs show: {}\".format(self.get_distance('movie', 'show')))\n",
    "    \n",
    "    def display_most_similar(self):\n",
    "        print('\\n')\n",
    "        print(\"\\ttop 10 similar words to \\\"movie\\\":\")\n",
    "        for w in self.get_most_similar('movie'):\n",
    "            print('\\t\\t- {}'.format(w))\n",
    "        print(\"\\ttop 10 similar words to \\\"perfect\\\":\")\n",
    "        for w in self.get_most_similar('perfect'):\n",
    "            print('\\t\\t- {}'.format(w))\n",
    "        print(\"\\ttop 10 similar words to \\\"horrible\\\":\")\n",
    "        for w in self.get_most_similar('horrible'):\n",
    "            print('\\t\\t- {}'.format(w))\n",
    "        print(\"\\ttop 10 similar words to \\\"good\\\":\")\n",
    "        for w in self.get_most_similar('good'):\n",
    "            print('\\t\\t- {}'.format(w))\n",
    "        print(\"\\ttop 10 similar words to \\\"bad\\\":\")\n",
    "        for w in self.get_most_similar('bad'):\n",
    "            print('\\t\\t- {}'.format(w))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "dfYIkXOkGmov"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 0: 0.6497008505200681\n",
      "\tTrain accuracy: 0.7903622984886169\n",
      "\tTest accuracy: 0.6572902202606201\n",
      "\n",
      "\n",
      "\tcos_dist good vs bad: 1.46150541305542\n",
      "\tcos_dist terrible vs horrible: 0.1410428285598755\n",
      "\tcos_dist he vs she: 0.3128286600112915\n",
      "\tcos_dist movie vs show: 0.5963776111602783\n",
      "\n",
      "\n",
      "\ttop 10 similar words to \"movie\":\n",
      "\t\t- long\n",
      "\t\t- all\n",
      "\t\t- script\n",
      "\t\t- plot\n",
      "\t\t- going\n",
      "\t\t- breakdowns\n",
      "\t\t- overproduced\n",
      "\t\t- like\n",
      "\t\t- again\n",
      "\t\t- stunt\n",
      "\ttop 10 similar words to \"perfect\":\n",
      "\t\t- shooting\n",
      "\t\t- portrait\n",
      "\t\t- african\n",
      "\t\t- somber\n",
      "\t\t- spooky\n",
      "\t\t- errs\n",
      "\t\t- depression\n",
      "\t\t- ring\n",
      "\t\t- fearlessly\n",
      "\t\t- treasure\n",
      "\ttop 10 similar words to \"horrible\":\n",
      "\t\t- why\n",
      "\t\t- bad\n",
      "\t\t- van\n",
      "\t\t- soul\n",
      "\t\t- chokes\n",
      "\t\t- muddled\n",
      "\t\t- incoherent\n",
      "\t\t- quick\n",
      "\t\t- missing\n",
      "\t\t- video\n",
      "\ttop 10 similar words to \"good\":\n",
      "\t\t- style\n",
      "\t\t- controlled\n",
      "\t\t- observant\n",
      "\t\t- neo\n",
      "\t\t- team\n",
      "\t\t- stuff\n",
      "\t\t- capture\n",
      "\t\t- strange\n",
      "\t\t- clear\n",
      "\t\t- jason\n",
      "\ttop 10 similar words to \"bad\":\n",
      "\t\t- why\n",
      "\t\t- nasty\n",
      "\t\t- bottom\n",
      "\t\t- muddled\n",
      "\t\t- literally\n",
      "\t\t- however\n",
      "\t\t- mystery\n",
      "\t\t- too\n",
      "\t\t- stars\n",
      "\t\t- desperately\n",
      "Epoch 1: 0.3893902776864518\n",
      "\tTrain accuracy: 0.8376128673553467\n",
      "\tTest accuracy: 0.6521331667900085\n",
      "\n",
      "\n",
      "\tcos_dist good vs bad: 1.3756362199783325\n",
      "\tcos_dist terrible vs horrible: 0.13814681768417358\n",
      "\tcos_dist he vs she: 0.24297338724136353\n",
      "\tcos_dist movie vs show: 0.7335861921310425\n",
      "\n",
      "\n",
      "\ttop 10 similar words to \"movie\":\n",
      "\t\t- folks\n",
      "\t\t- frightening\n",
      "\t\t- kurupt\n",
      "\t\t- wander\n",
      "\t\t- trivializes\n",
      "\t\t- hospital\n",
      "\t\t- loves\n",
      "\t\t- fake\n",
      "\t\t- headache\n",
      "\t\t- suppose\n",
      "\ttop 10 similar words to \"perfect\":\n",
      "\t\t- killer\n",
      "\t\t- intriguing\n",
      "\t\t- ride\n",
      "\t\t- capture\n",
      "\t\t- somber\n",
      "\t\t- fun\n",
      "\t\t- examination\n",
      "\t\t- watchable\n",
      "\t\t- change\n",
      "\t\t- sweetly\n",
      "\ttop 10 similar words to \"horrible\":\n",
      "\t\t- why\n",
      "\t\t- van\n",
      "\t\t- bad\n",
      "\t\t- trying\n",
      "\t\t- [johnnie\n",
      "\t\t- fails\n",
      "\t\t- tv\n",
      "\t\t- muddled\n",
      "\t\t- dull\n",
      "\t\t- couldn't\n",
      "\ttop 10 similar words to \"good\":\n",
      "\t\t- intensely\n",
      "\t\t- skilled\n",
      "\t\t- somber\n",
      "\t\t- hoot\n",
      "\t\t- introduction\n",
      "\t\t- guard\n",
      "\t\t- hotsies\n",
      "\t\t- becoming\n",
      "\t\t- guilt\n",
      "\t\t- dickens'\n",
      "\ttop 10 similar words to \"bad\":\n",
      "\t\t- muddled\n",
      "\t\t- why\n",
      "\t\t- unfunny\n",
      "\t\t- van\n",
      "\t\t- essentially\n",
      "\t\t- bottom\n",
      "\t\t- horrible\n",
      "\t\t- trying\n",
      "\t\t- slapstick\n",
      "\t\t- stars\n",
      "Epoch 2: 0.1881048570247309\n",
      "\tTrain accuracy: 0.8461718559265137\n",
      "\tTest accuracy: 0.6422878503799438\n",
      "\n",
      "\n",
      "\tcos_dist good vs bad: 1.1925536394119263\n",
      "\tcos_dist terrible vs horrible: 0.20827800035476685\n",
      "\tcos_dist he vs she: 0.2455918788909912\n",
      "\tcos_dist movie vs show: 0.9147602319717407\n",
      "\n",
      "\n",
      "\ttop 10 similar words to \"movie\":\n",
      "\t\t- response\n",
      "\t\t- folks\n",
      "\t\t- can\n",
      "\t\t- shamefully\n",
      "\t\t- trivializes\n",
      "\t\t- intelligence\n",
      "\t\t- dramas\n",
      "\t\t- itself\n",
      "\t\t- proving\n",
      "\t\t- thing's\n",
      "\ttop 10 similar words to \"perfect\":\n",
      "\t\t- change\n",
      "\t\t- motion\n",
      "\t\t- ride\n",
      "\t\t- watchable\n",
      "\t\t- capture\n",
      "\t\t- sweetly\n",
      "\t\t- killer\n",
      "\t\t- blast\n",
      "\t\t- finesse\n",
      "\t\t- beard\n",
      "\ttop 10 similar words to \"horrible\":\n",
      "\t\t- why\n",
      "\t\t- stiff\n",
      "\t\t- muddled\n",
      "\t\t- couldn't\n",
      "\t\t- bad\n",
      "\t\t- literally\n",
      "\t\t- would've\n",
      "\t\t- nasty\n",
      "\t\t- dubious\n",
      "\t\t- final\n",
      "\ttop 10 similar words to \"good\":\n",
      "\t\t- photographed\n",
      "\t\t- educational\n",
      "\t\t- deepest\n",
      "\t\t- guilt\n",
      "\t\t- easy\n",
      "\t\t- existence\n",
      "\t\t- weil's\n",
      "\t\t- purely\n",
      "\t\t- appreciate\n",
      "\t\t- exposes\n",
      "\ttop 10 similar words to \"bad\":\n",
      "\t\t- essentially\n",
      "\t\t- muddled\n",
      "\t\t- trying\n",
      "\t\t- why\n",
      "\t\t- shadyac\n",
      "\t\t- horrible\n",
      "\t\t- shatner\n",
      "\t\t- van\n",
      "\t\t- unfunny\n",
      "\t\t- chaotic\n",
      "Epoch 3: 0.10282900322666885\n",
      "\tTrain accuracy: 0.8651658892631531\n",
      "\tTest accuracy: 0.6394749283790588\n",
      "\n",
      "\n",
      "\tcos_dist good vs bad: 1.2721331119537354\n",
      "\tcos_dist terrible vs horrible: 0.2875993251800537\n",
      "\tcos_dist he vs she: 0.3234586715698242\n",
      "\tcos_dist movie vs show: 1.2175395488739014\n",
      "\n",
      "\n",
      "\ttop 10 similar words to \"movie\":\n",
      "\t\t- aren't\n",
      "\t\t- fumbled\n",
      "\t\t- vertiginous\n",
      "\t\t- response\n",
      "\t\t- other's\n",
      "\t\t- trama\n",
      "\t\t- arousing\n",
      "\t\t- anonymous\n",
      "\t\t- teen\n",
      "\t\t- screwups\n",
      "\ttop 10 similar words to \"perfect\":\n",
      "\t\t- change\n",
      "\t\t- ride\n",
      "\t\t- motion\n",
      "\t\t- burger\n",
      "\t\t- watchable\n",
      "\t\t- blast\n",
      "\t\t- sweetly\n",
      "\t\t- capture\n",
      "\t\t- introduction\n",
      "\t\t- scariest\n",
      "\ttop 10 similar words to \"horrible\":\n",
      "\t\t- stiff\n",
      "\t\t- nasty\n",
      "\t\t- kung\n",
      "\t\t- why\n",
      "\t\t- muddled\n",
      "\t\t- manipulative\n",
      "\t\t- frank\n",
      "\t\t- did\n",
      "\t\t- flashy\n",
      "\t\t- essentially\n",
      "\ttop 10 similar words to \"good\":\n",
      "\t\t- goyer\n",
      "\t\t- weil's\n",
      "\t\t- intensely\n",
      "\t\t- mechanisms\n",
      "\t\t- photographed\n",
      "\t\t- lyne's\n",
      "\t\t- cube\n",
      "\t\t- salvaged\n",
      "\t\t- crafty\n",
      "\t\t- deepest\n",
      "\ttop 10 similar words to \"bad\":\n",
      "\t\t- essentially\n",
      "\t\t- trying\n",
      "\t\t- muddled\n",
      "\t\t- formulaic\n",
      "\t\t- why\n",
      "\t\t- shadyac\n",
      "\t\t- humorless\n",
      "\t\t- horrible\n",
      "\t\t- awful\n",
      "\t\t- van\n",
      "Epoch 4: 0.06534543500765438\n",
      "\tTrain accuracy: 0.8867393732070923\n",
      "\tTest accuracy: 0.6305672526359558\n",
      "\n",
      "\n",
      "\tcos_dist good vs bad: 1.1772233247756958\n",
      "\tcos_dist terrible vs horrible: 0.36436939239501953\n",
      "\tcos_dist he vs she: 0.3895127773284912\n",
      "\tcos_dist movie vs show: 1.3006283044815063\n",
      "\n",
      "\n",
      "\ttop 10 similar words to \"movie\":\n",
      "\t\t- aren't\n",
      "\t\t- fumbled\n",
      "\t\t- vertiginous\n",
      "\t\t- other's\n",
      "\t\t- trama\n",
      "\t\t- rights\n",
      "\t\t- screwups\n",
      "\t\t- slickest\n",
      "\t\t- octane\n",
      "\t\t- thinness\n",
      "\ttop 10 similar words to \"perfect\":\n",
      "\t\t- burger\n",
      "\t\t- change\n",
      "\t\t- threatens\n",
      "\t\t- entree\n",
      "\t\t- introduction\n",
      "\t\t- breath\n",
      "\t\t- scariest\n",
      "\t\t- royally\n",
      "\t\t- momento\n",
      "\t\t- fabulously\n",
      "\ttop 10 similar words to \"horrible\":\n",
      "\t\t- sunk\n",
      "\t\t- kung\n",
      "\t\t- too\n",
      "\t\t- die\n",
      "\t\t- did\n",
      "\t\t- muddled\n",
      "\t\t- manipulative\n",
      "\t\t- nasty\n",
      "\t\t- dull\n",
      "\t\t- why\n",
      "\ttop 10 similar words to \"good\":\n",
      "\t\t- goyer\n",
      "\t\t- weil's\n",
      "\t\t- tends\n",
      "\t\t- crafty\n",
      "\t\t- ilk\n",
      "\t\t- products\n",
      "\t\t- redolent\n",
      "\t\t- moment\n",
      "\t\t- mechanisms\n",
      "\t\t- sensitivities\n",
      "\ttop 10 similar words to \"bad\":\n",
      "\t\t- essentially\n",
      "\t\t- humorless\n",
      "\t\t- formulaic\n",
      "\t\t- trying\n",
      "\t\t- why\n",
      "\t\t- muddled\n",
      "\t\t- van\n",
      "\t\t- plodding\n",
      "\t\t- shadyac\n",
      "\t\t- awful\n"
     ]
    }
   ],
   "source": [
    "c=CNN(len(vocab))\n",
    "c.train(train_data, test_data, train_targets, test_targets, epochs=5, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dhrJL02kGmow"
   },
   "source": [
    "The expected output for the above snippet is\n",
    "<pre>\n",
    "Starting training...\n",
    "\t100 batches finished. Cost : 0.697723334432\n",
    "\t200 batches finished. Cost : 0.69957424134\n",
    "\t300 batches finished. Cost : 0.697673715353\n",
    "\t400 batches finished. Cost : 0.692196451947\n",
    "\t500 batches finished. Cost : 0.693883402467\n",
    "    ...\n",
    "Epoch 0: 0.624233247656\n",
    "\tTrain accuracy: 0.828467607498\n",
    "\tTest accuracy: 0.736521303654   \n",
    "</pre>\n",
    "If you get any other output and you feel you are correct, you can proceed (However, I cannot think of any case where you can get a different output). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-ozMFo7kGmow"
   },
   "source": [
    "### Effect of Batch Size on Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tybsvV02Gmox"
   },
   "source": [
    "Study the effects of changing batch size. Just run the various experiments and observe the results (Run it in non-verbose mode). No need to make any comments here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "9JjNaWPtGmoy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 0: 0.3244241494630814\n",
      "\tTrain accuracy: 0.8047837018966675\n",
      "\tTest accuracy: 0.66713547706604\n",
      "\n",
      "\n",
      "\tcos_dist good vs bad: 1.5918223857879639\n",
      "\tcos_dist terrible vs horrible: 0.2783013582229614\n",
      "\tcos_dist he vs she: 0.2938779592514038\n",
      "\tcos_dist movie vs show: 0.4061277508735657\n",
      "\n",
      "\n",
      "\ttop 10 similar words to \"movie\":\n",
      "\t\t- come\n",
      "\t\t- merely\n",
      "\t\t- soulless\n",
      "\t\t- nary\n",
      "\t\t- atonal\n",
      "\t\t- tom\n",
      "\t\t- director's\n",
      "\t\t- we\n",
      "\t\t- presume\n",
      "\t\t- aimlessly\n",
      "\ttop 10 similar words to \"perfect\":\n",
      "\t\t- drama\n",
      "\t\t- splendor\n",
      "\t\t- creepiest\n",
      "\t\t- russo\n",
      "\t\t- lewis\n",
      "\t\t- trimmingsarrive\n",
      "\t\t- zaza's\n",
      "\t\t- handling\n",
      "\t\t- carnage\n",
      "\t\t- ya's\n",
      "\ttop 10 similar words to \"horrible\":\n",
      "\t\t- lackluster\n",
      "\t\t- days\n",
      "\t\t- bland\n",
      "\t\t- secrets\n",
      "\t\t- sleep\n",
      "\t\t- kurupt\n",
      "\t\t- unfaithful\n",
      "\t\t- attempts\n",
      "\t\t- choppy\n",
      "\t\t- living\n",
      "\ttop 10 similar words to \"good\":\n",
      "\t\t- ways\n",
      "\t\t- treatment\n",
      "\t\t- pumps\n",
      "\t\t- beautiful\n",
      "\t\t- strange\n",
      "\t\t- somber\n",
      "\t\t- team\n",
      "\t\t- seriousness\n",
      "\t\t- fulford\n",
      "\t\t- tongue\n",
      "\ttop 10 similar words to \"bad\":\n",
      "\t\t- fails\n",
      "\t\t- formulaic\n",
      "\t\t- desperately\n",
      "\t\t- manipulative\n",
      "\t\t- incoherent\n",
      "\t\t- were\n",
      "\t\t- too\n",
      "\t\t- idea\n",
      "\t\t- tedious\n",
      "\t\t- couldn't\n"
     ]
    }
   ],
   "source": [
    "c2=CNN(len(vocab))\n",
    "c2.train(train_data, test_data, train_targets, test_targets, batch_size=2, epochs=1, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 0: 0.21642378283240254\n",
      "\tTrain accuracy: 0.8132254481315613\n",
      "\tTest accuracy: 0.6652601957321167\n",
      "\n",
      "\n",
      "\tcos_dist good vs bad: 1.561958909034729\n",
      "\tcos_dist terrible vs horrible: 0.38920509815216064\n",
      "\tcos_dist he vs she: 0.1379326581954956\n",
      "\tcos_dist movie vs show: 0.34371328353881836\n",
      "\n",
      "\n",
      "\ttop 10 similar words to \"movie\":\n",
      "\t\t- unremittingly\n",
      "\t\t- presume\n",
      "\t\t- chooses\n",
      "\t\t- franchise\n",
      "\t\t- exaggerated\n",
      "\t\t- bloated\n",
      "\t\t- crimen\n",
      "\t\t- rule\n",
      "\t\t- unintentional\n",
      "\t\t- shallower\n",
      "\ttop 10 similar words to \"perfect\":\n",
      "\t\t- finesse\n",
      "\t\t- tosca\n",
      "\t\t- roller\n",
      "\t\t- ya's\n",
      "\t\t- onto\n",
      "\t\t- worthy\n",
      "\t\t- refreshing\n",
      "\t\t- shooting\n",
      "\t\t- handling\n",
      "\t\t- slam\n",
      "\ttop 10 similar words to \"horrible\":\n",
      "\t\t- essentially\n",
      "\t\t- stealing\n",
      "\t\t- unsatisfying\n",
      "\t\t- off\n",
      "\t\t- pity\n",
      "\t\t- choppy\n",
      "\t\t- literally\n",
      "\t\t- simplistic\n",
      "\t\t- jumble\n",
      "\t\t- title's\n",
      "\ttop 10 similar words to \"good\":\n",
      "\t\t- always\n",
      "\t\t- jason\n",
      "\t\t- tug\n",
      "\t\t- unusual\n",
      "\t\t- thriller\n",
      "\t\t- agenda\n",
      "\t\t- capture\n",
      "\t\t- spooky\n",
      "\t\t- authentic\n",
      "\t\t- breaking\n",
      "\ttop 10 similar words to \"bad\":\n",
      "\t\t- soul\n",
      "\t\t- fails\n",
      "\t\t- manipulative\n",
      "\t\t- desperately\n",
      "\t\t- were\n",
      "\t\t- cold\n",
      "\t\t- idea\n",
      "\t\t- too\n",
      "\t\t- witless\n",
      "\t\t- lacking\n"
     ]
    }
   ],
   "source": [
    "c3=CNN(len(vocab))\n",
    "c3.train(train_data, test_data, train_targets, test_targets, batch_size=3, epochs=1, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 0: 0.16231638401561554\n",
      "\tTrain accuracy: 0.8201430439949036\n",
      "\tTest accuracy: 0.6624472737312317\n",
      "\n",
      "\n",
      "\tcos_dist good vs bad: 1.5009946823120117\n",
      "\tcos_dist terrible vs horrible: 0.2552105188369751\n",
      "\tcos_dist he vs she: 0.29949021339416504\n",
      "\tcos_dist movie vs show: 0.4476293921470642\n",
      "\n",
      "\n",
      "\ttop 10 similar words to \"movie\":\n",
      "\t\t- niro\n",
      "\t\t- long\n",
      "\t\t- across\n",
      "\t\t- soulless\n",
      "\t\t- generic\n",
      "\t\t- can't\n",
      "\t\t- ill\n",
      "\t\t- madonna\n",
      "\t\t- only\n",
      "\t\t- showcase\n",
      "\ttop 10 similar words to \"perfect\":\n",
      "\t\t- closes\n",
      "\t\t- graceful\n",
      "\t\t- brutal\n",
      "\t\t- crafted\n",
      "\t\t- sweaty\n",
      "\t\t- portrait\n",
      "\t\t- bernard\n",
      "\t\t- volletta\n",
      "\t\t- al\n",
      "\t\t- payne's\n",
      "\ttop 10 similar words to \"horrible\":\n",
      "\t\t- certain\n",
      "\t\t- soul\n",
      "\t\t- literally\n",
      "\t\t- everybody\n",
      "\t\t- saccharine\n",
      "\t\t- green\n",
      "\t\t- rather\n",
      "\t\t- smug\n",
      "\t\t- only\n",
      "\t\t- were\n",
      "\ttop 10 similar words to \"good\":\n",
      "\t\t- thriller\n",
      "\t\t- neo\n",
      "\t\t- kids\n",
      "\t\t- spooky\n",
      "\t\t- gleefully\n",
      "\t\t- king\n",
      "\t\t- polanski\n",
      "\t\t- threatens\n",
      "\t\t- mann\n",
      "\t\t- monty\n",
      "\ttop 10 similar words to \"bad\":\n",
      "\t\t- desperately\n",
      "\t\t- literally\n",
      "\t\t- manipulative\n",
      "\t\t- fails\n",
      "\t\t- incoherent\n",
      "\t\t- idea\n",
      "\t\t- formulaic\n",
      "\t\t- cliches\n",
      "\t\t- cold\n",
      "\t\t- offensive\n"
     ]
    }
   ],
   "source": [
    "c4=CNN(len(vocab))\n",
    "c4.train(train_data, test_data, train_targets, test_targets, batch_size=4, epochs=1, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 0: 0.12999668564336886\n",
      "\tTrain accuracy: 0.8192050457000732\n",
      "\tTest accuracy: 0.6685419678688049\n",
      "\n",
      "\n",
      "\tcos_dist good vs bad: 1.5494121313095093\n",
      "\tcos_dist terrible vs horrible: 0.32952266931533813\n",
      "\tcos_dist he vs she: 0.3623473644256592\n",
      "\tcos_dist movie vs show: 0.41992610692977905\n",
      "\n",
      "\n",
      "\ttop 10 similar words to \"movie\":\n",
      "\t\t- sara\n",
      "\t\t- didn't\n",
      "\t\t- problem\n",
      "\t\t- orchard\n",
      "\t\t- long\n",
      "\t\t- country\n",
      "\t\t- amount\n",
      "\t\t- smug\n",
      "\t\t- afterschool\n",
      "\t\t- atonal\n",
      "\ttop 10 similar words to \"perfect\":\n",
      "\t\t- ways\n",
      "\t\t- motion\n",
      "\t\t- rewarding\n",
      "\t\t- agenda\n",
      "\t\t- deceptively\n",
      "\t\t- refreshing\n",
      "\t\t- thornberrys\n",
      "\t\t- examination\n",
      "\t\t- masterpiece\n",
      "\t\t- behan's\n",
      "\ttop 10 similar words to \"horrible\":\n",
      "\t\t- video\n",
      "\t\t- why\n",
      "\t\t- kung\n",
      "\t\t- feels\n",
      "\t\t- stealing\n",
      "\t\t- choppy\n",
      "\t\t- hampered\n",
      "\t\t- elmo\n",
      "\t\t- predictably\n",
      "\t\t- initially\n",
      "\ttop 10 similar words to \"good\":\n",
      "\t\t- ways\n",
      "\t\t- life\n",
      "\t\t- authentic\n",
      "\t\t- somber\n",
      "\t\t- manga\n",
      "\t\t- viewing\n",
      "\t\t- observant\n",
      "\t\t- thriller\n",
      "\t\t- emotional\n",
      "\t\t- haven't\n",
      "\ttop 10 similar words to \"bad\":\n",
      "\t\t- were\n",
      "\t\t- desperately\n",
      "\t\t- flair\n",
      "\t\t- too\n",
      "\t\t- stars\n",
      "\t\t- idea\n",
      "\t\t- cold\n",
      "\t\t- plotting\n",
      "\t\t- offensive\n",
      "\t\t- flashy\n"
     ]
    }
   ],
   "source": [
    "c5=CNN(len(vocab))\n",
    "c5.train(train_data, test_data, train_targets, test_targets, batch_size=5, epochs=1, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 0: 0.10834514336508287\n",
      "\tTrain accuracy: 0.8219017386436462\n",
      "\tTest accuracy: 0.6661978363990784\n",
      "\n",
      "\n",
      "\tcos_dist good vs bad: 1.5455517768859863\n",
      "\tcos_dist terrible vs horrible: 0.30691367387771606\n",
      "\tcos_dist he vs she: 0.5375597476959229\n",
      "\tcos_dist movie vs show: 0.5246798992156982\n",
      "\n",
      "\n",
      "\ttop 10 similar words to \"movie\":\n",
      "\t\t- speaking\n",
      "\t\t- generic\n",
      "\t\t- problem\n",
      "\t\t- listless\n",
      "\t\t- long\n",
      "\t\t- pint\n",
      "\t\t- lampoon's\n",
      "\t\t- script\n",
      "\t\t- anim\n",
      "\t\t- selection\n",
      "\ttop 10 similar words to \"perfect\":\n",
      "\t\t- calibrated\n",
      "\t\t- inter\n",
      "\t\t- spooky\n",
      "\t\t- crafted\n",
      "\t\t- brutal\n",
      "\t\t- balances\n",
      "\t\t- witness\n",
      "\t\t- motion\n",
      "\t\t- execution\n",
      "\t\t- delivered\n",
      "\ttop 10 similar words to \"horrible\":\n",
      "\t\t- essentially\n",
      "\t\t- days\n",
      "\t\t- couldn't\n",
      "\t\t- were\n",
      "\t\t- idea\n",
      "\t\t- saccharine\n",
      "\t\t- advantage\n",
      "\t\t- bland\n",
      "\t\t- half\n",
      "\t\t- concept\n",
      "\ttop 10 similar words to \"good\":\n",
      "\t\t- loved\n",
      "\t\t- spooky\n",
      "\t\t- ways\n",
      "\t\t- somber\n",
      "\t\t- becoming\n",
      "\t\t- life\n",
      "\t\t- kids\n",
      "\t\t- neo\n",
      "\t\t- authentic\n",
      "\t\t- detailing\n",
      "\ttop 10 similar words to \"bad\":\n",
      "\t\t- formulaic\n",
      "\t\t- soul\n",
      "\t\t- dull\n",
      "\t\t- silly\n",
      "\t\t- too\n",
      "\t\t- feels\n",
      "\t\t- pointless\n",
      "\t\t- schneider\n",
      "\t\t- clich\n",
      "\t\t- did\n"
     ]
    }
   ],
   "source": [
    "c6=CNN(len(vocab))\n",
    "c6.train(train_data, test_data, train_targets, test_targets, batch_size=6, epochs=1, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 0: 0.081408474178104\n",
      "\tTrain accuracy: 0.8213155269622803\n",
      "\tTest accuracy: 0.6708860993385315\n",
      "\n",
      "\n",
      "\tcos_dist good vs bad: 1.5765392780303955\n",
      "\tcos_dist terrible vs horrible: 0.2778332233428955\n",
      "\tcos_dist he vs she: 0.24149519205093384\n",
      "\tcos_dist movie vs show: 0.5590507388114929\n",
      "\n",
      "\n",
      "\ttop 10 similar words to \"movie\":\n",
      "\t\t- going\n",
      "\t\t- can't\n",
      "\t\t- empty\n",
      "\t\t- intentions\n",
      "\t\t- men\n",
      "\t\t- tired\n",
      "\t\t- inelegant\n",
      "\t\t- days\n",
      "\t\t- sum\n",
      "\t\t- franchise\n",
      "\ttop 10 similar words to \"perfect\":\n",
      "\t\t- photography\n",
      "\t\t- invitingly\n",
      "\t\t- oleander\n",
      "\t\t- ride\n",
      "\t\t- float\n",
      "\t\t- craft\n",
      "\t\t- shooting\n",
      "\t\t- grip\n",
      "\t\t- roller\n",
      "\t\t- confrontational\n",
      "\ttop 10 similar words to \"horrible\":\n",
      "\t\t- offensive\n",
      "\t\t- needed\n",
      "\t\t- tv\n",
      "\t\t- stealing\n",
      "\t\t- essentially\n",
      "\t\t- friday\n",
      "\t\t- sewer\n",
      "\t\t- [javier\n",
      "\t\t- why\n",
      "\t\t- oh\n",
      "\ttop 10 similar words to \"good\":\n",
      "\t\t- ways\n",
      "\t\t- brother\n",
      "\t\t- haven't\n",
      "\t\t- fabric\n",
      "\t\t- seus\n",
      "\t\t- laugh\n",
      "\t\t- nelson\n",
      "\t\t- part\n",
      "\t\t- capturou\n",
      "\t\t- martha\n",
      "\ttop 10 similar words to \"bad\":\n",
      "\t\t- which\n",
      "\t\t- trying\n",
      "\t\t- couldn't\n",
      "\t\t- however\n",
      "\t\t- dull\n",
      "\t\t- why\n",
      "\t\t- were\n",
      "\t\t- plotting\n",
      "\t\t- muddled\n",
      "\t\t- lacking\n"
     ]
    }
   ],
   "source": [
    "c8=CNN(len(vocab))\n",
    "c8.train(train_data, test_data, train_targets, test_targets, batch_size=8, epochs=1, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 0: 0.06529437454703657\n",
      "\tTrain accuracy: 0.8220189809799194\n",
      "\tTest accuracy: 0.6690107583999634\n",
      "\n",
      "\n",
      "\tcos_dist good vs bad: 1.6935162544250488\n",
      "\tcos_dist terrible vs horrible: 0.26092392206192017\n",
      "\tcos_dist he vs she: 0.2653992176055908\n",
      "\tcos_dist movie vs show: 0.35052400827407837\n",
      "\n",
      "\n",
      "\ttop 10 similar words to \"movie\":\n",
      "\t\t- listless\n",
      "\t\t- mistaken\n",
      "\t\t- long\n",
      "\t\t- santa\n",
      "\t\t- dooby\n",
      "\t\t- turgid\n",
      "\t\t- mugs\n",
      "\t\t- dopey\n",
      "\t\t- monsters\n",
      "\t\t- 98\n",
      "\ttop 10 similar words to \"perfect\":\n",
      "\t\t- smartly\n",
      "\t\t- delight\n",
      "\t\t- entree\n",
      "\t\t- j\n",
      "\t\t- objective\n",
      "\t\t- ya's\n",
      "\t\t- ride\n",
      "\t\t- formalism\n",
      "\t\t- playful\n",
      "\t\t- adds\n",
      "\ttop 10 similar words to \"horrible\":\n",
      "\t\t- hampered\n",
      "\t\t- days\n",
      "\t\t- manipulative\n",
      "\t\t- flashy\n",
      "\t\t- would've\n",
      "\t\t- idea\n",
      "\t\t- did\n",
      "\t\t- fails\n",
      "\t\t- cold\n",
      "\t\t- rather\n",
      "\ttop 10 similar words to \"good\":\n",
      "\t\t- loved\n",
      "\t\t- always\n",
      "\t\t- treat\n",
      "\t\t- audacious\n",
      "\t\t- style\n",
      "\t\t- opening\n",
      "\t\t- inventively\n",
      "\t\t- maintains\n",
      "\t\t- anime\n",
      "\t\t- kids\n",
      "\ttop 10 similar words to \"bad\":\n",
      "\t\t- too\n",
      "\t\t- were\n",
      "\t\t- showtime\n",
      "\t\t- cold\n",
      "\t\t- idea\n",
      "\t\t- unpleasant\n",
      "\t\t- essentially\n",
      "\t\t- why\n",
      "\t\t- formulaic\n",
      "\t\t- couldn't\n"
     ]
    }
   ],
   "source": [
    "c10=CNN(len(vocab))\n",
    "c10.train(train_data, test_data, train_targets, test_targets, batch_size=10, epochs=1, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 0: 0.04370334985555581\n",
      "\tTrain accuracy: 0.8222534656524658\n",
      "\tTest accuracy: 0.6629160642623901\n",
      "\n",
      "\n",
      "\tcos_dist good vs bad: 1.6712089776992798\n",
      "\tcos_dist terrible vs horrible: 0.3265742063522339\n",
      "\tcos_dist he vs she: 0.26655322313308716\n",
      "\tcos_dist movie vs show: 0.5167897939682007\n",
      "\n",
      "\n",
      "\ttop 10 similar words to \"movie\":\n",
      "\t\t- across\n",
      "\t\t- morally\n",
      "\t\t- hasn't\n",
      "\t\t- amount\n",
      "\t\t- script\n",
      "\t\t- tuxedo\n",
      "\t\t- uninteresting\n",
      "\t\t- makers\n",
      "\t\t- long\n",
      "\t\t- felt\n",
      "\ttop 10 similar words to \"perfect\":\n",
      "\t\t- shimizu\n",
      "\t\t- powerful\n",
      "\t\t- our\n",
      "\t\t- portrait\n",
      "\t\t- delivers\n",
      "\t\t- retelling\n",
      "\t\t- seriousness\n",
      "\t\t- performances\n",
      "\t\t- ride\n",
      "\t\t- format\n",
      "\ttop 10 similar words to \"horrible\":\n",
      "\t\t- stealing\n",
      "\t\t- flashy\n",
      "\t\t- offensive\n",
      "\t\t- rather\n",
      "\t\t- kung\n",
      "\t\t- congratulatory\n",
      "\t\t- would've\n",
      "\t\t- feels\n",
      "\t\t- fails\n",
      "\t\t- unfunny\n",
      "\ttop 10 similar words to \"good\":\n",
      "\t\t- heaven\n",
      "\t\t- kids\n",
      "\t\t- treat\n",
      "\t\t- ways\n",
      "\t\t- strange\n",
      "\t\t- jason\n",
      "\t\t- ambitious\n",
      "\t\t- man\n",
      "\t\t- laugh\n",
      "\t\t- auspicious\n",
      "\ttop 10 similar words to \"bad\":\n",
      "\t\t- why\n",
      "\t\t- dull\n",
      "\t\t- too\n",
      "\t\t- completely\n",
      "\t\t- silly\n",
      "\t\t- did\n",
      "\t\t- feels\n",
      "\t\t- essentially\n",
      "\t\t- cold\n",
      "\t\t- flat\n"
     ]
    }
   ],
   "source": [
    "c15=CNN(len(vocab))\n",
    "c15.train(train_data, test_data, train_targets, test_targets, batch_size=15, epochs=1, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 0: 0.022094361855313122\n",
      "\tTrain accuracy: 0.8174463510513306\n",
      "\tTest accuracy: 0.6596342921257019\n",
      "\n",
      "\n",
      "\tcos_dist good vs bad: 1.7223000526428223\n",
      "\tcos_dist terrible vs horrible: 0.23593521118164062\n",
      "\tcos_dist he vs she: 0.39524173736572266\n",
      "\tcos_dist movie vs show: 0.5202136039733887\n",
      "\n",
      "\n",
      "\ttop 10 similar words to \"movie\":\n",
      "\t\t- long\n",
      "\t\t- script\n",
      "\t\t- ill\n",
      "\t\t- arnold\n",
      "\t\t- sara\n",
      "\t\t- only\n",
      "\t\t- problem\n",
      "\t\t- can't\n",
      "\t\t- sequel\n",
      "\t\t- advantage\n",
      "\ttop 10 similar words to \"perfect\":\n",
      "\t\t- retelling\n",
      "\t\t- ride\n",
      "\t\t- signals\n",
      "\t\t- masterpiece\n",
      "\t\t- flaws\n",
      "\t\t- offerings\n",
      "\t\t- franco\n",
      "\t\t- powerful\n",
      "\t\t- shooting\n",
      "\t\t- charmingly\n",
      "\ttop 10 similar words to \"horrible\":\n",
      "\t\t- [t]he\n",
      "\t\t- rather\n",
      "\t\t- friday\n",
      "\t\t- mckay\n",
      "\t\t- affleck\n",
      "\t\t- mindless\n",
      "\t\t- downbeat\n",
      "\t\t- only\n",
      "\t\t- schneider's\n",
      "\t\t- feels\n",
      "\ttop 10 similar words to \"good\":\n",
      "\t\t- features\n",
      "\t\t- huppert\n",
      "\t\t- fairy\n",
      "\t\t- illuminating\n",
      "\t\t- digital\n",
      "\t\t- worth\n",
      "\t\t- blisteringly\n",
      "\t\t- douglas\n",
      "\t\t- tsai\n",
      "\t\t- thanks\n",
      "\ttop 10 similar words to \"bad\":\n",
      "\t\t- why\n",
      "\t\t- too\n",
      "\t\t- tv\n",
      "\t\t- someone\n",
      "\t\t- however\n",
      "\t\t- essentially\n",
      "\t\t- bears\n",
      "\t\t- trying\n",
      "\t\t- simplistic\n",
      "\t\t- feels\n"
     ]
    }
   ],
   "source": [
    "c30=CNN(len(vocab))\n",
    "c30.train(train_data, test_data, train_targets, test_targets, batch_size=30, epochs=1, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 0: 0.013400489202989812\n",
      "\tTrain accuracy: 0.795052170753479\n",
      "\tTest accuracy: 0.6502578258514404\n",
      "\n",
      "\n",
      "\tcos_dist good vs bad: 1.6879076957702637\n",
      "\tcos_dist terrible vs horrible: 0.29935145378112793\n",
      "\tcos_dist he vs she: 0.7008346915245056\n",
      "\tcos_dist movie vs show: 0.5126837491989136\n",
      "\n",
      "\n",
      "\ttop 10 similar words to \"movie\":\n",
      "\t\t- limp\n",
      "\t\t- sum\n",
      "\t\t- generic\n",
      "\t\t- listless\n",
      "\t\t- problem\n",
      "\t\t- showcase\n",
      "\t\t- script\n",
      "\t\t- ill\n",
      "\t\t- can't\n",
      "\t\t- amount\n",
      "\ttop 10 similar words to \"perfect\":\n",
      "\t\t- conviction\n",
      "\t\t- sunday\n",
      "\t\t- accused\n",
      "\t\t- fashioned\n",
      "\t\t- introduction\n",
      "\t\t- mixes\n",
      "\t\t- usual\n",
      "\t\t- respectable\n",
      "\t\t- zaza's\n",
      "\t\t- laugh\n",
      "\ttop 10 similar words to \"horrible\":\n",
      "\t\t- problem\n",
      "\t\t- michell\n",
      "\t\t- imagine\n",
      "\t\t- stars\n",
      "\t\t- ill\n",
      "\t\t- slow\n",
      "\t\t- painfully\n",
      "\t\t- oedekerk\n",
      "\t\t- tv\n",
      "\t\t- only\n",
      "\ttop 10 similar words to \"good\":\n",
      "\t\t- morton\n",
      "\t\t- blisteringly\n",
      "\t\t- heaven\n",
      "\t\t- warm\n",
      "\t\t- fun\n",
      "\t\t- lovely\n",
      "\t\t- suspenseful\n",
      "\t\t- overcomes\n",
      "\t\t- benefits\n",
      "\t\t- kids\n",
      "\ttop 10 similar words to \"bad\":\n",
      "\t\t- too\n",
      "\t\t- awful\n",
      "\t\t- little\n",
      "\t\t- why\n",
      "\t\t- idea\n",
      "\t\t- dull\n",
      "\t\t- essentially\n",
      "\t\t- however\n",
      "\t\t- boring\n",
      "\t\t- inept\n"
     ]
    }
   ],
   "source": [
    "c50=CNN(len(vocab))\n",
    "c50.train(train_data, test_data, train_targets, test_targets, batch_size=50, epochs=1, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KzOv1TG_Gmoz"
   },
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZVha1w_BGmo0"
   },
   "source": [
    "Add 2 functions - get_distance and get_most_similar to the CNN class (the big one). \n",
    "* get_distance(word1,word2) - should return the cosine distance between the 2 words.\n",
    "* get_most_similar(word) - should return top 10 most similar words to the word passed.\n",
    "\n",
    "Now, use the 2 functions to record the distances between a list of word-pairs as the training progresses. (One easy way to go about could be to save the embedding matrix in the hard-disk for every 5 updates.):\n",
    "* Study the distance between words of opposite sentiment as the training progresses. Ex: Good and Bad, Good and horrible, etc.\n",
    "* Study the distance between words of same sentiment. Ex: Good and Beautiful, Bad and Terrible, etc.\n",
    "* Study how the non-sentiment bearing words relate to each other. Ex: his, her, an, it, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This section shows results after training with batch_size = 1\n",
    "Note: To see distance and similarity outputs during training, refer to the training output when verbose=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance between words of opposite sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "zw4bN713Gmo0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1772233\n"
     ]
    }
   ],
   "source": [
    "print(c.get_distance('good', 'bad'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5896721\n"
     ]
    }
   ],
   "source": [
    "print(c.get_distance('beautiful', 'horrible'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3137771\n"
     ]
    }
   ],
   "source": [
    "print(c.get_distance('like', 'dislike'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6863925\n"
     ]
    }
   ],
   "source": [
    "print(c.get_distance('fun', 'boring'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8917293\n"
     ]
    }
   ],
   "source": [
    "print(c.get_distance('perfect', 'disgusting'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance between words of similar sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1008879\n"
     ]
    }
   ],
   "source": [
    "print(c.get_distance('good', 'beautiful'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.42478412\n"
     ]
    }
   ],
   "source": [
    "print(c.get_distance('perfect', 'fantastic'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4673578\n"
     ]
    }
   ],
   "source": [
    "print(c.get_distance('beautiful', 'wonderful'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23816931\n"
     ]
    }
   ],
   "source": [
    "print(c.get_distance('bad', 'terrible'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.28796977\n"
     ]
    }
   ],
   "source": [
    "print(c.get_distance('bad', 'boring'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3643694\n"
     ]
    }
   ],
   "source": [
    "print(c.get_distance('terrible', 'horrible'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relation between non-sentiment bearing words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.90474015\n"
     ]
    }
   ],
   "source": [
    "print(c.get_distance('him', 'her'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38951278\n"
     ]
    }
   ],
   "source": [
    "print(c.get_distance('he', 'she'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.63445246\n"
     ]
    }
   ],
   "source": [
    "print(c.get_distance('a', 'an'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.247942\n"
     ]
    }
   ],
   "source": [
    "print(c.get_distance('he', 'it'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2219324\n"
     ]
    }
   ],
   "source": [
    "print(c.get_distance('she', 'it'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5465028\n"
     ]
    }
   ],
   "source": [
    "print(c.get_distance('red', 'blue'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.94863224\n"
     ]
    }
   ],
   "source": [
    "print(c.get_distance('we', 'they'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7178945\n"
     ]
    }
   ],
   "source": [
    "print(c.get_distance('the', 'that'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3006283\n"
     ]
    }
   ],
   "source": [
    "print(c.get_distance('movie', 'show'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similarities between words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['goyer', \"weil's\", 'tends', 'crafty', 'ilk', 'products', 'redolent', 'moment', 'mechanisms', 'sensitivities']\n"
     ]
    }
   ],
   "source": [
    "print(c.get_most_similar('good'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"aren't\", 'fumbled', 'vertiginous', \"other's\", 'trama', 'rights', 'screwups', 'slickest', 'octane', 'thinness']\n"
     ]
    }
   ],
   "source": [
    "print(c.get_most_similar('movie'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['classic', 'frustration', 'confirmation', 'bought', 'sumptuous', 'goers', 'sick', 'images', 'maniac', 'ww']\n"
     ]
    }
   ],
   "source": [
    "print(c.get_most_similar('beautiful'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['essentially', 'humorless', 'formulaic', 'trying', 'why', 'muddled', 'van', 'plodding', 'shadyac', 'awful']\n"
     ]
    }
   ],
   "source": [
    "print(c.get_most_similar('bad'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sunk', 'kung', 'too', 'die', 'did', 'muddled', 'manipulative', 'nasty', 'dull', 'why']\n"
     ]
    }
   ],
   "source": [
    "print(c.get_most_similar('horrible'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"petersburg's\", 'pap', 'drung', 'gosto', 'girlish', 'theatrically', \"halloween's\", 'carry', 'irwin', 'minus']\n"
     ]
    }
   ],
   "source": [
    "print(c.get_most_similar('the'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['inept', 'poorly', 'stupid', 'idea', '50', 'acceptable', 'mawkish', 'wilder', 'lame', 'shadyac']\n"
     ]
    }
   ],
   "source": [
    "print(c.get_most_similar('he'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['e', 'brings', 'polished', 'candid', 'chicago', 'wickedly', 'overcomes', 'huppert', 'bourne', 'jackson']\n"
     ]
    }
   ],
   "source": [
    "print(c.get_most_similar('red'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cold', 'damned', 'expected', 'anything', 'cliches', 'dull', 'starts', 'except', 'unintentionally', 'necessarily']\n"
     ]
    }
   ],
   "source": [
    "print(c.get_most_similar('should'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UMOB3iO5ag_X"
   },
   "source": [
    "### Learnings:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bj0s5x-FarR3"
   },
   "source": [
    "List out the observations and conclusions you made from the various experiments. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "TpC2Sjpfa4Zj"
   },
   "source": [
    "#### 1 epoch and Batch Size > 1\n",
    "- The CNN learns non-sentiment words better with a bigger batch size. For example, It seems to be good at predicting 'movie' with batch size = 30 and 50.\n",
    "- The CNN learns sentiment words better with a smaller batch size (e.g. good, bad, perfect, horrible).\n",
    "\n",
    "#### Epoch > 1 and Batch Size = 1\n",
    "- The cosine distances gets worse when training on multiple epochs for \"terrible\" vs \"horrible\"\n",
    "- The cosine distances gets better when training on multiple epochs for \"good\" vs \"bad\"\n",
    "- The top 10 similar words to \"movie\" gets worse as training progresses over multiple epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "A3.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
